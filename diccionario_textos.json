{"Texto0.pdf": {"Beyond Black Box AI-Generated Plagiarism Detection: From Sentence to Document Level": "\n\nMujahid Ali Quidwai\n\nNew York University\n\nmaq4265@nyu.edu\n\nChunhui Li\n\nColumbia University\n\ncl4282@columbia.edu\n\nParijat Dube\n\nIBM Research\n\npdube@us.ibm.com\n\n###### ", "Abstract": "\n\nThe increasing reliance on large language models (LLMs) in academic writing has led to a rise in plagiarism. Existing AI-generated text classifiers have limited accuracy and often produce false positives. We propose a novel approach using natural language processing (NLP) techniques, offering quantifiable metrics at both sentence and document levels for easier interpretation by human evaluators. Our method employs a multi-faceted approach, generating multiple paraphrased versions of a given question and inputting them into the LLM to generate answers. By using a contrastive loss function based on cosine similarity, we match generated sentences with those from the student's response. Our approach achieves up to 94% accuracy in classifying human and AI text, providing a robust and adaptable solution for plagiarism detection in academic settings. This method improves with LLM advancements, reducing the need for new model training or reconfiguration, and offers a more transparent way of evaluating and detecting AI-generated text.\n\n## ", "1 Introduction": "\n\nIn recent years, large language models (LLMs) have demonstrated remarkable capabilities across a wide range of natural language processing (NLP) tasks, including text classification, sentiment analysis, translation, and question-answering He et al. (2023).\n\nThese foundational models exhibit immense potential in tackling a diverse array of NLP tasks, spanning from natural language understanding (NLU) to natural language generation (NLG), and even laying the groundwork for Artificial General Intelligence (AGI) Yang et al. (2023). In the world of advanced LLMs, ChatGPT (2023) as an AI model developed by OpenAI (2023b) has become one of the most popular and widely used models, setting new records for performance and flexibility in many applications. According to the latest available data, ChatGPT (2023) currently has over 100 million users and the website currently generates 1 billion visitors per month Duarte (2023). While ChatGPT has brought numerous benefits such as it allows us to obtain information more effectively, improves people's writing skills etc., however, it has also introduced considerable risks OpenAI (2023a).\n\nA major risk associated with the growing dependence on ChatGPT is the escalation of plagiarism in academic writing Khalil and Er (2023), which subsequently compromises the integrity and purpose of assignments and examinations. Thanks to its advanced training process and access to abundant pre-training data sets, ChatGPT is capable of resembling human-like language when provided with a prompt Joshi et al. (2023). It even exceeds human performance in some academic writing while maintaining authenticity and richness. Furthermore, humans are unable to accurately distinguish between Human Generated Text (HGT) and Machine Generated Text (MGT), regardless of their familiarity with ChatGPT Herbold et al. (2023). These factors present significant challenges in maintaining educational integrity and challenge the current paradigm of how teachers teach.\n\nTo reduce potential plagiarism caused by the use of LLMs, researchers have developed various\n\nFigure 1: Popular LLMs and AI-generated text detection toolsAI-generated text classifiers or tools such as Log-Likelihood Solaiman et al. (2019), RoBERTa-QA (HC3) Guo et al. (2023), GPTZero (2023), OpenAI Classifier OpenAI (2023b), DetectGPT Mitchell et al. (2023), and Turntin Fowler (2023). Figure 1 lists popular LLMs used for text generation and AI-generated text detection tools.\n\nExisting approaches for detecting text generated by language models have several limitations as highlighted in Table 1.\n\nFor instance, these tools may rapidly become outdated due to technological advancements, such as new versions of GPT models, necessitating classifier retraining and often resulting in limited accuracy. Models trained specifically on one language model might not effectively detect text generated by a different language model (e.g., DetectGPT classifier works only on text generated using GPT2 OpenAI (2023b)). Additionally, some detection tools provide non-quantitative label results, and all possess a black-box nature concerning prediction accuracy. Thus the predictions made by such tools lack explainability and are challenging for human evaluators to comprehend. This issue leads to a high number of false positive punishments in academic settings Fowler (2023).\n\nWe propose a novel approach for detecting plagiarized text, which focuses on NLP techniques. Our approach offers more quantifiable metrics at the sentence level, allowing for easier interpretation by human evaluators and eliminating the black-box nature of existing AI text detection methods. Our approach is not limited to ChatGPT but can also be applied to other LLMs such as BardAI Google (2023), Character.AI Character.AI (2023), and so on, it also can adapt automatically as those LLMs upgrade. This adaptability helps to ensure that it does not become outdated quickly as technology advances.\n\nIn evaluating our approach, we used the open dataset known as the ChatGPT Comparison Corpus (HC3) Guo et al. (2023). This dataset contains 10,000 questions and their corresponding answers from both human experts and ChatGPT, covering a range of domains including open-domain, computer science, finance, medicine, law, and psychology. Our approach achieves **94%** accuracy in classifying between human answers and ChatGPT answers in the HC3 data set.\n\nThe paper is structured as follows. Section 2 contains a review of relevant literature. Our proposed end-to-end approach for AI-text detection is detailed in Section 3, where we describe the method framework. In Section 4, we present our main results from the experimental evaluation. Lastly, we summarize our findings and discuss future directions in Section 5, which serves as the conclusion.\n\n## ", "2 Related Work": "", "DetectGPT": " Mitchell et al. (2023), and Turntin Fowler (2023). Figure 1 lists popular LLMs used for text generation and AI-generated text detection tools.\n\nExisting approaches for detecting text generated by language models have several limitations as highlighted in Table 1.\n\nFor instance, these tools may rapidly become outdated due to technological advancements, such as new versions of GPT models, necessitating classifier retraining and often resulting in limited accuracy. Models trained specifically on one language model might not effectively detect text generated by a different language model (e.g., DetectGPT classifier works only on text generated using GPT2 OpenAI (2023b)). Additionally, some detection tools provide non-quantitative label results, and all possess a black-box nature concerning prediction accuracy. Thus the predictions made by such tools lack explainability and are challenging for human evaluators to comprehend. This issue leads to a high number of false positive punishments in academic settings Fowler (2023).\n\nWe propose a novel approach for detecting plagiarized text, which focuses on NLP techniques. Our approach offers more quantifiable metrics at the sentence level, allowing for easier interpretation by human evaluators and eliminating the black-box nature of existing AI text detection methods. Our approach is not limited to ChatGPT but can also be applied to other LLMs such as BardAI Google (2023), Character.AI Character.AI (2023), and so on, it also can adapt automatically as those LLMs upgrade. This adaptability helps to ensure that it does not become outdated quickly as technology advances.\n\nIn evaluating our approach, we used the open dataset known as the ChatGPT Comparison Corpus (HC3) Guo et al. (2023). This dataset contains 10,000 questions and their corresponding answers from both human experts and ChatGPT, covering a range of domains including open-domain, computer science, finance, medicine, law, and psychology. Our approach achieves **94%** accuracy in classifying between human answers and ChatGPT answers in the HC3 data set.\n\nThe paper is structured as follows. Section 2 contains a review of relevant literature. Our proposed end-to-end approach for AI-text detection is detailed in Section 3, where we describe the method framework. In Section 4, we present our main results from the experimental evaluation. Lastly, we summarize our findings and discuss future directions in Section 5, which serves as the conclusion.\n\n## 2 Related Work\n\nThe field of AI-generated text detection has garnered significant interest, but only a few models and tools have achieved widespread adoption. In this section, we discuss state-of-the-art approaches, the datasets used for training their classifiers, and their limitations.\n\n### DetectGPT\n\nDetectGPT Mitchell et al. (2023) is a zero-shot machine-generated text detection method that leverages the negative curvature regions of an LLM's log probability function. The approach does not require training a separate classifier, collecting a dataset of real or generated passages, or watermarking generated text. Despite its effectiveness, DetectGPT is limited to GPT-2 generated text, and its performance may not extend to other LLMs Tang et al. (2023).\n\n### ", "Human ChatGPT Comparison Corpus (HC3)": "\n\nGuo et al. (2023) introduced the HC3 dataset, which contains tens of thousands of comparison responses from both human experts and ChatGPT ChatGPT (2023). They conducted comprehensive human evaluations and linguistic analyses to study the characteristics of ChatGPT's responses, the differences and gaps from human experts, and future directions for LLMs. Furthermore, they built\n\n\nTable 1: Problems with current approaches three different detection systems to effectively detect whether a text is generated by ChatGPT or humans. However, this approach might still suffer from high false positive rates and it does not provide correct sentence-level comparison metrics.\n\n### ", "OpenAI AI Text Classifier": "\n\nThe OpenAI AI Text Classifier [14] is a fine-tuned GPT model designed to predict the likelihood of a piece of text being AI-generated. This free tool aims to foster discussions on AI literacy, but it has limitations: it requires a minimum of 1,000 characters, can mislabel AI-generated and human-written text, and can be evaded by editing AI-generated text. Additionally, it also suffers from high positive rates.\n\nIn our research, we aim to address the limitations of these existing methods by developing a novel approach for detecting plagiarized text, focusing on natural language processing techniques that provide more quantifiable metrics and eliminate the black-box nature of existing AI text detection methods\n\n## ", "3 Our Method": "\n\nIn this section, we present our approach to effectively compare and detect plagiarism in student responses. Our method utilizes an advanced paraphrasing model, a state-of-the-art language model, and a contrastive loss function to deliver a comprehensive and transparent evaluation system. Figure 2 shows the different components of our proposed model architecture.\n\n### ", "Paraphrasing Model": "\n\nTo simulate the variety of questions a student might pose to a large language model (LLM), we employ a paraphrasing model (refer to Figure 2). This model generates multiple paraphrased versions of a given question, accounting for the diversity in student queries and ensuring robustness in the detection process.\n\nUsing Google's T5 language model [13] an initial dataset of questions is selected and paraphrased. The T5 model is trained on a large corpus of text, which enables it to understand the context and rephrase questions that preserve the original meaning while introducing variety. This model mimics the array of questions students might ask an LLM, a crucial feature to ensure robustness in the detection process.\n\nFor example, consider the following original question Q1 from the Reddit ELI5 (HC3 dataset) [14]:\n\n1. What is a hacker really doing? I've always wanted to know why hackers are jamming on their keyboards and what they can possibly be doing that requires such precision and speed. Please explain like I'm five.\n\nThe paraphrasing model generates the following paraphrased versions:\n\n1. What do hackers actually do? I've always been curious about why they type so fast on their keyboards and what they're doing that needs such accuracy and quickness. Can you explain it to me as if I were a five-year-old?\n2. I've always wondered what hackers are truly up to when they're typing rapidly on their keyboards. What kind of tasks are they performing that demand such skill and swiftness? Please explain it in a simple way, as if I were just five years old.\n3. What is it that hackers do when they're furiously typing on their keyboards? I'm curious about the activities they're involved in that require such speed and precision. Could you break it down for me in terms a five-year-old would understand?\n\n### ", "LLM Integration": "\n\nOnce we have the paraphrased questions, we input them into an LLM. This model, particularly the GPT-3.5-turbo from OpenAI ChatGPT [2], is adept at generating coherent and contextually appropriate answers. Its proficiency stems from pre-training on an extensive amount of textual data, allowing it to provide accurate and relevant responses to the paraphrased questions.\n\nFor example, we input the first paraphrased question P1 into the LLM and generate the following answer:\n\n1. Hackers are like computer experts who solve puzzles. They use their keyboards to give commands to computers to find secret information or fix problems. They need to be fast and accurate because computers follow instructions very quickly, and one wrong command can cause mistakes. Just like playing a game, they need to be good at using their keyboards to win the computer puzzle.\n\n \n\n### ", "Evaluation Process": "\n\nTo facilitate a detailed comparison between the LLM-generated answers and student responses, we break down each answer into individual sentences. This granular approach enhances transparency and allows for a more in-depth evaluation of potential plagiarism.\n\nFor example, consider the LLM-generated answer A1 and a human answer H1 for question Q1 from the Reddit ELI5 dataset:\n\n1. I've always wanted to know why hackers are jamming on their keyboards In reality, this doesn't happen. This is done in movies to make it look dramatic and exciting. Real computer hacking involves staring at a computer screen for hours of a time, searching a lot on Google, muttering hmmm and various expletives to oneself now and then, and stroking one 's hacker - beard while occasionally tapping on a few keys .\", \"Computers are stupid, they don't know what they are doing, they just do it. If you tell a computer to give a cake to every person that walks through the door, it will do. Hackers are the people that get extra cake by going around the building and back through the door. GLaDOS however, will give you no cake .\", \"Hackers have a deep and complete understanding of a subject ( e.g. a machine or computer program ). They change the behavior of the subject to something that was never intended or even thought it would be possible by the creator of the subject .\n\nWe next do a pair-wise comparison between a sentence in H1 and all the sentences in A1, A2, and A3, to identify the AI generated sentence which is most similar to H1.\n\n### ", "Cosine Similarity": "\n\nTo compare two sentences we measure cosine similarity between the embeddings for the sentences generated using text-embedding-ada-002. The use of cosine similarity on sentence level contextualembeddings captures semantic and syntactic congruence between compared sentences. We use the term Human-Machine (HM) comparison for comparing sentence pairs involving a human-generated sentence and a machine-generated sentence. While Machine-Machine (MM) comparison involves comparing two machine-generated sentences.\n\n### ", "Linear Discriminant Analysis": "\n\nWe apply Linear Discriminant Analysis (LDA) [20] --a supervised classification method -- to categorize sentences as human- or AI-generated using cosine similarity scores. These scores and their respective category labels form our dataset, serving as independent and dependent variables, respectively. The LDA model is trained using sklearn's LinearDiscriminantAnalysis class. The trained model is then used to predict the probability of a sentence in the test set being AI-generated.\n\nTo optimize classification, we explore a range of threshold values from 0 to 1 in a binary system. By assigning samples in datasets HM and MM to categories 0 and 1 respectively, we can conduct the LDA analysis on these two groups of datasets. Consequently, we determine the optimal threshold for classifying human-generated text and AI-generated\n\nFigure 2: Model Architecture for our proposed methodtext aware the accuracy is maximized.\n\n## ", "4 Experimental Evaluation": "\n\nIn our experimental evaluation, we aim to measure the accuracy of our approach in detecting similarities between human and machine-generated answers. We use the Human ChatGPT Comparison Corpus (HC3) dataset, which contains human and ChatGPT-generated answers to the same questions.\n\n### ", "Dataset Preparation": "\n\nFor our analysis, we prepare two datasets to evaluate our model at the sentence and document levels. We use the HC3 dataset for sentence-level evaluation and then we did a summation over sentence-level cosine similarity to get the average similarity for the document. Further, to evaluate generalization performance of our model, we use GPT-wikit-intro dataset (Aaditya Bhat, 2023) for document-level evaluation and comparison with other models.\n\n#### ", "4.1.1 Sentence-level Dataset: HC3": "\n\nWe first use the HC3 dataset, which contains questions and corresponding human and machine responses. The HC3 dataset has an additional machine response for each question, resulting in two machine-generated answers.\n\nNext, we break down each answer for a given question into individual sentences, creating a dataset of roughly 43,000 sentence-level comparisons for machine-machine (MM) and human-machine (HM) categories. We use this dataset to compare the human response to the machine response at the sentence level, as well as compare the machine responses to each other at the sentence level using cosine similarity. Some example cosine similarity values for HM and MM categories are presented in Table 2.\n\nFigure 3 shows the distribution of cosine similarity for HM and MM. For a visual representation of the cosine similarity scores distribution, we generate a Kernel Density Estimation (KDE) plot (Chen, 2017). We also calculate the mean and standard deviation of these scores (see Table 3) for sentence level in HM and MM samples, providing insights into the data. While the mean of the two classes is significantly different, they also have high standard deviations. This dataset is to be used to train and test our LDA model at the sentence level.\n\nTable 4 shows the threshold value used in the LDA classifier and the corresponding accuracy on the test set.\n\n#### ", "4.1.2 Document-Level Dataset: HC3": "\n\nFor a comprehensive understanding, we also conduct a document-level analysis utilizing the HC3 dataset. Rather than dissecting the responses into separate sentences, this level of examination treats the entire response as a single unit.\n\nThe document-level dataset is constructed by averaging the highest cosine similarity scores from the sentence-level comparison within each response. This approach ensures that the most closely matched sentences significantly impact the document-level similarity metric, thereby emphasizing the presence of highly similar sentences in the text. This similarity value serves as the foundation for our LDA model at the document level, allowing for a macroscopic comparison of the machine and human responses.\n\nThe distribution of cosine similarity at the document level is shown in Figure 4. Table 5 provides the mean and standard deviation of cosine similarity scores for HM and MM samples in the document level dataset.\n\n\nTable 2: Example results of cosine similarity (CS) on HM and MM sample with their corresponding categorical label (0,1)\n\nFigure 3: Distribution of cosine similarity at sentence level for HM and MM.\n\n The LDA classifier's threshold value and the corresponding accuracy on the test set for the document-level analysis are presented in Table 6. Observe that, in contrast to sentence level statistics (Table 3), the standard deviation of the two classes under document level comparison (Table 5) are smaller thereby resulting in a more discriminant classifier.\n\n### ", "Experimental Setup": "\n\nUsing the prepared dataset, we conduct a series of experiments to assess the performance of our proposed method in various plagiarism scenarios.\n\nAll the elements from the test set i.e., questions and corresponding student answers, including original and paraphrased questions alongside their corresponding AI-generated answers, are subsequently stored in a vector database, more specifically, Milvus (Wang et al., 2021), an open-source vector database. This step ensures efficient data management, comparison, and high-speed searching of vector data. We incorporate FastText, a module developed by Facebook (Bojanowski et al., 2017), for vector ranking. The vectors representing paraphrased answers are ranked, creating a hierarchy of sentences based on similarity. A vector embedding generator from OpenAI aids in transforming the text into numerical form, allowing machine learning algorithms to process it. This transformation is pivotal for comparing student responses with AI-generated answers.\n\n### ", "Results and Analysis": "\n\nFrom Table 4 and Table 6 we observe that the LDA classifier works better at the document level compared to the sentence level. We next conduct the document level evaluation of our model on the GPT-wiki-intro dataset. This dataset comprises questions along with their corresponding GPT-2 generated introductions and human-written introductions from Wikipedia articles. We perform document-level analysis on the first 100 examples from the GPT-wiki-intro dataset by comparing the AI-generated introductions to the human-written introductions, as well as comparing the AI-generated introductions to each other.\n\nOur evaluation aims to demonstrate the explainability of our tool and its ability to provide both sentence and document level analysis. By comparing our results with existing benchmarks, we highlight the advantages of our approach in detecting plagiarism more effectively and transparently.\n\nOur model is compared with two state-of-the-art (SOTA) models: HC3 and OpenAI's text classifier. In order to evaluate the effectiveness of using the proposed paraphrasing model, we used two versions of our model, a model without paraphrasing (**A**) and a model employing paraphrasing (**B**) on the test set. This allows us to directly assess the impact of paraphrasing on model performance.\n\nConfusion matrices for all the models under evaluation are shown in Table 7. While derived performance metrics (F1 score, precision, and recall) are provided in Table 8. We observe no improvement in model performance with paraphrasing on this data set. We plan to investigate other potential approaches to improve model performance including varying the temperature and P value (OpenAI,\n\n\nTable 4: LDA Model Results: Sentence level\n\nFigure 4: Distribution of cosine similarity at the Document level for HM and MM\n\n\nTable 3: Sentence Level Cosine Similarity Statistics 2023a) of LLM used for answer generation. We also plan to study our model performance on other datasets for a robust evaluation of the value of paraphrasing.\n\nOur model provides the probability of a text being AI-generated, both at sentence and document levels, enhancing transparency for evaluators examining potential plagiarism. For each sentence in a test document - in this case, a student response - the model calculates the probability of that sentence being LLM-generated. When utilizing the Reddit ELI5 (HC3 dataset), our model contrasts the human response with the LLM response on a sentence-by-sentence basis, as demonstrated in Table 9. This added transparency makes it easier for human evaluators to interpret the results and contributes to the elimination of the black-box nature often associated with existing AI text detection methods. To summarize, our method:\n\n* Effectively generates diverse paraphrased questions using an advanced paraphrasing model.\n* Produces accurate and contextually appropriate answers with the state-of-the-art LLM.\n* Provides a comprehensive and transparent sentence-level evaluation, enabling the detection of subtle instances of plagiarism that might be overlooked by traditional methods.\n\n## ", "5 Conclusion": "\n\nIn conclusion, this research presents a novel and effective method for detecting machine-generated text in academic settings, offering a valuable contribution to the field of plagiarism detection. By leveraging a comprehensive comparison technique, our approach provides more accurate and explainable evaluations compared to existing methods. The sentence level quantifiable metrics facilitate easier interpretation for human evaluators, mitigating the black-box nature of existing AI text detection methods.\n\nOur model is adaptable to various NLG models, including cutting-edge LLMs like BardAI and Character.AI, ensuring its relevance and effectiveness as technology continues to evolve. This adaptability makes our approach a significant asset in maintaining academic integrity in the face of rapidly advancing natural language processing technologies.\n\n\nTable 6: LDA Model Results: Document level\n\n\nTable 7: Confusion matrices for SOTA models and our model tested on GPT-wiki-intro dataset. Our model performance on Class 0 is better than both RoBERT-QA and Open AI Classifier, while on Class 1 our performance is better than RoBERTa-QA. Our Model-B uses paraphrasing.\n\n\nTable 8: Document level F1 score, precision, and recall of the models. Our Model-B uses paraphrasing.\n\n\nTable 5: Document Level Cosine Similarity Statistics Future research directions include collecting additional unbiased datasets for evaluation and comparing the performance of our model with other detection tools. We also plan to explore the incorporation of different algorithms at the sentence level, assembling them to achieve even better performance. Moreover, we plan to employ stylometry techniques to identify each student's unique writing style as more data from their responses are collected. This process will create a distinct signature based on the student's writing patterns, making it increasingly easy to detect plagiarism in future submissions.\n\nThese efforts will further refine our model and contribute to the ongoing pursuit of robust, transparent, and adaptable plagiarism detection methods in academia.\n\n## ", "References": "\n\n* (1)\n* Bhat (2023) Aaditya Bhat. 2023. Gpt-wiki-intro (revision 0e458f5).\n* Bojanowski et al. (2017) Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. Enriching word vectors with subword information. _Transactions of the Association for Computational Linguistics_, 5:135-146.\n* Character (2023) Character.AI. 2023. Character.ai. https://beta.character.ai/. Accessed on May 15, 2023.\n* Chat (2023) ChatGPT. 2023. Chatgpt official website. https://openai.com/blog/chatgpt. Accessed on May 15, 2023.\n* Chen (2017) Yen-Chi Chen. 2017. A tutorial on kernel density estimation and recent advances.\n* Duarte (2023) Fabio Duarte. 2023. Number of chatgpt users (2023). https://explodingtopics.com/blog/chatgpt-users. Accessed on May 15, 2023.\n* Fowler (2023) Geoffrey A. Fowler. 2023. We tested a new chatgpt-detector for teachers. it flagged an innocent student. https://www.washingtonpost.com/technology/2023/04/01/chatgpt-cheating-detection-turnitin/.\n* Google (2023) Google. 2023. Bardia. https://blog.google/technology/ai/try-bard/. Accessed on May 15, 2023.\n* GPTZero (2023) GPTZero. 2023. Gptzero official website. https://gptzero.me/. Accessed on May 15, 2023.\n* Guo et al. (2023) Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng Wu. 2023. How close is chatgpt to human experts? comparison corpus, evaluation, and detection.\n* He et al. (2023) Xinlei He, Xinyue Shen, Zeyuan Chen, Michael Backes, and Yang Zhang. 2023. Mgbench: Benchmarking machine-generated text detection.\n* Herbold et al. (2023) Steffen Herbold, Annette Hautli-Janisz, Ute Heuer, Zlata Kikteva, and Alexander Trautsch. 2023. Ai, write an essay for me: A large-scale comparison of human-written versus chatgpt-generated essays.\n* a blessing or a curse for undergraduate computer science students and instructors?\n* Khalil and Erkan (2023) Mohammad Khalil and Erkan Erkan. 2023. Will chatgpt get you caught? rethinking of plagiarism detection.\n* Mitchell et al. (2023) Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D. Manning, and Chelsea Finn. 2023. Detectgpt: Zero-shot machine-generated text detection using probability curvature.\n* OpenAI (2023a) OpenAI. 2023a. Gpt-4 technical report.\n* OpenAI (2023b) OpenAI. 2023b. Openai official website. https://openai.com/. Accessed on May 15, 2023.\n* Roberts and Raffel (2020) Adam Roberts and Colin Raffel. 2020. Exploring transfer learning with T5: the text-to-text transfer transformer. Google AI Blog. Google AI Blog.\n* Solaiman et al. (2019) Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford, Gretchen Krueger, Jong Wook Kim, Sarah Kreps, Miles McCain, Alex Newhouse, Jason Blazakis, Kris McGuffie, and Jasmine Wang. 2019. Release strategies and the social impacts of language models.\n* Tang et al. (2023) Ruixiang Tang, Yu-Neng Chuang, and Xia Hu. 2023. The science of detecting llm-generated texts.\n* Tharwat et al. (2017) Alaa Tharwat et al. 2017. Linear discriminant analysis: A detailed tutorial.\n* Wang et al. (2021) Jianguo Wang, Xiaomeng Yi, Rentong Guo, Hai Jin, Peng Xu, Shengjun Li, Xiangyu Wang, Xiangzhou Guo, Chengming Li, Xiaohai Xu, et al. 2021. Milvus: A purpose-built vector data management system. In _Proceedings of the 2021 International Conference on Management of Data_, pages 2614-2627.\n* Yang et al. (2023) Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Bing Yin, and Xia Hu. 2023. Harnessing the power of llms in practice: A survey on chatgpt and beyond.\n\n \n\n\nTable 9: This table depicts the sentence-level comparison of responses to Question 1 Q1, given by a human H1 and a Large Language Model A1. The cosine similarity values, derived from embeddings, represent the highest similarity between each pair of sentences in the human and LLM responses.\n\n"}, "Texto1.pdf": {"Detecting LLM-Generated Text in Computing Education: A Comparative Study for ChatGPT Cases": "\n\nMichael Sheinman Orenstrakh\n\nOscar Karnalim\nAlthough Artificial Intelligence (AI) can foster education [(9)], it might be misused to breach academic integrity. Paraphrasing tools [(40)] and code obfuscation tools [(2)] for example, are misused to cover up evidence for plagiarism (a breach of academic integrity about copying one's work and reusing it without proper acknowledgment [(14)]).\n\nMisuse of AI chatbots with large language models (LLM) [(6)] such as ChatGPT1 is another trending threat for breaching academic integrity. Students can complete exams or assessments with limited effort, resulting in questionable performance; it is unclear whether the learning objectives are actually met. The misuse can be considered as contract cheating (i.e., getting help in exchange for mutual incentives [(27)]) since AI chatbots provide responses in exchange for additional user data. However, considering AI responses are generated based on other people's textual data without proper acknowledgment, we believe it is more justifiable to consider the misuse as plagiarism.\n\nFootnote 1: https://openai.com/blog/chatgpt\n\nWhile checking student work for plagiarism, instructors are often aided by automated detectors. A number of detectors have been developed to detect whether a work is a result of LLM. Two of them are GPT-2 Output Detector [(50)] and Giant Language model Test Room (GLTR) [(16)]. Nevertheless, due to the recency of misuse of AI chatbots, Computing educators might have limited information about publicly available detection detectors. Further, it is challenging to choose the most suitable detector for their teaching environment. To the best of our knowledge, there are no empirical studies comparing the detectors in terms of effectiveness.\n\nIn response to the aforementioned gaps, we investigate LLM-generated text detectors and formulate the following research question (RQ): \"How effective are LLM-generated text detectors?\"\n\nIt is clear that there is a need in the community to understand if the currently available detectors are able to detect LLM-generated content [(37; 45; 52)] and what there reliability is.\n\nAs an additional contribution, we also report our experience in using the LLM-generated text detectors. It might be useful for readers interested in employing those detectors in their classrooms.\n\n## ", "2. Related Work": "\n\nThis section discusses common breaches of academic integrity in computing education and misuse of AI to breach academic integrity.\n\n### ", "Common Breaches of Academic Integrity": "", "Misuse of AI": " chatbots with large language models (LLM) [(6)] such as ChatGPT1 is another trending threat for breaching academic integrity. Students can complete exams or assessments with limited effort, resulting in questionable performance; it is unclear whether the learning objectives are actually met. The misuse can be considered as contract cheating (i.e., getting help in exchange for mutual incentives [(27)]) since AI chatbots provide responses in exchange for additional user data. However, considering AI responses are generated based on other people's textual data without proper acknowledgment, we believe it is more justifiable to consider the misuse as plagiarism.\n\nFootnote 1: https://openai.com/blog/chatgpt\n\nWhile checking student work for plagiarism, instructors are often aided by automated detectors. A number of detectors have been developed to detect whether a work is a result of LLM. Two of them are GPT-2 Output Detector [(50)] and Giant Language model Test Room (GLTR) [(16)]. Nevertheless, due to the recency of misuse of AI chatbots, Computing educators might have limited information about publicly available detection detectors. Further, it is challenging to choose the most suitable detector for their teaching environment. To the best of our knowledge, there are no empirical studies comparing the detectors in terms of effectiveness.\n\nIn response to the aforementioned gaps, we investigate LLM-generated text detectors and formulate the following research question (RQ): \"How effective are LLM-generated text detectors?\"\n\nIt is clear that there is a need in the community to understand if the currently available detectors are able to detect LLM-generated content [(37; 45; 52)] and what there reliability is.\n\nAs an additional contribution, we also report our experience in using the LLM-generated text detectors. It might be useful for readers interested in employing those detectors in their classrooms.\n\n## 2. Related Work\n\nThis section discusses common breaches of academic integrity in computing education and misuse of AI to breach academic integrity.\n\n### Common Breaches of Academic Integrity\n\nAcademic integrity encourages students to act honestly, trustworthy, respectfully, and responsibly in learning2. Lancaster [(25)] lists five common breaches of academic integrity in computing education: plagiarism, collusion, contract cheating, exam cheating, and research fraud. It is important to inform students about instructors' expectations about academic integrity in their courses [(49)] and penalize those who breach academic integrity.\n\nFootnote 2: https://lo.unisa.edu.au/course/view.php?id=6751&amp.section=6\n\nPlagiarism happens when ideas, words, or even code is reused without proper acknowledgment and permission to the original author(s) [(14)]. It is commonly identified with the help of automated detectors [(3)] such as Turnitin3, Lichen [(38)], MOS4, and JPlag [(39)]. Any submissions with high similarity will be investigated and if they are indeed a result of misconduct, the students will be penalized [(20)].\n\nFootnote 3: https://www.turnitin.com/\n\nNevertheless, identifying plagiarism is not always straightforward; some perpetrators disguise their act with automated paraphrasing [(23; 40)], essay spinning [(26)] or code obfuscation [(2)]. The automated detectors should be resilient to common disguising practices in addition to being effective and efficient. GPlag [(29)] and BPlag [(8)] for examples, focus on content semantic while measuring similarity among submissions. Tahaei and Noelle (2016) and Yan et al. (2017) developed detectors that detect substantial changes among consecutive saves. Ljubovic and Pajic (2017) developed a detector that is automatically integrated to a programming workspace to record any code edits.\n\nCollusion is also about reusing ideas, words, or code without proper acknowledgment. However, the original author(s) is aware about the matter and somewhat allows it (Tahaei and Noelle, 2016). Typically, this occurs when two or more students work closely beyond reasonable levels of collaboration (Sen et al., 2017). Collusion can be identified in the same manner as plagiarism with the help of automated detectors. Similar submissions are reported by the detectors and then manually investigated by the instructors; students whose submissions are indeed a result of misconduct will be penalized.\n\nContract cheating occurs when third parties are paid to complete student assessments (Sen et al., 2017). The third parties can be professional companies or even their colleagues. Contract cheating is quite challenging to identify as the third parties tend to know how to evade detection. It is only identifiable when the writing style and the quality of the submission is substantially different to those of the student's prior submissions. To expedite the identification process, instructors can either use the help of authorship identification detectors (Kumar et al., 2017) such as Turnitin Authorship Investigate5(Tururthy et al., 2017) or check contract cheating sites (Sen et al., 2017).\n\nFootnote 5: https://help.turnitin.com/MicroContent/authorship-investigate.htm\n\nExam cheating happens when some students have unfair advantages in the exams (Sen et al., 2017). The advantages can vary from concealed notes during exams, leaked exam questions, to impersonation (i.e., an individual switch places with a student to take the exam). Exam cheating can be identified via careful investigation on the whole process of the exams. Sometimes, such identification can be aided with online proctoring systems (Turthy et al., 2017) (e.g., Proctorio6 and ProctorExam7) or local monitoring tools (e.g., NetSupport8).\n\nFootnote 6: https://protorio.com/\n\nFootnote 7: https://protortexam.com/\n\nFootnote 8: https://www.netsupportschool.com/\n\nFootnote 9: https://grants.nih.gov/policy/research_integrity/definitions.htm\n\nResearch fraud means reporting research results without verifiable evidence (Sen et al., 2017). It can be data fabrication (i.e., generating artificial data to benefit the students) or data falsification (i.e., updating the data so that it aligns with the students' desired findings). Both are parts of research misconduct9 and they can happen in research-related assessments. Research fraud can be identified via careful investigation on the whole process of research. Due to its complex nature, such misbehaviour is manually identified on most cases. However, instructors can get some help from source metadata (Sen et al., 2017) and automated image manipulation detection (Beng et al., 2017).\n\nFootnote 9: https://protortexam.com/\n\n### Misuse of AI\n\nAI substantially affects education (Beng et al., 2017). It improves student learning experience via the help of intelligent tutoring systems (Sen et al., 2017) and personalized learning materials (Sen et al., 2017). AI expedites the process of providing feedback (Beng et al., 2017), identifying breaches of academic integrity (Sen et al., 2017), maintaining student retention (Beng et al., 2017), learning programming (Sen et al., 2017), creating programming exercises (Sen et al., 2017), and recording attendance (Beng et al., 2017).\n\nAdvances in AI might also be misused for breaching academic integrity. Paraphrasing tools (Sen et al., 2017) which are intended to help students learn paraphrasing are misused to cover up plagiarism. Code generators like GitHub Copilot (Sen et al., 2017) which are intended to help programmers in developing software are misused to complete programming tasks that should be solved independently. Code obfuscation tools (Beng et al., 2017) which are intended to secure code in production are misused to disguise similarities in copied code submissions.\n\n AI chatbots [34], especially those with Large Language Model (LLM) [6] are intended to help people searching information, but they are misused to unethically complete exams10 and assessments11. LLM is derived from Language Model (LM), a statistical model at which each sequence of words are assigned with a probability [11]. Per query or question, the response is generated by concatenating sequences of words that have high probability with the query or the question.\n\nFootnote 10: https://edition.cnn.com/2023/01/26/tech/chatgpt-passes-exarms/index.html\n\nFootnote 11: https://theconversation.com/chatgpt-students-could-use-ai-to-cheat-but-its-a-chance-to-rethink-assessment-altogether-198019\n\nChatGPT is a popular example of LLM. The tool is developed by OpenAI, a non-profit American research laboratory on top of GPT-3, a LLM with deep learning to generate human-like text. The tool relies on reinforcement and supervised learning to further tune the model.\n\nA number of automated detectors have been developed to help instructors identifying AI misuses for breaching academic integrity. In the context of plagiarism and collusion, automated detectors nullify common alterations that can be done without understanding the content [24; 43] and remove contents that are not evident for raising suspicion [48]. In dealing with misuses of AI chatbots, a few automated detectors are developed under the same way as the chatbots via pretrained model, but dedicated to detect AI-generated texts. GPT-2 Output Detector [50] and GLTR [16] are two of the examples.\n\n## ", "3. Methodology": "\n\nThis section discusses how the research question stated in the introduction would be addressed and our preliminary work to discover publicly available LLM-generated text detectors.\n\nWe collected historical assignment data dating back to 2016 from two publicly funded research-focused institutions, one in North America and one in South America. The data collected was from upper-year undergraduate computer science and engineering students.\n\nWe analyzed a total of 164 submissions (124 were submitted by humans, 30 were generated using ChatGPT, and 10 were generated by ChatGPT and altered using the Quillbot paraphrasing tool) and compared them against eight LLM-generated text detectors. This results in a total of \\(1,312\\) prediction results.\n\nOf the 164 submissions, 134 were written in English (20 of which were generated by a LLM, and another 10 which were LLM-generated and paraphrased) and 20 were written in Spanish (10 of which were AI-generated). The submissions were collected between 2016 and 2018 (prior to the release of ChatGPT), and were made in \"databases\", \"networking\", and a \"final thesis project\" course. These courses were specifically selected as they are upper-year computer science major courses that touch on a mix of systems and theory (databases and networking), as well as technical writing in computer science with a programming/development component (final thesis project). The students in these courses were primarily in a computer science major. It should also be noted that Spanish was selected as an alternative language to analyse because it is one of the world's most popular languages, and some of the authors have experience writing and evaluating technical material in this language.\n\nThe assessments analyzed in this study (see Table 1) are taken from three undergrad courses. The first course is a databases course offered to third-year computer science students in their first or second semester. It is a mix of database theory and practical systems application. There are 101 paper submissions from this course which involved a final assessment where students wrote a report analyzing two industry players and their use of databases and data centers, this was written in English.\n\n The second course is a networking course offered to third-year computer science students in their second semester. It is a mix of theoretical concepts and practical system application. There are 13 paper submissions from this course which involved an exam question where students explain how they would implement the NOVEL-SMTP and NEO-SMTP email protocols using only UDP, this was written in English.\n\nThe third course is a final thesis project course offered to fourth-year computer science students throughout their final year of study (across both semesters). It is meant to bridge theory and practice to develop something that can be used/implemented in the real world. There are 10 paper submissions from this course which involved improving computing systems and engineering processes in their local community, this was written in Spanish.\n\nDue to the character limitations, data below 1,000 characters was excluded and data above 2,500 characters was truncated to the last complete sentence. This ensures the input data fits within the range of all detectors. As many LLM-generated text detection platforms have a 2,500 character maximum, to ensure fairness across platform, we used 2,500 characters as our upper-bound.\n\nLLM-generated texts were created with the help of ChatGPT12, a popular LLM. The handouts were parsed to prompts by removing irrelevant information (course code, deadlines, submission instruction) so the prompts only contain the core requirements of the task. These prompts were then fed into ChatGPT to generate a solution to the assignment.\n\nFootnote 12: https://openai.com/blog/chatgpt\n\nIt should be noted, the authors mined through over 2, 000 submissions in programming, data structures and algorithms, and compilers courses, however, the submission data varied too much for the content to easily be extracted and analyzed for detectors. Often due to a lack of context after removing any code. The selected submissions were purely writing-based and did not involve coding components, they did in some cases discuss theoretical concepts in computer science.\n\nFinally, all of the detectors were tested in April 2023.\n\n### ", "Discovering Publicly Available LLM-generated Text Detectors": "\n\nPublicly available LLM-generated text detectors were discovered from January to February 2023 from social media (i.e., Twitter, Facebook, and blogs), online news, and previous literature on LLM-generated text detection (GPT-2, GLTR). Public interest in LLM-generated text detectors followed the release of GPTZero which went viral on January, 2023. After GPTZero, many other companies launched their own LLM-generated text detectors.\n\nA number of LLM-generated text detectors were discovered but we limited this study to LLM-generated text detectors that appear to offer proprietary solutions to LLM-generated text detection. We found that some LLM-generated text detectors are likely to be replicas of open source work (GPT-2) and hence we excluded such detectors from the study.\n\n\nTable 1. Questions analyzed from submissions We identified eight such publicly available LLM-generated text detectors, as shown in Table 2. Two of them (GPT-2 Output Detector and GLTR) are featured with technical reports [16; 50].\n\n**GPT-2 Output Detector**[50] is a LLM-generated text detector based on the RoBERTa large pretrained model [30]. RoBERTa is a transformers model trained on a large corpus of raw English data. The GPT-2 Output Detector starts with the pre-trained ROBERTA-large model and trains a classifier for web data and the GPT-2 output dataset. The GPT-2 Detector returns the probability that an input text is real on GPT-2 text with accuracy of 88% at 124 million parameters and 74% at 1.5 billion parameters [50]. The detector is limited to the first 510 tokens, although there are extensions that extend this limit [36].\n\n**GLTR**[16] is a detector that applies statistical methods to detect GPT-2 text. The model is based on three simple tests: the probability of the word, the absolute rank of a word, and the entropy of the predicted distribution. This detector shows an interface where each word is highlighted along with a top-k class for that word.\n\nThe GLTR detector does not provide quantifiable overall probability that a text is AI-generated. To make a fair comparison between GLTR and other detectors, we define a detector on top of GLTR to make probability predictions using the normal distribution. We compute an average \\(\\mu\\) and a standard deviation \\(\\sigma\\) over a sample dataset of 20 human and 20 ChatGPT submissions. The results were \\(\\mu=35.33\\), and \\(s=15.68\\). We then used those results to normalize a prediction by computing the standard score of a data point \\(x\\) using \\(\\frac{x-\\mu}{s}\\). This score is sent as input to the sigmoid function to obtain a probability prediction.\n\n**GPTZero** was the first detector [54] to claim to detect ChatGPT data. The original version of the detector used two measures: perplexity and burstiness. Perplexity refers to a measurement of how well GPT-2 can predict the next word in the text. This appears similar to the way the GLTR detector works [16]. The second measure is burstiness: the distribution of sentences. The idea is that humans tend to write with bursts of creativity and are more likely to have a mix of short and long sentence. The current version of GPTZero gives four classes of results. Table 3 shows how different classes are interpreted as probability. GPTZero claims an 88% accuracy for human text and 72% accuracy for AI text for this detector [53].\n\n\nTable 2: Discovered publicly available LLM-generated text detectors; model info refers to how detailed the information of the used LLM (complete, partial, and none) \n\n**AI Text Classifier** is OpenAI's 2023 model fine tuned to distinguish between human-written and AI-generated text [35]. The model is trained on text generated from 34 models from 5 different organization. The model provides 5 different categories for the results based on the internal probabilities the model provides. Table 4 shows how different classes are interpreted as probability. The interpretations are based on the final category, not the internal model. Usage of this classifier requires at least 1,000 characters.\n\n**GPTKit** uses an ensemble of 6 other models, including DistilBERT [46], GLTR, Perplexity, PPL, RoBERTa [30], and RoBERTa (base). The predictions of these models are used to form an overall probability that a text is LLM-generated. However, the exact weight used for each of the detectors is unclear. The detector claims an accuracy of 93% based on testing on a dataset of 100K+ responses [15].\n\n**CheckForAI** claims to combine the GPT-2 Output Detector along with custom models to help limit false readings [22]. The detector also supports account sign up, history storage, and file uploads. The detector provides four classes to compute the probability of text, as shown in Table 5. This detector is currently limited to 2,500 characters.\n\n**CopyLeaks** offers products for plagiarism and AI content detection targeted broadly for individuals, educators, and enterprises. The detector highlights paragraphs written by a human and by AI. CopyLeaks also claims detection across multiple languages, including Spanish (tested in this paper). CopyLeaks claims an accuracy of 99.12% [10]. The detector is currently available publicly [10].\n\n\nTable 4: AI Text Classifier interpretation.\n\n\nTable 3: GPTZero accuracy interpretation.\n\n\nTable 5: CheckForAI accuracy interpretation.\n\n\n\n**Originality.AI** is a detector targeted for content publishers. The detector is available through a commercial sign-up page (Krishnan et al., 2018) with a minimum fee $20. We received research access for analysis of the detector. The detector comes with API access and a number of additional features for content creators. A self-proclaimed study by Originality on ChatGPT suggests that the detector has an accuracy of 98.65% (Krishnan et al., 2018).\n\nWe did not impose a systematic approach (Krishnan et al., 2018) to discover publicly available LLM-generated text detectors. Most of the detectors are recent and cannot be easily found on the internet or academic papers. A systematic approach might cover fewer results.\n\n### ", "Addressing the RQ: Effectiveness of LLM-generated text detectors": "\n\nA detector is only worthy of use if it is reasonably effective. We addressed the RQ by comparing detectors listed in Table 2 under three metrics: accuracy, false positives, and resilience. Instructors prefer to use detectors that are reasonably accurate, reporting a minimal number of false positives, and are resilient to disguises.\n\nAccuracy refers to how effective the detectors are in identifying LLM-generated texts. We present all accuracy results using two measures of accuracy, as we have found that using only one measure may mislead about some aspect of the results.\n\nThe first method (averages) takes the average prediction each detector across a dataset. As discussed in the discovery section, each detector either provides a probability that a text is LLM-generated or a category that represents such a probability. We apply our category to AI conversion tables to obtain a probability for each detector. These probabilities are averaged for the final results.\n\nThe second method (thresholds) is calculated as the proportion of correctly-classified LLM-generated texts. These are measured as the number of texts that correctly receive above or below a 50% score out of the total number of texts. This measure is strict, so a prediction of 50% is always considered to be incorrect.\n\nFalse positives are original submissions that are suspected by LLM-generated text detectors. Fewer false positives are preferred. For this metric, we collected student submissions before the release of ChatGPT (2019) and measured their degree of originality with the detectors. Any suspected submissions (originality degree less than 50%) were expected to be false positives.\n\nResilience refers to how good LLM-generated text detectors are in removing disguises. Some students might disguise their LLM-generated texts to avoid getting caught. QuillBot (QuillBot, 2019) is a paraphrasing tool capable of paraphrasing text. The tool uses Artificial Intelligence to reword writing. We paraphrased 10 ChatGPT submissions through QuillBot and measured the results.\n\nIt is worth noting that measuring effectiveness of LLM-generated text detectors is time consuming and labour intensive. Further, some detectors are not supported with API integration; the authors needed to manually copy and paste each test case.\n\n### ", "Summarizing our experience using the LLM-generated text detectors": "\n\nWe also report our experience in using the LLM-generated text detectors. Several aspects are considered: intuitiveness, clarity of documentation, extendability, variety of inputs, quality of reports, number of supported LLM-generated languages, and pricing.\n\n\n\n## ", "4. Results": "\n\nThis section discusses our findings from addressing the research question and our experience using LLM-generated text detectors.\n\n### ", "Addressing the RQ: Effectiveness of LLM-generated Text Detector": "\n\nTable 6 shows accuracy of each detector across human and ChatGPT data using the threshold method. The data shows CopyLeaks to be the most accurate LLM-generated text detector, with an accuracy of 97.06%. CopyLeaks is followed by\n\n\nTable 6. Overall accuracy of LLM-generated text detectors measured using thresholds. Sorted from best to worst.\n\n\nTable 7. Accuracy of LLM-generated text detectors measured using weighted averages. Sorted from best to worst.\n\n\nTable 8. False positive readings on LLM-generated text detectors. Sorted from best to worst.\n\nthe GPT-2 Output Detector/CheckForAI (96.62%), GLTR (88.73%), GPTKit (87.50%), OpenAI's Detector (77.37%), and GPTZero (49.69%).\n\nTable 7 shows the results using averages instead of thresholds. The results show CopyLeaks to provide the best probabilities (99.53%), followed by CheckForAI (96.56%), the GPT-2 Output Detector (96.29%), GPTKit (82.09%), OpenAI's Detector (82%), OriginalityAI (76.63%), GLTR (65.84%), GPTZero (64.47%).\n\n\nTable 10: Resilience against Quillbot (weighted). Sorted from highest to lowest weight.\n\n\nTable 11: Accuracy of LLM-generated text detectors measured using weighted averages (using submissions in Spanish). Sorted from best to worst.\n\n\nTable 9: Resilience against Quillbot (accuracy). Sorted from highest to lowest accuracy.\n\n The data in Tables 6 and 7 are both normally distributed, verified using the Shapiro-Wilk and Kolmogorov-Smirnov tests. Thus, no correction needed to be applied. Overall, from the t-tests (Table 6: \\(t=1.67\\) and \\(p=0.116\\), Table 7: \\(t=1.154\\), \\(p=0.268\\), both with 14 degrees of freedom) we did not find significant differences in the accuracy of LLM-generated text detectors between human and ChatGPT data.\n\nTable 8 shows the false positive results on the human data from the databases and network assignments. GPTKit is the only detector that managed to achieve no false positives across the entire set of human submissions. This is followed by CopyLeaks (1), the GPT-2 Output Detector/CheckForAI (2), OpenAI's detector (6), OriginalityAI (7), GLTR (20), and finally GPTZero (52).\n\nA further investigation of GPTKit, which appears to be the the best detector for avoiding false positives, shows that this detector is still prone to false positives. While none of our original test samples appeared more than 50% fake, we found that some submissions score up to 37% fake from GPTKit. In some cases, removing the last paragraph(s) from these submissions led to a false positive. Figures 1 and 2 show such a case. We note that in this case the output of GPTKit also shows that the detector merged separate paragraphs into a single one. This unexpected merge may contribute to the problem.\n\nTable 9 shows results of 10 ChatGPT papers before and after the Quillbot paraphraser. The results are measured using overall accuracy. The GLTR detector was the most resilient, with none of the predictions changing. It is worth noting that the overall weighted result of GLTR also decreased by 10%, although the change did not effect the accuracy. In contrast, the rest of the detector saw a significant drop following the transformation of Quillbot.\n\n\nTable 12: Overall accuracy of LLM-generated text detectors measured using thresholds (using submissions in Spanish). Sorted from best to worst.\n\nFigure 1: Report introduction with 37% AI Probability on GPTKit.\n\n Figures 3 and 4 show an example of a ChatGPT data point that went from 98% before Quillbot to 5% after Quillbot on Originality.\n\nManuscript submitted to ACM\n\nFigure 4: ChatGPT report plugged to Originality AI after Quillbot .\n\nFigure 3: ChatGPT report plugged to Originality AI before Quillbot .\n\nFigure 2: Truncated introduction with 54% AI Probability on GPTKit.\n\n Tables 11 and 12 show results from the capstone course data, written using Spanish. We found that CopyLeaks and the AI Text Classifier tend always output fake predictions on AI data. In contrast, the GPT-2 Output Detector, GPTZero, CheckForAI, GLTR, GPTKit, and Originality tend to output human predictions.\n\nThe data in Tables 11 and 12 are both normally distributed, verified using the Shapiro-Wilk and Kolmogorov-Smirnov tests. Thus, no correction needed to be applied. Overall, from the t-tests (Table 11: \\(t=1.766\\) and \\(p=0.099\\), Table 12: \\(t=1.862\\), \\(p=0.084\\), both with 14 degrees of freedom) we did not find significant differences in the accuracy of LLM-generated text detectors between human (Spanish text) and ChatGPT (Spanish text) data.\n\nThe GLTR detector shows an interesting mild success with Spanish data. The average top-k score on human data was 104, while the average top-\\(k\\) score on ChatGPT data was 85. When we changed the implementation of GLTR to set a mean of a 94.5 top-k score, GLTR managed to achieve the highest accuracy of 65% on Spanish text.\n\n### ", "Our experience using the LLM-generated text detectors": "\n\nGenerally, many LLM-generated text detectors are intuitive to use. Similar with many online similarity detectors for identifying text plagiarism [(3)]. They have a web-based interface where a user can paste the text they want to check its originality. GPTZero and CheckForAI allow their users to upload a document instead.\n\nWhile there are a number of LLM-generated text detectors, only two of them have their technical reports publicly available (GPT-2 Output Detector [(50)] and GLTR [(16)]). This is possibly due to at least two reasons. First, technical reports might be misused by some individuals to trick the detectors. Second, some detectors are commercial.\n\nMost LLM-generated text detectors do not facilitate API integration. GPTZero, GPTKit, OriginalityAI, CopyLeaks provide such a feature with a fee. Without API integration, it is challenging to integrate the detectors to existing teaching environments, especially learning management system. LLM-generated text detectors are unlikely to be independently used as the task is labor intensive.\n\nAs many of the detectors are commercial, their code is not publicly available. This might complicate instructors to further develop the detectors to fit their particular needs. The only open source detectors are the GPT-2 Output Detection and GLTR.\n\nThe detectors are also limited in the input formats they support. Most of them only allow raw text pasted in a form, making them difficult to automate. The PDF parsers that we attempted to use often parsed in an incorrect order and had a tendency to include unwanted characters. We had to write custom scripts to parse the text in a format that translates all information to text.\n\nDetection results are challenging to interpret. Detectors attempt to combat this problem by highlighting content that is more likely to be AI-generated. Table 13 shows the highlighting support each detector provides. Highlighting is provided on either a paragraph, sentence, or a word basis.\n\nWhile highlighting does seem to mitigate some barriers, we found that the highlighting feature can still be misleading. This was particularly evident in GPTZero, which highlighted 52 human submissions as either possibly or entirely AI-generated. Figure 5 shows a sample human report where some sentences were highlighted as more likely to be written by AI. It is unclear what makes the highlighted text more likely be written by AI than the other sentences.\n\nIn terms of output quality, it seems like the detectors are limited in their ability to export results. Nevertheless, some detectors were more effective than others. We provided screenshots of GPTKit, GPTZero, and Originality in this report since they provided more detailed results and it was easier to screenshot the results along with the text in contrast to the other detectors. It was more challenging to show full results of other detectors as they did not allow side-by-side results.\n\n \n\n## ", "6. Conclusion": "\n\n\nTable 13. Highlighting support per detector.\n\nFigure 5. A false positive using GPTZero.\n\nMost LLM-generated text detectors only support English as the language of LLM-generated text. While one can still send text in other languages, the results do not appear meaningful as we previously showed.\n\nAs many LLM-generated text detectors are commercial and they are relatively new, there appear to mostly individual pricing options. GPTZero CopyLeaks, for instance, have business pricing. GPTZero currently has a subscription plan for business users for $19.99USD per month.\n\nThese detectors might be far less useful for instructors living in countries with weak currency; the pricing options are only available in USD.\n\n## ", "5. Discussion": "\n\nThe current state of LLM-generated text detectors suggests that they are not yet ready to be trusted blindly for academic integrity purposes or as reliable plagiarism detectors such as Turnitin, MOSS, or JPlag. Our study demonstrates that detectors under-perform compared to the GPT-2 Output Detector and GLTR, which are older and freely available detectors from 2019.\n\nAt first glance, it appears that LLM-generated text detectors are fairly accurate with human data being correctly detected \\(\\sim 89.59\\%\\)13 while the average accuracy for ChatGPT-generated data is substantially lower; \\(\\sim 77.42\\%\\)14. Upon deeper inspection, it is apparent that the number of potential false positives can lead to a wide array of issues, especially if being trusted for plagiarism detection at educational institutions.\n\nFootnote 13: this percentage is the average accuracy for human data using Tables 6 and 7.\n\nDelving further, when a paraphraser (in this case, QuillBot) is utilized the average accuracy is slightly reduced for human data \\(\\sim 89.02\\%\\)15 but this substantially reduces the accuracy of ChatGPT-generated data \\(\\sim 49.17\\%\\)16. This means that in more than half of all cases, ChatGPT-generated data cannot correctly be identified by these detectors. Though, some detectors perform better than others (e.g., GLTR), it is still a serious concern for users of these detectors.\n\nFootnote 15: this percentage is the average accuracy for ChatGPT-generated data using Tables 6 and 7.\n\nAdditionally, once non-English languages are introduced, these detectors are easily exacerbated. We investigate submissions made in Spanish and see that the average accuracy for human data lowers to an average of \\(\\sim 70.99\\%\\)17, and ChatGPT-generated data reduces to an abysmal \\(\\sim 17.50\\%\\)18. Though only Spanish was investigated, it introduces the need for additional research into alternative languages (non-English).\n\nFootnote 15: this percentage is the average accuracy for human data using Tables 6 and 7.\n\nPresently, all LLM-generated text detectors struggle with languages other than English, code, and special symbols, resulting in fairly inaccurate results. As a point of clarity, it would be ideal for these detectors to explicitly state their limitations and aim to produce human predictions in such cases.\n\nIn terms of usability, LLM-generated text detectors need some improvements. Although they are intuitive to use and generate acceptable reports, many of them are not well documented at a technical level, some do not have APIs making them more difficult to integrate into local and larger systems (e.g., Learning Management Systems), and the support of these detectors is limited. Furthermore, some of these detectors require processing fees.\n\nFrom our results, LLM-generated text detectors appear to lack in understandability. We are aware that all of these detectors leverage similar large language models for detection purposes. However, they might differ in terms of their technical implementation, parameters, pre-trained data, etc. These are unlikely to be revealed since most of the detectors are for commercial-use and, thus, proprietary. While some detectors highlight sentences that are more likely to be AI-generated (Table 13), the results produced by the detectors are not clear enough for users of these detectors.\n\n## ", "6. Threats to Validity": "\n\nOur study has several threats to validity:\n\n* The findings of the study reflect detector results that are accurate as of April 2023. The detectors are volatile, and owners of these detectors could update their models. Results could change based on updates to LLM-generated text detectors.\n* Accuracy, false positives, and resilience were arguably sufficient to represent effectiveness. However, additional findings can be obtained by considering other effectiveness metrics.\n* The data sets were obtained from two institutions; one uses English as the operational language while another uses Spanish. This means that the findings might not be generalizable to other institutions, especially those with different operational languages.\n* While we believe that the data sets are sufficient to support our findings, we acknowledge that more data sets can strengthen the findings.\n\n## ", "7. Conclusion": "\n\nThis paper examines eight LLM-generated text detectors on the basis of effectiveness. The paper shows that while detectors manage to achieve a reasonable accuracy, they are still prone to flaws and can be challenging to interpret by the human eye. Ultimately, LLM-generated text detectors, while not yet reliable for academic integrity or plagiarism detection, show relatively accurate results for human-generated data compared to ChatGPT-generated data. However, false positives are a significant concern, especially when used for plagiarism detection in educational institutions. When a paraphrasing tool like QuillBot is employed, the accuracy decreases for both human and ChatGPT-generated data. Additionally, the detectors struggle with non-English languages, resulting in even lower accuracy. It is crucial for these detectors to acknowledge their limitations and aim for improved performance in various language contexts.\n\n### ", "Future Work": "\n\nFuture detectors could attempt to incorporate a combination of metrics along with their accuracy for AI detectors. A combination of many factors along with the accuracy and false positive rates may give educators better insights into the predictions. This could include text-based features such as burstiness and repetition as well as AI-learned features such as probabilities. These detectors could further be fine-tuned for specific domains to improve their reliability.\n\nAdditionally, there is a fundamental need to have accurate and understandable LLM-generated text detectors available for educators to combat against the rising concern of academic integrity due to these publicly available LLMs. It is also important for the researchers to contact the creators of these detectors to better understand the related issues and needs of the end users, but also to facilitate a deeper conversation about the functionality and correctness of their instruments.\n\nFinally, there is an apparent need to investigate the use of non-English languages using these detectors as large language models, like the one(s) used by ChatGPT, can produce content in languages other than English.\n\n## ", "References": "\n\n* (1)\n* Alberaki et al. (2021) Balqis Alberaki, Nazar Zaki, and Hany Alashwal. 2021. A Systematic Literature Review of Student' Performance Prediction Using Machine Learning Techniques. _Education Sciences_ 11, 9 (2021). https://doi.org/10.3390/educsci11909552 * Behera and Bhaskari (2015) Chandan Kumar Behera and D Lalitha Bhaskari. 2015. Different obfuscation techniques for code protection. _Procedia Computer Science_ 70 (2015), 757-763.\n* Blanchard et al. (2022) Jeremiah Blanchard, John R. Hott, Vincent Berry, Rebecca Carroll, Bob Edmison, Richard Glassey, Oscar Karnalim, Brian Plancher, and Seain Russell. 2022. Stop Reinventing the Wheel! Promoting Community Software in Computing Education. In _2022 Working Group Reports on Innovation and Technology in Computer Science Education_ (Dublin, Ireland). Association for Computing Machinery, 261-292. https://doi.org/10.1145/3571785.3574129\n* Bucci (2018) Enrico M. Bucci. 2018. Automatic detection of image manipulations in the biomedical literature. _Cell Death & Disease_ 9 (2018). https://doi.org/10.1038/s41419-018-0430-3\n* Image Based Attendance System: A Low Cost Solution to Record Student Attendance in a Classroom. In _2018 IEEE International Symposium on Multimedia (ISM)_ 259-266. https://doi.org/10.1109/ISM.2018.00037\n* Carlini et al. (2021) Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom B Brown, Dawn Song, Ulfar Erlingsson, et al. 2021. Extracting Training Data from Large Language Models.. In _USENIX Security Symposium_, Vol. 6.\n* Cavalcanti et al. (2021) Anderson Pinheiro Cavalcanti, Arthur Barbosa, Ruan Carvalho, Fred Freitas, Yi-Shan Tsai, Dragan Gasevic, and Rafael Ferreira Mello. 2021. Automatic feedback in online learning environments: A systematic literature review. _Computers and Education: Artificial Intelligence_ 2 (2021), 100027. https://doi.org/10.1016/j.caea.2021.100027\n* Cheers et al. (2021) Hayden Cheers, Yuqing Lin, and Shannus P. Smith. 2021. Academic source code plagiarism detection by measuring program behavioral similarity. _IEEE Access_ 9 (2021), 50391-50412. https://doi.org/10.1109/ACCESS.2021.3069367\n* Chen et al. (2020) Lijia Chen, Pingping Chen, and Zhijian Lin. 2020. Artificial Intelligence in Education: A Review. _IEEE Access_ 8 (2020), 75264-75278. https://doi.org/10.1109/ACCESS.2020.2988510\n* CopyLeaks (2023) CopyLeaks. 2023. CopyLeaks AI Content Detector beta. Accessed: 2023-03-01. https://copyleaks.com/ai-content-detector.\n* Croft et al. (2010) W Bruce Croft, Donald Metzler, and Trevor Strohman. 2010. _Search Engines: Information Retrieval in Practice_.\n* Dawson et al. (2020) Phillip Dawson, Wendy Sutherland-Smith, and Mark Ricksen. 2020. Can software improve marker accuracy at detecting contract cheating? A pilot study of the Turnitin authorship investigate alpha. _Assessment & Evaluation in Higher Education_ 45, 4 (2020), 473-482. https://doi.org/10.1080/02602938.2019.1662884\n* Dendir and Maxwell (2020) Seife Dendir and R. Stockton Maxwell. 2020. Cheating in online courses: Evidence from online proctoring. _Computers in Human Behavior Reports_ 2 (2020), 100033. https://doi.org/10.1016/j.chbr.2020.100033\n* Fraser (2014) Robert Fraser. 2014. Collaboration, collusion and plagiarism in computer science coursework. _Informatics in Education_ 13, 2 (2014), 179-195. https://doi.org/10.15388/infelou.2014.10\n* Ganesh (2023) Sachin Ganesh. 2023. GPTKit. Accessed: 2023-03-01. https://quillbot.com/.\n* Gehrmann et al. (2019) Sebastian Gehrmann, Hendrik Strobelt, and Alexander M. Rush. 2019. GLTR: Statistical Detection and Visualization of Generated Text. https://doi.org/10.48550/ARKIV.1906.04043\n* Gillham (2023) Jonathan Gillham. 2023. Can Originality.AI Detect GPT 3, GPT 3.5 And ChatGPT Generated Text? Accessed: 2023-03-01. https://originality.ai/can-gpt-3-5-chatgpt-be-detected.\n* Gillham (2023) Jonathan Gillham. 2023. Originality AI Content Detector. Accessed: 2023-03-01. https://originality.ai/.\n* Kale and Prasad (2017) Sunil Digamberrao Kale and Rajesh Shardanand Prasad. 2017. A systematic review on author identification methods. _International Journal of Rough Sets and Data Analysis (JJRSDA)_ 4, 2 (2017), 81-91.\n* Karnalim et al. (2019) Oscar Karnalim, Simon, and William Chivers. 2019. Similarity detection techniques for academic source code plagiarism and collusion: a review. In _IEEE International Conference on Engineering, Technology and Education_. https://doi.org/10.1109/TALE48000.2019.9225953\n* A systematic literature review. _Information and Software Technology_ 51, 1 (2009), 7-15. https://doi.org/10.1016/j.infsof.2008.09.009\n* Klieger (2023) Benjamin Klieger. 2023. CheckForAI. Accessed: 2023-03-01. https://checkforai.com/.\n* Krishna et al. (2023) Kalpesh Krishna, Yixiao Song, Marzena Karpinska, John Wieting, and Mohit Iyyer. 2023. Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense. arXiv:2303.13408 [cs.CL]\n* Krizkova et al. (2016) Sarka Krizkova, Hana Tomaskova, and Martin Gavalec. 2016. Preference comparison for plagiarism detection systems. In _2016 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)_. IEEE, 1760-1767.\n* Lancaster (2018) Thomas Lancaster. 2018. Academic integrity for computer science instructors. In _Higher Education Computer Science_. 59-71. https://doi.org/10.1007/978-3-319-98590-9_5\n* Lancaster and Clarke (2009) Thomas Lancaster and Robert Clarke. 2009. Automated essay spinning-an initial investigation. In _10 th Annual Conference of the Subject Centre for Information and Computer Sciences_. 25.\n* Lancaster and Clarke (2016) Thomas Lancaster and Robert Clarke. 2016. Contract cheating: The outsourcing of assessed student work. _Handbook of academic integrity_ 1 (2016), 639-654.\n* Li and Wong (2021) Kam Cheong Li and Billy Tak-Ming Wong. 2021. Features and trends of personalised learning: a review of journal publications from 2001 to 2018. _Interactive Learning Environments_ 29, 2 (2021), 182-195. https://doi.org/10.1080/10494820.2020.1811735\n* Liu et al. (2006) Chao Liu, Chen Chen, Jiawei Han, and Philip S Yu. 2006. GPlag: detection of software plagiarism by program dependence graph analysis. In _12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_. 872-881. https://doi.org/10.1145/1150402.1150522 * Liu et al. (2019) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach. https://doi.org/10.48550/ARKIV.1907.11692\n* Ljubovic and Pajic (2020) Vedran Ljubovic and Enil Pajic. 2020. Plagiarism detection in computer programming using feature extraction from ultra-fine-grained repositories. _IEEE Access_ 8 (2020), 96505-96514. https://doi.org/10.1109/ACCESS.2020.2996146\n* Dakhel et al. (2023) Arghavan Moradi Dakhel, Vahid Majdinasab, Amin Nikanjam, Foutse Khomh, Michel C. Desmarais, and Zhen Ming (Jack) Jiang. 2023. GitHub Copilot AI pair programmer: Asset or Liability? _Journal of Systems and Software_ 203 (2023), 111734. https://doi.org/10.1016/j.jss.2023.11734\n* Mousavinasab et al. (2021) Elham Mousavinasab, Nahid Zarifsanaiey, Shararek R. Niakan Kalhori, Mahnaz Rakhshan, Leila Keikha, and Marjan Ghazi Saeedi. 2021. Intelligent tutoring systems: a systematic review of characteristics, applications, and evaluation methods. _Interactive Learning Environments_ 29, 1 (2021), 142-163. https://doi.org/10.1080/10494820.2018.1558257\n* Nicolescu and Tudorache (2022) Luminita Nicolescu and Monica Tcedorac. 2022. Human-Computer Interaction in Customer Service: The Experience with AI Chatbotts& A Systematic Literature Review. _Electronics_ 11, 10 (2022). https://doi.org/10.3390/electronics11101579\n* OpenAI (2023) OpenAI. 2023. AI Text Classifier. Accessed: 2023-01-31. https://beta.openai.com/ai- text-classifier.\n* Orenstrakh (2023) Michael Sheimman Orenstrakh. 2023. How I Improved the GPT2 Output Detector. Accessed: 2023-03-01. https://medium.com/gopenai/how-i-improved-the-gpt2-output-detector-fe4?ba133235.\n* Perkins et al. (2023) Mike Perkins, Jasper Roe, Darius Postma, James McGaughran, and Don Hickerson. 2023. Game of Tones: Faculty detection of GPT-4 generated content in university assessments. _arXiv preprint arXiv:2305.18081_ (2023).\n* Peeveler et al. (2019) Matthew Peeveler, Tushar Gurjar, Evan Macius, Andrew Aikens, Alexander Christoforides, and Barbara Cutler. 2019. Lichen: customizable, open source plagiarism detection in Submitry. In _50th ACM Technical Symposium on Computer Science Education_. https://doi.org/10.1145/3287324.3293867\n* Prechelt et al. (2002) Lutz Prechelt, Guido Malpohl, and Michael Philippsen. 2002. Finding plagiarisms among a set of programs with JPlag. _Journal of Universal Computer Science_ 8, 11 (2002), 1016-1038.\n* Prentice and Kinden (2018) Felicity M. Prentice and Clare E. Kinden. 2018. Paraphrasing tools, language translation tools and plagiarism: an exploratory study. _International Journal for Educational Integrity_ 14 (2018), 11. https://doi.org/10.1007/s40979-018-0036-7\n* Puryear and Sprint (2022) Ben Puryear and Gina Sprint. 2022. Github copilot in the classroom: learning to code with AI assistance. _Journal of Computing Sciences in Colleges_ 38, 1 (2022), 37-47.\n* Quillbot (2023) Quillbot. 2023. Quillbot. Accessed: 2023-03-01. https://quillbot.com/.\n* Raghkitwetsagul et al. (2018) Chaiyong Raghkitwetsagul, Jens Krinke, and David Clark. 2018. A comparison of code similarity analysers. _Empirical Software Engineering_ 23, 4 (2018), 2464-2519. https://doi.org/10.1007/s10664-017-9564-7\n* Rowland et al. (2018) Susan Rowland, Christine Slade, Kai-Sheng Wong, and Brooke Whiting. 2018. 'Just turn to us': the persuasive features of contract cheating websites. _Assessment & Evaluation in Higher Education_ 43, 4 (2018), 652-665. https://doi.org/10.1080/02602938.2017.1391948\n* Sadasivan et al. (2023) Vinu Sankar Sadasivan, Aounon Kumar, Sriram Balasubramanian, Wenxiao Wang, and Soheil Feizi. 2023. Can AI-Generated Text be Reliably Detected? arXiv:2303.11156 [cs.CL]\n* Sanh et al. (2019) Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter https://doi.org/10.48550/ARKIV.1910.01108\n* Volume 1_ (Lugano and Virtual Event, Switzerland) _(ICER '22)_. Association for Computing Machinery, New York, NY, USA, 27-43. https://doi.org/10.1145/3501385.3543957\n* Samon et al. (2020) Simon, Oscar Karnalim, Judy Sheard, Ilor Dema, Amey Karkare, Juho Leinonen, Michael Latt, and Renee McCauley. 2020. Choosing Code Segments to Exclude from Code Similarity Detection. In _ACM Working Group Reports on Innovation and Technology in Computer Science Education_. 1-19. https://doi.org/10.1145/3437800.3439201\n* Simon et al. (2018) Simon, Judy Sheard, Michael Morgan, Andrew Petersen, Amber Settle, and Jane Sinclair. 2018. Informing students about academic integrity in programming. In _20th Australasian Computing Education Conference_. 113-122. https://doi.org/10.1145/3160489.3160502\n* Solaiman et al. (2019) Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford, Gretchen Krueger, Jong Wook Kim, Sarah Kreps, Miles McCain, Alex Newhouse, Jason Blazakis, Kris McGuffie, and Jasmine Wang. 2019. Release Strategies and the Social Impacts of Language Models. , 45 pages. https://d4mucfpskywv.cloudfront.net/papers/GPT_2_Report.pdf\n* Tahaei and Noelle (2018) Narjes Tahaei and David C. Noelle. 2018. Automated plagiarism detection for computer programming exercises based on patterns of resubmission. In _ACM Conference on International Computing Education Research_. 178-186. https://doi.org/10.1145/3230977.3231006\n* Tang et al. (2023) Ruixiang Tang, Yu-Neng Chuang, and Xia Hu. 2023. The Science of Detecting LLM-Generated Texts. arXiv: 2303.07205 [cs.CL]\n* Tian (2023) Edward Tian. 2023. GPTZero Classrooms. Accessed: 2023-03-01. https://gptzero.substack.com/p/gptzero-classrooms.\n* Tian (2023) Edward Tian. 2023. GPTZero Release Tweet. Accessed: 2023-03-01. https://twitter.com/elward_the/status/1610067688449007618.\n* Ullah et al. (2018) Farhan Ullah, Junfeng Wang, Muhammad Farhan, Masood Habib, and Shekhaad Khalid. 2018. Software plagiarism detection in multiprogramming languages using machine learning approach. _Concurrency and Computation: Practice and Experience_ 33, 4 (Oct 2018), e5000. https://doi.org/10.1002/cpe.5000\n* Yamamoto and Lennon (2018) Kentaro Yamamoto and Mary Louise Lennon. 2018. Understanding and detecting data fabrication in large-scale assessments. _Quality Assurance in Education_ 26, 2 (2018), 196-212. https://doi.org/10.1108/QAE-07-2017-0038\n* Yan et al. (2018) Lisa Yan, Nick McKeown, Mehran Sahami, and Chris Piech. 2018. TMOSS: Using intermediate assignment work to understand excessive collaboration in large classes. In _49th ACM Technical Symposium on Computer Science Education_. 110-115. https://doi.org/10.1145/3159450.3159490 "}, "Texto2.pdf": {"DistilBERT: A Novel Approach to Detect Text Generated by Large Language Models (LLM)": "\n\nBV Pranay Kumar (pranaybv4u@gmail.com )\n\nKakatiya University\n\nMD Shaheer Ahmed\n\nChristu Jyothi Institute of Technology and Science\n\nManchala Sadanandam\n\nKakatiya University\n\n###### ", "Abstract": "\n\nWe propose a novel approach to detect text generated by large language models.\n\n\n\n# DistilBERT: A Novel Approach to Detect Text Generated by Large Language Models (LLM)\n\nBV Pranay Kumar\\({}^{1,2}\\)\n\n\\({}^{1}\\)Department of Computer Science and Engineering, Kakatiya University, Warangal, 506009, Telangana, India.\n\n MD Shaheer Ahmed\\({}^{2,3}\\)\n\n\\({}^{1}\\)Department of Computer Science and Engineering, Christu Jyothi Institute of Technology and Science, Jangaon, 506167, Telangana, India.\n\n Manchala Sadanandam\\({}^{1,2}\\)\n\n\\({}^{1}\\)Department of Computer Science and Engineering, Kakatiya University, Warangal, 506009, Telangana, India.\n\n\\({}^{2}\\)Department of Computer Science and Engineering, Christu Jyothi Institute of Technology and Science, Jangaon, 506167, Telangana, India.\n\n\\({}^{3}\\)Department of Computer Science and Engineering, Kakatiya University, Warangal, 506009, Telangana, India.\n\n\\({}^{4}\\)Corresponding author(s). E-mail(s): pranaybv4u@gmail.com;\n\nContributing authors: shaheerhasidea@gmail.com; sadanb4u@gmail.com;\n\n\\({}^{\\dagger}\\)These authors contributed equally to this work.\n\n###### Abstract\n\nLarge language models (LLMs) have emerged as powerful tools for generating human-quality text, raising concerns about their potential for misuse in academic settings. This paper investigates the use of DistilBERT, a distilled version of BERT, for detecting LLM-generated text. We evaluate its performance on two publicly available datasets, LLM-Detect AI Generated Text and DAIGT-V3 Train Dataset, achieving an average accuracy of around 94%. Our findings suggest that DistilBERT is a promising tool for safeguarding academic integrity in the era of LLMs.\n\n**Keywords:** DistilBert, BERT, LLM, Transformer, LLM-generated text, ChatGPT\n\n## ", "1 Introduction": "\n\nThe rapid evolution of Large Language Models (LLMs)[1] presents both remarkable opportunities and significant challenges for academic settings, including higher education and research. With their fluency and factuality, LLMs hold promise for tasks like generating content, summarizing research, and automating administrative processes[2]. However, their ability to mimic human writing raises concerns about potential misuse in fields like scientific authorship, where authenticity and integrity are paramount. For educators, distinguishing student-generated text from LLM outputs becomes crucial in ensuring the validity of assignments and assessments.\n\nAddressing these concerns lies in developing reliable methods for detecting LLM-generated text. Among the popular LLMs, the Bidirectional Encoder Representations from Transformers (BERT) [3] model stands out for its high performance in various natural language processing tasks. However, its immense size and computational complexity limit its widespread adoptionin real-time applications. This is where DistilBERT [4] emerges as a compelling alternative. A smaller, faster, and more resource-efficient version of BERT, DistilBERT inherits its predecessor's capabilities while offering broader accessibility and lower deployment costs [5].\n\nThis research investigates the potential of DistilBERT in tackling the crucial task of LLM generated text detection. We delve into evaluating its performance against human-written and LLM-generated text, focusing on its ability to accurately discern the origin of a given text sample. The findings aim to shed light on DistilBERT's suitability as a practical solution for safeguarding academic integrity and empowering educators in a landscape increasingly influenced by advanced language models.\n\n### ", "Concerns with LLM generated text": "\n\nWhile LLMs like ChatGPT [6], Bard [7], and Claude [8] offer mesmerizing capabilities, their very strengths pose inherent dangers, particularly in academic settings. These models excel at generating text that is not only grammatically correct and stylistically cohesive but also often infused with factual details gleaned from vast datasets. This ability to \"hallucinate [9]\" knowledge, weave intricate narratives, and even engage in self-referential loops, aptly dubbed the \"Curse of Recursion[10],\" presents a two-fold threat:\n\n**1. Deception and Plagiarism [11]**: The ease with which LLMs can mimic human writing opens the door to a wave of academic dishonesty. Imagine an essay composed entirely by an LLM, seamlessly integrated with citations and seemingly backed by factual evidence. Unwary educators and plagiarism detection systems might struggle to identify such fabrications, potentially undermining the entire foundation of academic trust and intellectual merit.\n\n**2. Erosion of Critical Thinking [12]**: The abundance of readily available, machine-generated \"knowledge\" risks fostering a culture of intellectual dependence. Students accustomed to relying on LLMs for summaries, research assistance, and even essay writing might lose the crucial skills of critical evaluation, independent thought, and original argumentation. This could lead to a generation ill-equipped to navigate the complexities of information overload and discern truth from fiction.\n\nTherefore, developing robust methods for detecting LLM-generated text is not just a technological challenge but an ethical imperative. It is about safeguarding the very essence of academic endeavor - the pursuit of genuine understanding, honest inquiry, and the independent construction of knowledge. DistilBERT, with its potential for efficient and accurate LLM detection, emerges as a promising tool in this crucial fight. By empowering educators and upholding academic integrity, we can ensure that the transformative power of language models is harnessed for good, fostering a future where technology augments human intellect rather than supplants it.\n\n## ", "2 Experimental Setup": "\n\n### ", "Datasets": "\n\nTwo datasets were utilized in this research: the \"LLM - Detect AI Generated Text\" dataset and the \"DAIGT-V3 Train Dataset\".\n\nThe \"LLM - Detect AI Generated Text\" dataset, available on Kaggle, comprises a collection of essays, some authored by students and others generated by various large language models (LLMs). The objective of the dataset is to identify whether a particular essay was produced by an LLM. It includes around 10,000 essays, all penned in response to one of seven essay prompts. The essays from two prompts form the training set, while the rest make up the hidden test set. Almost all training set essays were written by students, with only a few LLM-generated essays provided as examples.\n\nThe \"LLM - Detect AI Generated Text\" dataset includes three CSV files:\n\n* **train_prompts.csv**: Contains prompts used to generate the essays in the training set.\n* **train_essays.csv**: Contains 1378 unique values and four columns ('id', 'prompt_id', 'text', 'generated'). The 'generated' column indicates whether the text was generated by an LLM.\n* **test_essays.csv**: Contains three columns ('id', 'prompt_id', 'text').\n\nOn the other hand, the \"DAIGT-V3 Train Dataset\", accessible on Kaggle, is designed to train and evaluate models for detecting LLM-generated text. This dataset includes 20,000 human-written essays and 20,000 LLM-generated essays. Like the \"LLM - Detect AI Generated Text\" dataset, the essays in this dataset are written in response to a variety of prompts, which are also provided in the dataset. The dataset includes two CSV files:\n\n* **test_v3_drcat_01.csv**: Contains five columns ('text', 'label', 'prompt_name', 'source', 'RDizzl3_seven').\n* **train_v3_drcat_02.csv**: Contains six columns ('text', 'label', 'prompt_name', 'source', 'RDizzl3_seven', 'model').\n\nBoth datasets provide valuable data for training and evaluating models capable of distinguishing between human-written and AI-generated text.\n\n### ", "Software Setup": "\n\nThe proposed methods of this study were implemented using Python programming language. The proposed method was implemented using Hugging face transformers class embedded in Python. The libraries used and their respective versions are TensorFlow version 2.12.0, Keras version 0.1.7, KerasNLP version 0.6.1, NumPy version 1.23.5, Pandas version 2.0.3, scikit-learn version 1.1.3, matplotlib version 3.7.3, and seaborn version 0.14.1.\n\n### ", "Data Preprocessing": "\n\nData preprocessing is a vital stage in any machine learning endeavor, particularly when dealing with text data. It involves cleaning the data and preparing it for the model. In the realm of Natural Language Processing (NLP)[13], text data can contain noise in various forms such as emotions, punctuation, and text in different cases.\n\nFor the \"LLM - Detect AI Generated Text\" and \"DAIGT-V3 Train Dataset\", several preprocessing steps were undertaken. Initially, all the text was converted to lowercase to ensure uniformity and prevent duplication due to case differences. Subsequently, punctuation marks and numbers were removed from the text as they often do not contribute meaningful information for the task at hand.\n\nNext, common words, known as stopwords, that do not carry much meaning were eliminated. These often include words like 'is', 'the', 'and', etc. Following this, techniques known as stemming and lemmatization were used to reduce words to their root form. For instance, 'running' might be reduced to 'run'. This step aids in grouping similar words together. Lastly, extra white spaces in the text were removed to clean up the text.\n\nBy applying these preprocessing steps, the text data was made more suitable for the subsequent machine learning model, thereby enhancing the model's ability to extract useful features from the text and improve its predictive performance\n\n## ", "3 Methodology": "", "Models": " (LLM)\n\nBV Pranay Kumar (pranaybv4u@gmail.com )\n\nKakatiya University\n\nMD Shaheer Ahmed\n\nChristu Jyothi Institute of Technology and Science\n\nManchala Sadanandam\n\nKakatiya University\n\n###### Abstract\n\nWe propose a novel approach to detect text generated by large language models.\n\n\n\n# DistilBERT: A Novel Approach to Detect Text Generated by Large Language Models (LLM)\n\nBV Pranay Kumar\\({}^{1,2}\\)\n\n\\({}^{1}\\)Department of Computer Science and Engineering, Kakatiya University, Warangal, 506009, Telangana, India.\n\n MD Shaheer Ahmed\\({}^{2,3}\\)\n\n\\({}^{1}\\)Department of Computer Science and Engineering, Christu Jyothi Institute of Technology and Science, Jangaon, 506167, Telangana, India.\n\n Manchala Sadanandam\\({}^{1,2}\\)\n\n\\({}^{1}\\)Department of Computer Science and Engineering, Kakatiya University, Warangal, 506009, Telangana, India.\n\n\\({}^{2}\\)Department of Computer Science and Engineering, Christu Jyothi Institute of Technology and Science, Jangaon, 506167, Telangana, India.\n\n\\({}^{3}\\)Department of Computer Science and Engineering, Kakatiya University, Warangal, 506009, Telangana, India.\n\n\\({}^{4}\\)Corresponding author(s). E-mail(s): pranaybv4u@gmail.com;\n\nContributing authors: shaheerhasidea@gmail.com; sadanb4u@gmail.com;\n\n\\({}^{\\dagger}\\)These authors contributed equally to this work.\n\n###### Abstract\n\nLarge language models (LLMs) have emerged as powerful tools for generating human-quality text, raising concerns about their potential for misuse in academic settings. This paper investigates the use of DistilBERT, a distilled version of BERT, for detecting LLM-generated text. We evaluate its performance on two publicly available datasets, LLM-Detect AI Generated Text and DAIGT-V3 Train Dataset, achieving an average accuracy of around 94%. Our findings suggest that DistilBERT is a promising tool for safeguarding academic integrity in the era of LLMs.\n\n**Keywords:** DistilBert, BERT, LLM, Transformer, LLM-generated text, ChatGPT\n\n## 1 Introduction\n\nThe rapid evolution of Large Language Models (LLMs)[1] presents both remarkable opportunities and significant challenges for academic settings, including higher education and research. With their fluency and factuality, LLMs hold promise for tasks like generating content, summarizing research, and automating administrative processes[2]. However, their ability to mimic human writing raises concerns about potential misuse in fields like scientific authorship, where authenticity and integrity are paramount. For educators, distinguishing student-generated text from LLM outputs becomes crucial in ensuring the validity of assignments and assessments.\n\nAddressing these concerns lies in developing reliable methods for detecting LLM-generated text. Among the popular LLMs, the Bidirectional Encoder Representations from Transformers (BERT) [3] model stands out for its high performance in various natural language processing tasks. However, its immense size and computational complexity limit its widespread adoptionin real-time applications. This is where DistilBERT [4] emerges as a compelling alternative. A smaller, faster, and more resource-efficient version of BERT, DistilBERT inherits its predecessor's capabilities while offering broader accessibility and lower deployment costs [5].\n\nThis research investigates the potential of DistilBERT in tackling the crucial task of LLM generated text detection. We delve into evaluating its performance against human-written and LLM-generated text, focusing on its ability to accurately discern the origin of a given text sample. The findings aim to shed light on DistilBERT's suitability as a practical solution for safeguarding academic integrity and empowering educators in a landscape increasingly influenced by advanced language models.\n\n### Concerns with LLM generated text\n\nWhile LLMs like ChatGPT [6], Bard [7], and Claude [8] offer mesmerizing capabilities, their very strengths pose inherent dangers, particularly in academic settings. These models excel at generating text that is not only grammatically correct and stylistically cohesive but also often infused with factual details gleaned from vast datasets. This ability to \"hallucinate [9]\" knowledge, weave intricate narratives, and even engage in self-referential loops, aptly dubbed the \"Curse of Recursion[10],\" presents a two-fold threat:\n\n**1. Deception and Plagiarism [11]**: The ease with which LLMs can mimic human writing opens the door to a wave of academic dishonesty. Imagine an essay composed entirely by an LLM, seamlessly integrated with citations and seemingly backed by factual evidence. Unwary educators and plagiarism detection systems might struggle to identify such fabrications, potentially undermining the entire foundation of academic trust and intellectual merit.\n\n**2. Erosion of Critical Thinking [12]**: The abundance of readily available, machine-generated \"knowledge\" risks fostering a culture of intellectual dependence. Students accustomed to relying on LLMs for summaries, research assistance, and even essay writing might lose the crucial skills of critical evaluation, independent thought, and original argumentation. This could lead to a generation ill-equipped to navigate the complexities of information overload and discern truth from fiction.\n\nTherefore, developing robust methods for detecting LLM-generated text is not just a technological challenge but an ethical imperative. It is about safeguarding the very essence of academic endeavor - the pursuit of genuine understanding, honest inquiry, and the independent construction of knowledge. DistilBERT, with its potential for efficient and accurate LLM detection, emerges as a promising tool in this crucial fight. By empowering educators and upholding academic integrity, we can ensure that the transformative power of language models is harnessed for good, fostering a future where technology augments human intellect rather than supplants it.\n\n## 2 Experimental Setup\n\n### Datasets\n\nTwo datasets were utilized in this research: the \"LLM - Detect AI Generated Text\" dataset and the \"DAIGT-V3 Train Dataset\".\n\nThe \"LLM - Detect AI Generated Text\" dataset, available on Kaggle, comprises a collection of essays, some authored by students and others generated by various large language models (LLMs). The objective of the dataset is to identify whether a particular essay was produced by an LLM. It includes around 10,000 essays, all penned in response to one of seven essay prompts. The essays from two prompts form the training set, while the rest make up the hidden test set. Almost all training set essays were written by students, with only a few LLM-generated essays provided as examples.\n\nThe \"LLM - Detect AI Generated Text\" dataset includes three CSV files:\n\n* **train_prompts.csv**: Contains prompts used to generate the essays in the training set.\n* **train_essays.csv**: Contains 1378 unique values and four columns ('id', 'prompt_id', 'text', 'generated'). The 'generated' column indicates whether the text was generated by an LLM.\n* **test_essays.csv**: Contains three columns ('id', 'prompt_id', 'text').\n\nOn the other hand, the \"DAIGT-V3 Train Dataset\", accessible on Kaggle, is designed to train and evaluate models for detecting LLM-generated text. This dataset includes 20,000 human-written essays and 20,000 LLM-generated essays. Like the \"LLM - Detect AI Generated Text\" dataset, the essays in this dataset are written in response to a variety of prompts, which are also provided in the dataset. The dataset includes two CSV files:\n\n* **test_v3_drcat_01.csv**: Contains five columns ('text', 'label', 'prompt_name', 'source', 'RDizzl3_seven').\n* **train_v3_drcat_02.csv**: Contains six columns ('text', 'label', 'prompt_name', 'source', 'RDizzl3_seven', 'model').\n\nBoth datasets provide valuable data for training and evaluating models capable of distinguishing between human-written and AI-generated text.\n\n### Software Setup\n\nThe proposed methods of this study were implemented using Python programming language. The proposed method was implemented using Hugging face transformers class embedded in Python. The libraries used and their respective versions are TensorFlow version 2.12.0, Keras version 0.1.7, KerasNLP version 0.6.1, NumPy version 1.23.5, Pandas version 2.0.3, scikit-learn version 1.1.3, matplotlib version 3.7.3, and seaborn version 0.14.1.\n\n### Data Preprocessing\n\nData preprocessing is a vital stage in any machine learning endeavor, particularly when dealing with text data. It involves cleaning the data and preparing it for the model. In the realm of Natural Language Processing (NLP)[13], text data can contain noise in various forms such as emotions, punctuation, and text in different cases.\n\nFor the \"LLM - Detect AI Generated Text\" and \"DAIGT-V3 Train Dataset\", several preprocessing steps were undertaken. Initially, all the text was converted to lowercase to ensure uniformity and prevent duplication due to case differences. Subsequently, punctuation marks and numbers were removed from the text as they often do not contribute meaningful information for the task at hand.\n\nNext, common words, known as stopwords, that do not carry much meaning were eliminated. These often include words like 'is', 'the', 'and', etc. Following this, techniques known as stemming and lemmatization were used to reduce words to their root form. For instance, 'running' might be reduced to 'run'. This step aids in grouping similar words together. Lastly, extra white spaces in the text were removed to clean up the text.\n\nBy applying these preprocessing steps, the text data was made more suitable for the subsequent machine learning model, thereby enhancing the model's ability to extract useful features from the text and improve its predictive performance\n\n## 3 Methodology\n\n### Models\n\nThe primary model used in this research is DistilBERT, specifically the **distil_bert_base_uncased** variant. DistilBERT is a distilled version of BERT (Bidirectional Encoder Representations from Transformers), a transformer-based machine learning technique for natural language processing tasks.\n\nBERT is a powerful language model that has significantly improved the state-of-the-art on many Natural Language Processing tasks. However, it is also quite large, with models containing billions of parameters [14] and being trained on massive datasets. As a result, using BERT for production applications can be challenging due to the high computational requirements for training and inference.\n\nFigure 1: Architecture of DistilBERT\n\nThe architecture of DistilBERT is shown in Figure 1. DistilBERT addresses these issues by creating a smaller, faster, and cheaper version of BERT. It achieves this by using a process known as distillation, where a larger model (the teacher) transfers its knowledge to a smaller model (the student). The student model is trained to mimic the output of the teacher model, thus retaining most of its performance while reducing its size and computational requirements.\n\nThe **distil_bert_base_uncased** variant of DistilBERT is pre-trained on the same corpus as the BERT base model in a self-supervised manner. This means it was trained on raw texts without any human labeling, using an automatic process to generate inputs and labels from those texts using the BERT base model. It was trained with three objectives: distillation loss, masked language modeling (MLM) [15], and cosine embedding loss.\n\nCompared to BERT, DistilBERT retains more than 95% of the performance of BERT while having 40% fewer parameters. In terms of inference time, DistilBERT is more than 60% faster and requires 40% less memory than BERT. These advantages make DistilBERT a highly effective choice for many NLP tasks, especially in scenarios where computational resources are limited.\n\nThe Figure 2 depicts the DistilBERT classification process for identifying whether a given text was written by a human or generated by a machine. First, the input text is divided into individual tokens (words or sub-words) and then converted into numerical representations called embeddings. These embeddings are then fed into multiple attention layers that analyze the relationships between the tokens. Finally, a classification layer determines the final outcome, classifying the text as either human-written or machine-generated.\n\n### ", "Metrics of Evaluation": "\n\nThe performance of the models was evaluated using four key metrics: accuracy, precision, recall, and the F1 score. These metrics were calculated based on the True Positive (TP), True Negative (TN), False Positive (FP), and False Negative (FN) values derived from the model's predictions.\n\n**Accuracy** was computed as the ratio of correct predictions (both true positives and true negatives) to the total number of instances. This gives us an overall measure of how often the model is correct in its predictions.\n\n\\[\\text{Accuracy}=\\frac{TP+TN}{TP+TN+FP+FN}\\] (1)\n\n**Precision** was defined as the proportion of true positive predictions out of all positive predictions. It provides a measure of the model's ability to correctly identify positive instances.\n\n\\[\\text{Precision}=\\frac{TP}{TP+FP}\\] (2)\n\n**Recall**, also known as sensitivity, measures the proportion of actual positive instances that were correctly identified. It provides a measure of the model's ability to correctly identify positive instances.\n\n\\[\\text{Recall}=\\frac{TP}{TP+FN}\\] (3)\n\nFigure 2: Flow diagram of DistilBERT\n\nThe **F1 score** is the harmonic mean of precision and recall, giving equal weight to both metrics. It ranges from 0 to 1, with 1 indicating perfect precision and recall, and 0 indicating poor performance. The F1 score is especially useful when dealing with imbalanced datasets, as it takes both false positives and false negatives into account.\n\n\\[\\text{F1 Score}=\\frac{2*(\\text{Precision}*\\text{Recall})}{(\\text{Precision}+ \\text{Recall})}\\] (4)\n\nThese metrics provide a comprehensive evaluation of the model's performance. By considering both precision and recall, the F1 score offers a balance between these two metrics, making it a better choice than accuracy when dealing with imbalanced datasets.\n\n## ", "4 Results and Discussion": "\n\nIn this section, we present the results of the DistilBERT in Detecting Large Language Model(LLM) Generated Text.\n\n### ", "Confusion Matrix": "\n\nThe performance of a classification model for a DistilBERT is expressed in the form a confusion matrix.\n\n#### ", "4.1.1 Test Set Confusion Matrix": "\n\nFigure 3 displays the confusion matrix for the test set, evaluating the model's performance on **previously unseen data**. This matrix provides detailed insights into **true positives, true negatives, false positives, and false negatives**, offering deeper understanding of the model's accuracy and potential biases.\n\n#### ", "4.1.2 Validation Set Confusion Matrix": "\n\nFigure 4 presents the Validation Set Confusion Matrix, offering insights into the model's performance on unseen data during training. It analyzes how accurately the model predicted each class, revealing strengths and weaknesses for further evaluation.\n\n### ", "Classification Report": "\n\n#### ", "4.2.1 Test Set Classification Report": "\n\nTable 1 presents the **Classification Report for the Test Set**, offering a detailed breakdown of the model's performance on unseen data.\n\n#### ", "4.2.2 Validation Set Classification Report": "\n\n**Table 3** delves into the model's **Validation Set Classification Report**, providing a detailed analysis of its performance on held-out data.\n\nFigure 4: Validation Set Confusion Matrix\n\nFigure 3: Test Set Confusion Matrix\n\n\n\n### ", "Model Performance Analysis": "\n\n#### ", "4.3.1 Confusion Matrix Analysis": "\n\nOn the test set, the model achieved an overall accuracy of 93%. The model was more accurate at distinguishing between human-written text and AI-generated text, with a precision of 91% for human-written text and a recall of 84%. For AI-generated text, the precision was 89% and the recall was 100%. This suggests that the model tends to misclassify human-written text as AI-generated more frequently than the other way around.\n\nThe F1-score, which is the harmonic mean of precision and recall, was 0.91 for human-written text and 0.94 for AI-generated text. This indicates that the model performs slightly better at classifying AI-generated text than human-written text.\n\nOn the validation set, the model achieved an F1 score of 0.95, indicating excellent performance. The confusion matrix for this set showed 6561 true positives, 24 false positives, 1207 false negatives, and 10689 true negatives. These figures suggest that the model correctly identified a majority of instances, with only a small number of false positives and false negatives.\n\nIn conclusion, the confusion matrix analysis reveals that the model performs well in both the test and validation sets. However, it is particularly effective at classifying AI-generated text. Further work could be done to improve the model's performance on human-written text, especially in terms of reducing the number of false negatives\n\n#### ", "4.3.2 Classification Report Analysis": "\n\nThe model had an overall accuracy of 93% on the test set. For class 0, the model correctly identified all instances (precision = 1.00), but it missed some instances where it should have predicted class 0 (recall = 0.84). For class 1, the model was slightly less accurate at predicting class 1 than class 0, but it did not miss any instances where it should have predicted class 1.\n\nOn the validation set, the model also had an overall accuracy of 93%. The precision, recall, and F1-score for class 0 were similar to the test set. For class 1, the model was slightly less accurate at predicting class 1 than class 0, but it did not miss any instances where it should have predicted class 1.\n\nIn summary, the classification reports confirm the findings of the confusion matrices. The model performs well overall, but there is room for improvement in its performance on class 1, particularly in terms of recall\n\n### ", "Implications": "\n\nThe results of this study provide valuable insights into the performance of DistilBert in detecting Large Language Model (LLM) generated text. The model demonstrated high accuracy rates on both the test and validation sets, indicating its effectiveness in distinguishing between human-written text and AI-generated text.\n\nHowever, the model's performance was not uniform across all classes. While it was highly accurate at classifying human-written text, it struggled slightly more with classifying AI-generated text. This could imply that the model may benefit from further training or tuning to improve its performance on class 1, particularly in terms of recall.\n\nGiven the slight underperformance on AI-generated text, future work could focus on improving the model's ability to accurately classify this type of text. This could involve augmenting the training data with more examples of AI-generated text or adjusting the model's hyperparameters.\n\n\nTable 1: Test Set Classification Report\n\n\nTable 2: Validation Set Classification ReportAs the model becomes more accurate in distinguishing between human-written and AI-generated text, users can gain greater trust in the information they receive. This could be particularly useful in fields like journalism, where it's crucial to distinguish between credible news sources and fake news generated by bots.\n\nThe ability of DistilBERT to detect LLM-generated text can be used to ensure the quality of datasets used for various applications. For instance, in social media monitoring, this capability can help filter out bot-generated posts, thereby enhancing the reliability of data used for sentiment analysis or trend identification.\n\n### ", "Limitations": "\n\nAI models like DistilBERT, when used to classify whether a text is AI-generated or human-written, face several limitations:\n\n1. **Short Texts:** These models are very unreliable on short texts (below 1,000 characters). Even longer texts can sometimes be incorrectly labeled by the classifier.\n2. **Mislabeling:** Sometimes human-written text will be incorrectly but confidently labeled as AI-written by these classifiers.\n3. **Language Limitations:** These models perform significantly worse in languages other than English and are unreliable on code.\n4. **Predictable Text:** Text that is very predictable cannot be reliably identified. For example, it is impossible to predict whether a list of the first 1,000 prime numbers was written by AI or humans, because the correct answer is always the same.\n5. **Evasion:** AI-written text can be edited to evade the classifier. Classifiers like these can be updated and retrained based on successful attacks, but it is unclear whether detection has an advantage in the long-term.\n6. **Calibration Issues:** Classifiers based on neural networks [16] are known to be poorly calibrated outside of their training data. For inputs that are very different from text in their training set, the classifier is sometimes extremely confident in a wrong prediction.\n7. **Manipulation:** AI-generated text can be manipulated to escape detection. The tool may not perform as well with text written by children or in languages other than English, as its primary training was based on adult-written English content.\n8. **Machine Translation:** Influence of machine translation can lead to reduced accuracy. Machine translation leaves some traces of AI in the output, even if the original was purely human-written.\n9. **Human Manual Editing:** Cases where human-generated text is edited by a human for the purpose of avoiding detection are almost undetectable by current tools.\n10. **Machine Paraphrase:** Use of AI to transform AI-generated text results in text that the classifiers consider human-written. Most AI-generated texts remain undetected when machine-paraphrased.\n\n## ", "5 Conclusion": "\n\nThis research investigated the performance of DistilBERT in detecting LLM-generated text. We conducted experiments on two publicly available datasets, the LLM-Detect AI Generated Text dataset and the DAIGT-V3 Train Dataset, and evaluated the model's performance using four key metrics: accuracy, precision, recall, and the F1 score.\n\nThe results of the experiments showed that DistilBERT achieved high performance in detecting LLM-generated text. The model achieved an average accuracy of around 94% on the LLM-Detect AI Generated Text dataset and the DAIGT-V3 Train Dataset. Additionally, the model achieved high precision, recall, and F1 scores on both datasets.\n\nThese results suggest that DistilBERT is a promising tool for detecting LLM-generated text. The model's high performance and efficiency make it a viable option for a variety of applications, such as academic integrity assessment and content moderation. We also explored the impact of different hyperparameter settings on the model's performance. It is found that the model was most sensitive to the number of layers and the dropout rate. However, even with a small number of layers and a moderate dropout rate, the model still achieved high performance.\n\nOverall, the research demonstrates that DistilBERT is a powerful and effective tool for detecting LLM-generated text. The model's high performance and efficiency make it a promising option for a variety of applications.\n\n### ", "Future Work": "\n\nFuture research directions for evaluating DistilBert's performance in detecting Large Language Model (LLM) generated text could include the following:\n\n**Exploring Alternative Models:** While DistilBert has proven effective, comparing its performance against other prominent models like RoBERTa [17], ALBERT [18], or T5 [19] would offer a wider perspective on architectural suitability for this task. This broader understanding would guide us towards the most effective models for distinguishing human-written and AI-generated text.\n\n**Expanding Linguistic Reach:** The current focus on English language evaluation presents an opportunity for expansion. Future research can assess DistilBert's performance across diverse languages, gauging its ability to identify AI-generated text in multilingual settings. This would significantly enhance its real-world applicability.\n\n**Tackling Longer Sequences:** As the length of analyzed text sequences increases, DistilBert's performance might decline. Investigating its handling of longer sequences and devising methods to improve its performance in such scenarios is crucial for ensuring its effectiveness in diverse use cases.\n\n**Bridging the Gap to Real-World Applications:** Existing research often leans towards theoretical exploration. Future studies can delve deeper into real-world applications such as plagiarism detection or fake news identification, demonstrating the practical impact of these models and guiding their development towards tangible solutions.\n\n**Enhancing Model Interpretability**: Understanding the rationale behind a model's classification decisions can be invaluable. Future research on model interpretability can not only provide insights into the decision-making process but also potentially lead to performance improvements.\n\n**Combining Techniques:** Exploring the combination of DistilBert with other techniques like rule-based systems or other machine learning models holds promise for further performance gains. This synergistic approach could leverage the strengths of different methods to create a more robust and accurate system for LLM detection.\n\nBy pursuing these diverse research directions, we can refine and expand the capabilities of DistilBert and other models, ultimately unlocking their full potential for distinguishing human-written and AI-generated text across various languages and real-world applications.\n\n## ", "Data Availability Statement": "\n\nThe datasets analyzed during the current study are available in the following repositories:\n\n* Detect AI Generated Text: https://www.kaggle.com/competitions/ llm-detect-ai-generated-text/data\n* DAIGT-V3 Train Dataset: https://www.kaggle. com/datasets/thedrcat/daigt-v3-train-dataset\n\nAccess to these datasets may be subject to specific terms and conditions. Please refer to the respective repository pages for more details and instructions on how to obtain access.\n\n## ", "References": "\n\n* [1] Kim, J. K., Chua, M., Rickard, M. & Lorenzo, A. J. Chatgpt and large language model (llm) chatbots: The current state of acceptability and a proposal for guidelines on utilization in academic medicine. _Journal of Pediatric Urology_**19**, 598-604 (2023).\n* [2] Jungherr, A. Using chatgpt and other large language model (llm) applications for academic paper assignments (2023). URL https://doi.org/10.31235/osf.io/d84q6.\n* [3] Devlin, J., Chang, M., Lee, K. & Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding (2018). URL https://arxiv.org/pdf/1810.04805v2.\n* [4] Sanh, V., Debut, L., Chaumond, J. & Wolf, T. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter (2019). URL https://arxiv.org/pdf/1910.01108.pdf.\n\n * [5] Joshy, A. & Sundar, S. Analyzing the performance of sentiment analysis using bert, distilbert, and roberta (2022).\n* [6] Thorp, H. H. Chatgpt is fun, but not an author. _Science_**379**, 313 (2023).\n* [7] Aydin, O. Google bard generated literature review: Metaverse. _Journal of AI_**7**, 1-14 (2023).\n* [8] Lozic, E. & Stular, B. Chatgpt v bard v bing v claude 2 v aria v human-expert. how good are ai chatbots at scientific writing? (2023).\n* [9] Bouyamourn, A. Why llms hallucinate, and how to get (evidential) closure: Perceptual, intensional, and extensional learning for faithful natural language generation 3181-3193 (2023). URL https://doi.org/10.18653/v1/2023.emnlp-main.192.\n* [10] Shumailov, I. _et al._ The curse of recursion: training on generated data makes models forget (2023). URL https://doi.org/10.48550/arxiv.2305.17493.\n* [11] Leaver, T. & Srdarov, S. Chatgpt isn't magic. _M/C Journal_**26** (2023).\n* [12] Vaccino-Salvadore, S. Exploring the ethical dimensions of using chatgpt in language learning and beyond. _Languages_**8**, 191 (2023).\n* [13] Kang, Y., Cai, Z., Tan, C., Huang, Q. & Liu, H. Natural language processing (nlp) in management research: A literature review. _Management Research Review_**7**, 139-172 (2020).\n* [14] Sun, C., Qiu, X., Xu, Y. & Huang, X. How to fine-tune bert for text classification **11856**, 194-206 (2019). URL https://doi.org/10.1007/978-3-030-32381-3_16.\n* [15] Liu, Y. Robust evaluation measures for evaluating social biases in masked language models (2024). URL https://doi.org/10.48550/arxiv.2401.11601.\n* [16] Visa, A. A texture classifier based on neural network principles (1990).\n* [17] Liu, Y. _et al._ Roberta: A robustly optimized bert pretraining approach (2019). URL https://doi.org/10.48550/arxiv.1907.11692.\n* [18] Lan, Z. _et al._ Albert: A lite bert for self-supervised learning of language representations (2019). URL https://doi.org/10.48550/arxiv.1909.11942.\n* [19] Bahani, M., Ouaazizi, A. E. & Maalmi, K. The effectiveness of t5, gpt-2, and bert on text-to-image generation task. _Pattern Recognition Letters_**173**, 57-63 (2023).\n\n "}, "Texto3.pdf": {"Large Language Models can be Guided to Evade AI-Generated Text Detection": "\n\nNing Lu\\({}^{1,2,}\\)\n\nShengcai Liu\\({}^{1,3,*,}\\)\n\nequal contribution\n\nRui He\\({}^{1}\\)\n\nYew-Soon Ong\\({}^{3,4}\\)\n\nQi Wang\\({}^{1}\\)\n\nKe Tang\\({}^{1}\\)\n\n\\({}^{1}\\)Southern University of Science and Technology\n\n\\({}^{2}\\)Hong Kong University of Science and Technology\n\n\\({}^{3}\\)Centre for Frontier AI Research (CFAR), A*STAR\n\n\\({}^{4}\\)Nanyang Technological University\n\n###### ", "Abstract": "\n\nLarge language models (LLMs) have shown remarkable performance in various tasks and have been extensively utilized by the public. However, the increasing concerns regarding the misuse of LLMs, such as plagiarism and spamming, have led to the development of multiple detectors, including fine-tuned classifiers and statistical methods. In this study, we equip LLMs with prompts, rather than relying on an external paraphraser, to evaluate the vulnerability of these detectors. We propose a novel **S**ubstitution-based **I**n-**C**ontext example **O**ptimization method (SICO) to automatically construct prompts for evading the detectors. SICO is cost-efficient as it requires only 40 human-written examples and a limited number of LLM inferences to generate a prompt. Moreover, once a task-specific prompt has been constructed, it can be universally used against a wide range of detectors. Extensive experiments across three real-world tasks demonstrate that SICO significantly outperforms the paraphraser baselines and enables GPT-3.5 to successfully evade six detectors, decreasing their AUC by 0.5 on average. Furthermore, a comprehensive human evaluation as well as a validation experiment in the wild show that the SICO-generated text achieves human-level readability and task completion rates. Finally, the strong performance of SICO exhibits its potential as a reliable evaluation tool for future detectors. The codes and data are located on https://github.com/ColinLu50/Evade-GPT-Detector.\n\n## ", "1 Introduction": "\n\nThe rapid advancement of large language models (LLMs), such as GPT [1], PaLM [2], and LLaMa [3], has led to a largely-increased capacity for generating high-quality human-like text. However, there are also growing concerns surrounding the misuse of these models, including generating fake product reviews [4; 5] and misinformation [5], enabling academic dishonesty [6], and producing misleading answers on websites [7].\n\nIn response to these challenges, several methods for detecting AI-generated text have been proposed recently, ranging from fine-tuned classifiers [8; 9], statistical methods [10], to watermarking [11]. There are also online detection services provided by companies such as GPTzero [12]. However, the robustness of these detection methods has not been thoroughly evaluated. Recent studies [13; 14] have shown the vulnerability of these detectors to the so-called _paraphrase attacks_, which adopt an external paraphraser to rewrite the text generated by LLMs to evade detectors.\n\nIn this work, rather than relying on an external paraphraser, we explore equipping LLMs with carefully constructed prompts to evade detectors. The intuition is that, given the remarkable capabilities of LLMs, appropriate prompts can guide these models to potentially achieve and even exceed the evasion performance level of smaller external paraphrasers. We propose **SICO**, a **S**ubstitution-based **I**n-**C**ontexample **O**ptimization method, to automatically construct such prompts based on human-generated examples. Specifically, SICO iteratively substitutes words and sentences within the in-context examples to provide more representative demonstrations for LLMs to generate text that cannot be detected, where the substitution procedure is directed by a proxy detector (see Figure 1 for an overview of SICO).\n\nWe assess the evasion performance of SICO across three real-world tasks that are susceptible to the misuse of LLMs, i.e., academic essay writing, open-ended question answering, and fake review generation. The results demonstrate that SICO consistently outperforms the paraphraser baselines, leading to a decrease in AUC by approximately 0.5 on average for six existing detectors. Additionally, a comprehensive human evaluation involving 600 examples shows that the SICO-generated text is comparable to, and in some cases even better than, human-written text in terms of readability and task completion rates. To further evaluate the practical utility of SICO, we deploy it on Reddit, an online social platform, to generate responses for users' questions. The high percentage of generated responses that are liked by Reddit users shows that SICO is capable of generating human-approved content while being barely identified as AI. In addition to its strong evasion performance, SICO is also cost-efficient and easy to use. Unlike paraphraser-based methods that often require extensive computational resources - as evidenced by the fine-tuning of a 13B model on a large dataset [13] - SICO only requires 40 human-generated examples and a limited number of LLM inferences (e.g., costing approximately 1 USD using the GPT-3.5 API). Besides, once a task-specific prompt has been constructed by SICO, it can be universally used against a wide range of detectors.\n\nConsidering the importance of detecting AI-generated text to avoid their misuse, the results presented in this work certainly reveal the vulnerability of the existing detectors. Besides, this work presents the first empirical evidence that LLMs can evade detectors through a prompt-guided approach. Finally, the strong evasion performance of SICO suggests that it can be used as a standard evaluation tool for any future AI-generated text detectors. We hope that these findings can better facilitate the research concerning the responsible use of LLMs. To summarize, our main contributions are:\n\n* We introduce SICO, a novel in-context example learning method, to automatically construct prompts that can guide LLMs to evade detectors.\n* With low cost, SICO achieves strong performance in evading six existing detectors across three tasks, significantly outperforming the paraphraser baselines.\n* A comprehensive human evaluation, as well as a validation experiment in the wild, verifies that the SICO-generated text achieves human-level readability and task completion rates.\n\n## ", "2 Related works": "\n\n### ", "AI-generated text detection": "\n\nIn recent years, the research community has developed a wide range of detectors for AI-generated contents. In general, these detectors can be classified into three categories: training-based, statistical, and watermarking methods. Training-based methods treat the detection problem as a binary classification task, where neural networks are trained using AI-generated text and human-written text. Early studies utilized classifiers to identify fake reviews [15] and fake news [16]. More recently, researchers have trained classifiers using text generated by LLMs, such as the GPT-3.5 detector [8] and GPT-2 detector [9].\n\nStatistical methods, on the other hand, focus on zero-shot detection without any additional training overhead. These methods seek to distinguish between human-written text and AI-generated text based on the statistical characteristics of text, such as the statistical irregularities in measures like entropy [17], perplexity [18] and token rank [19]. A recent method, DetectGPT [10], exploits the phenomenon that AI-generated text tends to lie in the negative curvature regions of log probability of text.\n\n The watermarking methods involve modifying the LLM's text generation process to imprint specific patterns on the generated text, such that it can be detected [20; 21; 11]. Although the proposed method SICO primarily focuses on the first two types of detection methods, it can also help evade watermarking when acted as an external paraphraser, as shown in Appendix G.\n\n### ", "In-context learning": "\n\nWith the increasing scales of models and corpora [22; 23; 2; 24], LLMs have demonstrated the in-context learning (ICL) ability, allowing them to perform tasks with only a few examples provided as demonstrations [1]. Recent studies have focused on designing demonstrations during inference, which can be divided into demonstration selection, ordering, and formatting [25]. Specifically, demonstrations can be selected based on unsupervised metrics or supervised strategies [26; 27; 28]. For ordering, Liu et al. [29] sorted examples by their distances to the input. Regarding demonstration formatting, Wei et al. [30] proposed the so-called chain-of-thoughts (COT) format, and subsequent works have developed automatic COT [31]. In contrast to these works, we focus on iteratively optimizing demonstrations through substitutions. In principle, the proposed method SICO can be used in combination with the above-mentioned methods, potentially leading to improved performance.\n\n## ", "3 Substitution-based in-context example optimization (SICO)": "\n\nThe illustration of SICO is presented in Figure 1. First, LLM is asked to extract language features of human-written text. Then, the in-context examples are initialized and optimized. The final prompt is composed of the feature, task instruction, and optimized in-context examples. Below, we first describe how to evaluate a prompt during its optimization and then elaborate all the steps of SICO.\n\n### ", "Prompt evaluation": "\n\nGiven a natural language processing task, denote the task input as \\(x\\). To assess the utility of a prompt \\(p\\), we first collect a set of task inputs, \\(X_{eval}\\). For each input \\(x\\in X_{eval}\\), \\(p\\) and \\(x\\) are first concatenated (denoted by \\(p\\oplus x\\)) and fed into the LLM, whose output text (denoted by LLM\\((p\\oplus x)\\)) is then classified by a proxy detector. Let \\(\\mathcal{P}_{\\text{AI}}\\) be the predicted probability of LLM\\((p\\oplus x)\\) to be AI-generated, then the utility score of prompt \\(p\\), denoted by \\(\\mathcal{U}(p)\\), is defined as one minus the averaged predicted\n\nFigure 1: Illustration of how SICO generates prompts for the question answering task. The probability \\(P_{\\text{AI}}\\), as predicted by the proxy detector, indicates the likelihood that the given text is AI-generated. Once SICO prompt is constructed, it serves as a template, allowing users to insert various task inputs (highlighted in purple text).\n\n probability across \\(X_{eval}\\) (the higher \\(\\mathcal{U}\\), the better):\n\n\\[\\mathcal{U}(p)=1-\\frac{1}{|\\mathbf{X}_{eval}|}\\sum_{x\\in X_{eval}}\\mathcal{P}_{ \\text{AI}}(\\text{LLM}(p\\oplus x)).\\] (1)\n\n### ", "Prompt Construction": "\n\nData collectionWe first collect a set of \\(K\\) triplets, i.e., \\(D=\\{(x^{k}_{\\text{ic}},y^{k}_{\\text{AI}},y^{k}_{\\text{human}})\\}_{k=1}^{K}\\), where \\(x^{k}_{\\text{ic}}\\) is a task input and \\(y^{k}_{\\text{AI}},y^{k}_{\\text{human}}\\) are the corresponding outputs generated by the LLM and humans, respectively. Note \\(D\\) is used for prompt construction and it is independent of \\(X_{eval}\\) which is used for prompt evaluation.\n\nFeature extractionThis step involves the \\(K\\) pairs of AI-generated and human-written outputs from \\(D\\), denoted by \\(\\{(y^{k}_{\\text{AI}},y^{k}_{\\text{human}})\\}_{k=1}^{K}\\). We provide LLM with these pairs and ask LLM to extract the distinct linguistic features of human-written text, denoted as \\(t_{\\text{feature}}\\).\n\nIn-context example optimizationWe initialize the in-context examples as \\((x^{k}_{\\text{ic}},y^{k}_{\\text{ic}})\\), where \\(y^{k}_{\\text{ic}}\\) is generated by paraphrasing \\(y^{k}_{\\text{AI}}\\). More specifically, the text feature \\(t_{\\text{feature}}\\) is concatenated with a paraphrasing instruction to instruct LLM to paraphrase \\(y^{k}_{\\text{AI}}\\) to generate \\(y^{k}_{\\text{ic}}\\) .\n\nThen the in-context output \\(y_{\\text{ic}}\\) is iteratively optimized to be less AI-like, which is directed by the proxy detector. By presenting more and more representative in-context demonstrations to LLM, it is expected to understand how to generate human-like outputs. Formally, the optimization goal can be expressed as:\n\n\\[y^{*}_{\\text{ic}}=\\operatorname*{arg\\,min}_{y^{\\prime}_{\\text{ic}}\\in\\text{SIM }(y^{\\prime}_{\\text{ic}})}\\mathcal{P}_{\\text{AI}}(y^{\\prime}_{\\text{ic}}),\\] (2)\n\nwhere \\(\\text{SIM}(y_{\\text{ic}})\\) denotes the set of text that is semantically similar to \\(y_{\\text{ic}}\\). The goal of setting such semantic restriction is to maintain the usability of the text during optimization. In SICO, we generate semantically similar text by replacing words with their synonyms and rephrasing sentences. This is explained in detail below.\n\nSubstitution typeTo generate \\(y^{\\prime}_{\\text{ic}}\\) that is semantically similar to \\(y_{\\text{ic}}\\), we employ substitution at word level and sentence level in turn. For word-level substitution, we use WordNet [32], a lexical database of English words, to construct a synonym substitution set. We restrict substitutions to content words that carry meanings and ensure that the substitution would not change the part-of-speech tags. We use a mask language model to filter out the candidate words that not fits the context. For sentence-level substitution, we utilize a paraphrasing instruction combined with extracted feature, denoted as \\(t_{\\text{feature}}\\oplus p_{\\text{para}}\\). This combined instruction is used to prompt LLM to generate paraphrases for each sentence in \\(y_{\\text{ic}}\\).\n\n**Algorithm** As illustrated in Algorithm 1, SICO would optimize \\(\\{y_{\\text{ic}}^{k}\\}_{k=1}^{K}\\) for \\(N\\) iterations (lines 4-17). At each iteration, each \\(y_{\\text{ic}}^{k}\\) would be optimized by greedy substitution (line 11), as presented in Algorithm 2. Specifically, for the \\(i\\)-th original word/sentence \\(y_{i}\\) in the text \\(y\\), let \\(C_{i,j}\\) denote its \\(j\\)-th synonym/paraphrase, and let SUB\\((y_{i},C_{i,j})\\) denote the new text resulting from substituting \\(y_{i}\\) with \\(C_{i,j}\\). For each \\(y_{i}\\), SICO finds the best synonym/paraphrase \\(C_{i,*}\\) by checking which \\(C_{i,j}\\) gives the lowest AI-probability when substituting \\(y_{i}\\) (Line 1 in Algorithm 2).\n\nAfter obtaining the optimized in-context output \\(\\hat{y}_{\\text{ic}}\\), the new prompt is constructed as \\(\\hat{p}=t_{\\text{feature}}\\oplus p_{\\text{task}}\\oplus\\{(x_{\\text{ic}}^{k}, \\hat{y}_{\\text{ic}}^{k})\\}_{k=1}^{K}\\), where \\(p_{\\text{task}}\\) is the task instruction, as illustrated in Figure 1. Then \\(\\hat{p}\\) would be compared with the current best prompt \\(p^{*}\\) based on their utility scores as defined in Eq. (1). If \\(\\hat{p}\\) scores higher, SICO replaces \\(p^{*}\\) with it. After \\(N\\) iterations, \\(p^{*}\\) is returned as the final prompt. More implementation details of SICO are shown in Appendix A.\n\n### ", "SICO for Paraphrasing": "\n\nThe approach described above directly generates the task output to evade detectors. We refer to this direct approach as SICO-Gen. Alternatively, SICO can be easily adapted for paraphrasing, which we term as SICO-Para. Instead of direct generation, SICO-Para evades detectors in two steps. Initially, LLM produces an intermediate task output, typically incapable of evading detectors. Then. this output is paraphrased using SICO-Para to successfully evade detectors. Switching from SICO-Gen to SICO-Para requires only two adjustments: (1) the task input \\(x\\) is set to the AI-generated output text in \\(D\\) and \\(\\mathbf{X}_{eval}\\); (2) task instruction \\(p_{\\text{task}}\\) is modified to paraphrasing instruction.\n\n## ", "4 Experiments": "\n\n### ", "Experimental setup": "\n\n**Tasks & datasets** We consider three real-world tasks that are susceptible to the misuse of LLMs, i.e., academic essay writing (Writing), open-ended question answering (QA), and fake review generation (Review). We use GPT-3.5, one of the most powerful LLMs, to complete the tasks and generate text in our experiments.\n\nFor academic writing, we employ Wikipedia paragraphs from SQuAD dataset [33] as human-written text. Following the approach in Mitchell et al. [10], we use the first 30 words of these paragraphs as task inputs and ask GPT-3.5 to complete the rest. For open-ended question answering, we sample questions from Eli5 [34] dataset and ask GPT-3.5 to generate answers, following Krishna et al. [13]. For fake review generation, we first instruct GPT-3.5 to extract the business name and five keywords from human-written reviews from Yelp dataset [35], and then generate fake reviews based on the extracted information with specified sentiment. For each task, we collect 200 examples from GPT-3.5 (called original AI-generated text) and 200 human-written examples from corresponding dataset. More details about dataset can be found in Appendix F.\n\n**Detectors** Six representative detectors belonging to three different types are considered. Details of these detectors can be found in Appendix C.\n\n_Training-based methods._ (i) GPT-3.5 Detector (GPT3-D) [8]: a RoBERTa model [36] fine-tuned on text generated by GPT-3.5. (ii) GPT2 Detector (GPT2-D) [9]: a RoBERTa detector officially released by OpenAI, fine-tuned on GPT2-generated text.\n\n_Statistical methods._ (i) DetectGPT [10] evaluates the variation in a language model's log probability by introducing minor perturbations to the detected text. (ii) Log-Rank [10] is a statistical method that employs a language model to compute the mean prediction rank of each token in a text, given its preceding context. We utilize a relatively smaller language model, GPT2-medium [23], for both methods. Because Mireshghallah et al. [37] find that smaller language models have better detection performance than larger ones.\n\n _APIs_.3 (i) GPTzero [12] is a widely-used commercial detector, cooperated with many academic organizations. (ii) OpenAI Detector (OpenAI-D) [38] is officially offered by OpenAI, fine-tuned from a language model.\n\nFootnote 3: We consider the API versions of May 15, 2023. For OpenAI-D, we follow the implementation of Krishna et al. [13].\n\n**Baselines** We consider four paraphrasing baselines that evade detectors by paraphrasing the original AI-generated text. Specifically, two recently proposed methods are considered: (1) _Parrot_[14] and (2) _DIPPER_[13]. Both methods employ an external neural network specifically trained for paraphrasing. In addition, we include two prompting baselines to instruct GPT-3.5 to paraphrase the original AI-generated text: (3) _GPT-Para_ that uses the straightforward instruction _\"Paraphrase this\"_ to assess the capabilities of GPT-3.5 without intricate prompt engineering, and (4) _Human Prompt_ that utilizes a human-designed prompt. More details can be found in Appendix A.2.\n\n**Evaluation metrics** We use the area under the ROC curve (AUC) to measure the performance of detectors. The ROC curves are also illustrated to show the detection performance under different classification thresholds. For each task, we evaluate AUC score using 200 human-written text and 200 original or paraphrased AI-generated text. For each task input, we run each evasion method _only once_, instead of repeating multiple times until successful evasion, to simulate real-world scenarios where the target detector is inaccessible.\n\n**Experimental settings** We set \\(|\\mathbf{X}_{eval}|=32\\), \\(K=8\\), \\(N=6\\), and use GPT-3.5, specifically _gpt-3.5-turbo-0301_, as the LLM, where the inference parameters are kept in default. And we use GPT3-D as the proxy detector. Experiments using other LLMs and proxy detectors are presented in Section 5.2.\n\n### ", "Evasion Performance and Analysis": "\n\nTable 1 presents the performance of SICO and other baselines against six detectors in AUC score. SICO consistently outperforms other baselines by a substantial margin in all cases. Notably, in most cases, SICO reduces the AUC score to less than 0.5, equivalent to the expected performance of a random classifier. Figure 2 shows the ROC curves of evasion methods on academic writing task. One\n\n\nTable 1: AUC scores of detectors on text generated by different methods. \u201c-\u201d refers to the detector\u2019s AUC score on the original AI-generated text, without applying any evasion methods. Symbol \u2018*\u2019 represents that SICO uses GPT3-D as the proxy detector for prompt construction. For each detector, the lowest AUC score is indicated in **bold**, and the second-lowest is underlined.\n\n can clearly observe that SICO curves lie below others along different thresholds, often lower than the random classifier curve. More evasion results including ROC cures and detection rates are shown in Appendix H.\n\nOne interesting trend is that SICO-Para consistently outperforms SICO-Gen against statistical detectors, i.e., DetectGPT and Log-Rank. We speculate this performance difference comes from the varying influence of the prompt on the generated text between the two methods. In SICO-Para, the distribution of generated text is largely influenced by the original AI-generated text, which is in the prompt. However, in SICO-Gen, the distribution of generated text depends more on the previously generated text. Given that statistical detectors have access to the newly generated text but not the prompt, their estimation of token probability becomes less accurate for SICO-Para text, thus misleading the detection. It might also explain why GPT-Para can reduce the performance of statistical detectors.\n\n### ", "Human Evaluation": "\n\nFrom the users' perspective, using AI-generated text goes beyond evading detection systems; the usability of text is equally critical. For example, for academic writing task, users expect the text to be readable, properly formatted, and relevant to the given topic. Therefore, we evaluate the usability of text based on two criteria: readability and task completion rate. For each task, we randomly sample 200 examples generated by four methods (50 per method), including human-written text. Then we ask three human annotators to rate the readability of text on a scale from 1 to 5, and judge if the text accomplishes the task's goal. More details of human evaluation are shown in Appendix D.\n\nAs shown in Table 2, both SICO-Gen and SICO-Para demonstrate superior performance over DIPPER in terms of task completion and readability over three tasks. Furthermore, SICO-generated text performs competitively compared with human-written text in both metrics, with a negligible difference less than 0.1. In contrast, DIPPER exhibits inferior performance relative to human-written text, particularly with a notable 0.27 decline in readability.\n\nFigure 2: ROC curves of six detectors on the text generated by different evasion methods on academic writing task.\n\n \n\n### ", "Real-life Experiments": "\n\nTo further assess the applicability of SICO in real-world, we simulate one potential misuse case of LLM, where SICO is deployed as an automatic reply bot on Reddit4, a popular online social platform. We wrote a script to monitor the new posts submitted in the community for asking questions and used GPT-3.5 equipped with SICO-Para to automatically reply them. The prompt we used is trained for question answering task. On Reddit, except for giving comments, users can express their approval of other's responses by clicking the \"like\" or \"dislike\" button. To minimize the social impact, we limit the number of responses to 40 and deleted them after collecting results.\n\nFootnote 4: https://www.reddit.com/\n\nThe quantitative results in Table 3 demonstrate that users generally react positively to the text from SICO. Specifically, 40% of the responses from SICO receive \"likes\" from Reddit users, significantly higher than the 2.5% that are disliked. The remaining 57.5% of responses go unnoticed, which is common in social media. Besides, in 12.5% of cases, users left additional comments to express their approval or to ask further questions, demonstrating their engagement and interest in SICO's response. Figure 3 presents two SICO's responses that got approval from users, as indicated by \"likes\" and comments.\n\n### ", "Cost Efficiency": "\n\nIn terms of the data prerequisite, SICO only needs \\(K+|\\mathbf{X}_{eval}|\\) human-written input-output examples to build prompt, which is \\(8+32=40\\) in the experiments. The other AI-generated text can be produced by LLM leveraging these human samples. Furthermore, SICO offers the advantage of low cost for prompt construction. Based on three repeated runs, the actual USD costs of SICO-Para are \\(1.04\\pm 0.04\\), \\(1.08\\pm 0.05\\), and \\(0.75\\pm 0.04\\) for Writing, QA, Review tasks, respectively.\n\n## ", "5 Further Experiments": "\n\n### ", "Ablation Study": "\n\nWe conducted an ablation study over academic writing task to to evaluate the contribution of individual components within the SICO framework. \"Human-ICE\" denotes the approach where human-written text is directly utilized as the in-context example for constructing the prompt. \"w/o feature\" and \"w/o ICE\" refer to the prompts without feature text and the optimized in-context examples, respectively. \"w/o OPT\" represents the initial prompt before optimization (see Line 3 in Algorithm 1). In our experiment, we explore SICO-Para on three types of detectors: GPT3-D, OpenAI-D and DetectGPT.\n\nResults in Table 4 shows that directly using human-written text is ineffective, even making the detection more accurate. We speculate that the human-written examples are too heterogeneous\n\n\nTable 2: Human evaluation results. \u201cAvg.D.\u201d represents the average difference between the results achieved by the evasion method and the results achieved by human-written text on the three tasks. The best value of each task is set **bold**.\n\n\nTable 3: Quantitative results of Reddit test.\n\nand characterized in multiple ways, so LLM cannot effectively learn their attributes. Besides, the importance of feature text is comparatively less than that of optimized in-context examples. Furthermore, the result reveals the significant role of the optimization step in SICO. Using in-context examples that are not optimized is essentially equivalent to not using any in-context examples.\n\n### ", "SICO with Different Proxy Detectors and LLMs": "\n\nAs described in Section 3, SICO requires a proxy detector and a LLM to construct a prompt. In this experiment, we explore the performance of SICO-Para on writing task, using three types of proxy detectors: (1) training-based model GPT-3.5 detector, (2) API detector GPTzero, and (3) statistical method DetectGPT. For different LLMs, we adopt Vicuna-13B [39], an open-source chatbot fine-tuned from LLaMa [3]. Results in Table 5 show that SICO maintains a high degree of detection evasion performance, regardless of proxy detectors or LLMs. In most cases, SICO manages to reduce the AUC of detectors by approximately 0.4.\n\n\nTable 4: The AUC scores of detectors on text generated by different methods. \u201c\u2013\u201d indicates the case where no evasion method is used. \u2019AVG\u2019 represents the average AUC scores across detectors.\n\nFigure 3: Screenshots of replies from SICO that got approved by Reddit users. (Left) Approval is shown by the number of \u201clike\u201ds. (Right) The original poster express thanks to SICO\u2019s response.\n\n\nTable 5: The AUC scores of SICO using different proxy detectors and LLMs on writing task. The first line indicates the performance without applying any evasion method.\n\n \n\n### ", "Examples of the generated text": "\n\nTable 6 presents the fake reviews generated by SICO-Gen and SICO-Para. The generated text shows high readability and fulfill the task's goal, successfully mentioning all keywords and generating positive reviews of the specified object. The AI probability, denoted as \\(\\mathcal{P}_{\\text{AI}}\\) in the table, is determined by the GPT3-D. More examples are shown in Appendix I.\n\n## ", "6 Conclusion": "\n\nIn conclusion, we have proposed a novel in-context learning approach, SICO, designed to guide LLMs in generating text that can effectively evade detectors. Our extensive experiments on evasion demonstrate the superior performance of SICO, which significantly reduces the detection capabilities of existing AI text detectors across three tasks. A comprehensive human evaluation shows SICO text can achieve human-level readability and task completion rates. The experiment in the wild shows the functionality of SICO in real life.\n\nLooking ahead, SICO could act as a data generator and be integrated during the training phase of AI detectors, which may enhance their robustness. Furthermore, the core concept of SICO, namely, substitution-based in-context learning, could be applied to a variety of text generation tasks. We believe that this opens up new avenues for future research in the fields of text generation and in-context learning.\n\n## ", "7 Ethics statement": "\n\nThe intention of this paper is not to offer a potential method for evading AI-generated text detection systems. Instead, our aim is to raise awareness within the broader community about the vulnerabilities of existing AI-generated text detection systems to such technology. As many LLMs are public available and free to use, many people can adjust their prompt and generate text that evades these detectors. Given the ease of evasion illustrated in this study, these detectors are not robust yet.\n\nWe hope the research community can stress test their detectors against text generated by carefully crafted prompt, and create more robust detectors in the future. To support the research in this field, we will make our training methods and relevant data/code publicly available.\n\n\nTable 6: Fake reviews generated by SICO. The first line shows the task input of review generation.\n\n\n\n## ", "References": "\n\n* Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.\n* Chowdhery et al. [2022] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. _arXiv preprint arXiv:2204.02311_, 2022.\n* Touvron et al. [2023] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.\n* and machine-based detection. In Leonard Barolli, Flora Amato, Francesco Moscato, Tomoya Enokido, and Makoto Takizawa, editors, _Advanced Information Networking and Applications\n- Proceedings of the 34th International Conference on Advanced Information Networking and Applications, AINA-2020, Caserta, Italy, 15-17 April_, volume 1151 of _Advances in Intelligent Systems and Computing_, pages 1341-1354. Springer, 2020. doi: 10.1007/978-3-030-44041-1_114. URL https://doi.org/10.1007/978-3-030-44041-1_114.\n* Lin et al. [2022] Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022_, pages 3214-3252. Association for Computational Linguistics, 2022. doi: 10.18653/v1/2022.acl-long.229. URL https://doi.org/10.18653/v1/2022.acl-long.229.\n* Stokel-Walker [2022] Chris Stokel-Walker. Ai bot chatgpt writes smart essays-should academics worry? _Nature_, 2022.\n* Stokelverflow [2023] StackOverflow. Temporary policy: Chatgpt is banned, 2023. URL https://meta.stackoverflow.com/questions/421831/temporary-policy-chatgpt-is-banned.\n* Guo et al. [2023] Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng Wu. How close is chatgpt to human experts? comparison corpus, evaluation, and detection. _CoRR_, abs/2301.07597, 2023. doi: 10.48550/arXiv.2301.07597. URL https://doi.org/10.48550/arXiv.2301.07597.\n* Solaiman et al. [2019] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford, and Jasmine Wang. Release strategies and the social impacts of language models. _CoRR_, abs/1908.09203, 2019. URL http://arxiv.org/abs/1908.09203.\n* Mitchell et al. [2023] Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D. Manning, and Chelsea Finn. Detectgpt: Zero-shot machine-generated text detection using probability curvature. _CoRR_, abs/2301.11305, 2023. doi: 10.48550/arXiv.2301.11305. URL https://doi.org/10.48550/arXiv.2301.11305.\n* Kirchenbauer et al. [2023] John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and Tom Goldstein. A watermark for large language models. _CoRR_, abs/2301.10226, 2023. doi: 10.48550/arXiv.2301.10226. URL https://doi.org/10.48550/arXiv.2301.10226.\n* Tian [2023] Edward Tian. Gptzero: an ai detector, 2023. URL https://gptzero.me/.\n* Krishna et al. [2023] Kalpesh Krishna, Yixiao Song, Marzena Karpinska, John Wieting, and Mohit Iyyer. Paraphrasing evades detectors of ai-generated text, but retrieval is an effective defense. _CoRR_, abs/2303.13408, 2023. doi: 10.48550/arXiv.2303.13408. URL https://doi.org/10.48550/arXiv.2303.13408.\n\n* Sadasivan et al. [2023] Vinu Sankar Sadasivan, Aounon Kumar, Sriram Balasubramanian, Wenxiao Wang, and Soheil Feizi. Can ai-generated text be reliably detected? _CoRR_, abs/2303.11156, 2023. doi: 10.48550/arXiv.2303.11156. URL https://doi.org/10.48550/arXiv.2303.11156.\n* an adversarial study. In _Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 2: Short Papers_. The Association for Computer Linguistics, 2016. doi: 10.18653/v1/p16-2057. URL https://doi.org/10.18653/v1/p16-2057.\n* Zellers et al. [2019] Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. Defending against neural fake news. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alche-Buc, Emily B. Fox, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada_, pages 9051-9062, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/3e9f0fc9b2f89e043bc6233994dfcf76-Abstract.html.\n* Lavergne et al. [2008] Thomas Lavergne, Tanguy Urvoy, and Francois Yvon. Detecting fake content with relative entropy scoring. In Benno Stein, Efstathios Stamatatos, and Moshe Koppel, editors, _Proceedings of the ECAI'08 Workshop on Uncovering Plagiarism, Authorship and Social Software Misuse, Patras, Greece, July 22, 2008_, volume 377 of _CEUR Workshop Proceedings_. CEUR-WS.org, 2008. URL https://ceur-ws.org/Vol-377/paper4.pdf.\n* 21st International Conference on Applications of Natural Language to Information Systems, NLDB 2016, Salford, UK, June 22-24, 2016, Proceedings_, volume 9612 of _Lecture Notes in Computer Science_, pages 421-426. Springer, 2016. doi: 10.1007/978-3-319-41754-7_43. URL https://doi.org/10.1007/978-3-319-41754-7_43.\n* August 2, 2019, Volume 3: System Demonstrations_, pages 111-116. Association for Computational Linguistics, 2019. doi: 10.18653/v1/p19-3019. URL https://doi.org/10.18653/v1/p19-3019.\n* Abdelnabi and Fritz [2021] Sahar Abdelnabi and Mario Fritz. Adversarial watermarking transformer: Towards tracing text provenance with data hiding. In _42nd IEEE Symposium on Security and Privacy, SP 2021, San Francisco, CA, USA, 24-27 May 2021_, pages 121-140. IEEE, 2021. doi: 10.1109/SP40001.2021.00083. URL https://doi.org/10.1109/SP40001.2021.00083.\n* Grinbaum and Adomaiitis [2022] Alexei Grinbaum and Laurynas Adomaiitis. The ethical need for watermarks in machine-generated language. _CoRR_, abs/2209.03118, 2022. doi: 10.48550/arXiv.2209.03118. URL https://doi.org/10.48550/arXiv.2209.03118.\n* Devlin et al. [2019] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)_, pages 4171-4186. Association for Computational Linguistics, 2019. doi: 10.18653/v1/n19-1423. URL https://doi.org/10.18653/v1/n19-1423.\n* Radford et al. [2019] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. _OpenAI blog_, 1(8):9, 2019.\n* Gou et al. [2022] Yunhao Gou, Tom Ko, Hansi Yang, James T. Kwok, Yu Zhang, and Mingxuan Wang. Leveraging per image-token consistency for vision-language pre-training. _CoRR_, abs/2211.15398, 2022. doi: 10.48550/arXiv.2211.15398. URL https://doi.org/10.48550/arXiv.2211.15398.\n\n * Dong et al. [2022] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. A survey for in-context learning. _arXiv preprint arXiv:2301.00234_, 2022.\n* Kim et al. [2022] Hyuhng Joon Kim, Hyunsoo Cho, Junyeob Kim, Taeuk Kim, Kang Min Yoo, and Sang-goo Lee. Self-generated in-context learning: Leveraging auto-regressive language models as a demonstration generator. _CoRR_, abs/2206.08082, 2022. doi: 10.48550/arXiv.2206.08082. URL https://doi.org/10.48550/arXiv.2206.08082.\n* Gonen et al. [2022] Hila Gonen, Srini Iyer, Terra Blevins, Noah A. Smith, and Luke Zettlemoyer. Demystifying prompts in language models via perplexity estimation. _CoRR_, abs/2212.04037, 2022. doi: 10.48550/arXiv.2212.04037. URL https://doi.org/10.48550/arXiv.2212.04037.\n* Rubin et al. [2022] Ohad Rubin, Jonathan Herzig, and Jonathan Berant. Learning to retrieve prompts for in-context learning. In Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz, editors, _Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022, Seattle, WA, United States, July 10-15, 2022_, pages 2655-2671. Association for Computational Linguistics, 2022. doi: 10.18653/v1/2022.naacl-main.191. URL https://doi.org/10.18653/v1/2022.naacl-main.191.\n* Liu et al. [2021] Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What makes good in-context examples for gpt-3? _arXiv preprint arXiv:2101.06804_, 2021.\n* Wei et al. [2022] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. _arXiv preprint arXiv:2201.11903_, 2022.\n* Zhang et al. [2022] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in large language models. _arXiv preprint arXiv:2210.03493_, 2022.\n* Miller [1998] George A Miller. _WordNet: An electronic lexical database_. MIT press, 1998.\n* Rajpurkar et al. [2016] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ Questions for Machine Comprehension of Text. _arXiv e-prints_, art. arXiv:1606.05250, 2016.\n* Fan et al. [2019] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5: long form question answering. In Anna Korhonen, David R. Traum, and Lluis Marquez, editors, _Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers_, pages 3558-3567. Association for Computational Linguistics, 2019. doi: 10.18653/v1/p19-1346. URL https://doi.org/10.18653/v1/p19-1346.\n* Zhang et al. [2015] Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada_, pages 649-657, 2015. URL https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f8dc8b4be867a9a02-Abstract.html.\n* Liu et al. [2019] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining approach. _CoRR_, abs/1907.11692, 2019. URL http://arxiv.org/abs/1907.11692.\n* Mireshghallah et al. [2023] Fatemehsadat Mireshghallah, Justus Mattern, Sicun Gao, Reza Shokri, and Taylor Berg-Kirkpatrick. Smaller language models are better black-box machine-generated text detectors. _CoRR_, abs/2305.09859, 2023. doi: 10.48550/arXiv.2305.09859. URL https://doi.org/10.48550/arXiv.2305.09859.\n* OpenAI [2023] OpenAI. Openai ai text classifier, January 2023. URL https://beta.openai.com/ai-text-classifier.\n\n * Chiang et al. [2023] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, 2023.\n* June 1, 2003_. The Association for Computational Linguistics, 2003. URL https://aclanthology.org/N03-1033/.\n* Ni et al. [2022] Jianmo Ni, Gustavo Hernandez Abrego, Noah Constant, Ji Ma, Keith B. Hall, Daniel Cer, and Yinfei Yang. Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, _Findings of the Association for Computational Linguistics: ACL 2022, Dublin, Ireland, May 22-27, 2022_, pages 1864-1874. Association for Computational Linguistics, 2022. doi: 10.18653/v1/2022.findings-acl.146. URL https://doi.org/10.18653/v1/2022.findings-acl.146.\n* Muennighoff et al. [2023] Niklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers. MTEB: massive text embedding benchmark. In Andreas Vlachos and Isabelle Augenstein, editors, _Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2023, Dubrovnik, Croatia, May 2-6, 2023_, pages 2006-2029. Association for Computational Linguistics, 2023. URL https://aclanthology.org/2023.eacl-main.148.\n* Raffel et al. [2020] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _Journal of Machine Learning Research_, 21(140):1-67, 2020. URL http://jmlr.org/papers/v21/20-074.html.\n\n Implementation Details\n\n### ", "Sico": "\n\n#### ", "a.1.1 Feature extraction": "\n\nIn feature extraction step, we instruct LLM to extract 5 features and calculate the utility score \\(\\mathcal{U}\\) of prompts encompassing each of these features. Then we select the feature with the highest utility for further steps. The goal of this step is to find a good feature to accelerate process, and make the whole process stable. Because sometimes LLM cannot extract useful features to evade detectors. The pseudo-code illustrating this selection process is outlined in Algorithm 3. Table 7 presents the prompt template used for feature extraction. Here, \\(K\\) text pairs generated by AI and Human are positioned within their respective positions. Table 20 shows the examples for feature extracted by LLM.\n\n```\n1:list of features \\(T_{\\text{feature}}\\), prompt utility function \\(\\mathcal{U}(\\cdot)\\)\n2:Initialize \\(t^{*}_{\\text{feature}}\\leftarrow\\emptyset\\)\n3:Initialize \\(\\mathcal{U}_{max}\\leftarrow-\\infty\\)\n4:for each feature \\(t_{\\text{feature},i}\\) in \\(T_{\\text{feature}}\\)do\n5: Construct prompt \\(p_{i}\\gets t_{\\text{feature},i}\\oplus p_{\\text{task}}\\)\n6:if\\(\\mathcal{U}(p_{i})>\\mathcal{U}_{max}\\)then\n7:\\(t^{*}_{\\text{feature}}\\gets t_{\\text{feature},i}\\)\n8:\\(\\mathcal{U}_{max}\\leftarrow\\mathcal{U}(p_{i})\\)\n9:endif\n10:endfor\n11:return\\(t^{*}_{\\text{feature}}\\) ```\n\n**Algorithm 3** Feature selections\n\nLLM consistently extract useful features.To test if LLM can reliably extract useful features, we conducted three separated experiments by running three feature extractions on different sets of human-written text and AI-generated text. We use three different extracted features to guide LLM generation and test the AUC drop after adopting 3 different features compared with the originally generated text on the Writing task. Table 8 shows the results, indicating that LLM consistently extracts useful features for detector evasion from different examples.\n\n#### ", "a.1.2 Task instructions": "\n\nTable 9 shows the actual task instruction \\(p_{\\text{task}}\\) we used in SICO. As mentioned in Section 3, feature text \\(t_{\\text{feature}}\\) at first step will be inserted before these task instructions. The \"Paraphrase\" instruction is \\(p_{\\text{para}}\\) used in paraphrase generation for substitution (Line 6 of Algorithm 1).\n\n\nTable 8: AUC drop of different features extracted based on different human-written and AI-generated text.\n\n \n\n#### ", "a.1.3 Word Substitution": "", "Baselines": "** We consider four paraphrasing baselines that evade detectors by paraphrasing the original AI-generated text. Specifically, two recently proposed methods are considered: (1) _Parrot_[14] and (2) _DIPPER_[13]. Both methods employ an external neural network specifically trained for paraphrasing. In addition, we include two prompting baselines to instruct GPT-3.5 to paraphrase the original AI-generated text: (3) _GPT-Para_ that uses the straightforward instruction _\"Paraphrase this\"_ to assess the capabilities of GPT-3.5 without intricate prompt engineering, and (4) _Human Prompt_ that utilizes a human-designed prompt. More details can be found in Appendix A.2.\n\n**Evaluation metrics** We use the area under the ROC curve (AUC) to measure the performance of detectors. The ROC curves are also illustrated to show the detection performance under different classification thresholds. For each task, we evaluate AUC score using 200 human-written text and 200 original or paraphrased AI-generated text. For each task input, we run each evasion method _only once_, instead of repeating multiple times until successful evasion, to simulate real-world scenarios where the target detector is inaccessible.\n\n**Experimental settings** We set \\(|\\mathbf{X}_{eval}|=32\\), \\(K=8\\), \\(N=6\\), and use GPT-3.5, specifically _gpt-3.5-turbo-0301_, as the LLM, where the inference parameters are kept in default. And we use GPT3-D as the proxy detector. Experiments using other LLMs and proxy detectors are presented in Section 5.2.\n\n### Evasion Performance and Analysis\n\nTable 1 presents the performance of SICO and other baselines against six detectors in AUC score. SICO consistently outperforms other baselines by a substantial margin in all cases. Notably, in most cases, SICO reduces the AUC score to less than 0.5, equivalent to the expected performance of a random classifier. Figure 2 shows the ROC curves of evasion methods on academic writing task. One\n\n\nTable 1: AUC scores of detectors on text generated by different methods. \u201c-\u201d refers to the detector\u2019s AUC score on the original AI-generated text, without applying any evasion methods. Symbol \u2018*\u2019 represents that SICO uses GPT3-D as the proxy detector for prompt construction. For each detector, the lowest AUC score is indicated in **bold**, and the second-lowest is underlined.\n\n can clearly observe that SICO curves lie below others along different thresholds, often lower than the random classifier curve. More evasion results including ROC cures and detection rates are shown in Appendix H.\n\nOne interesting trend is that SICO-Para consistently outperforms SICO-Gen against statistical detectors, i.e., DetectGPT and Log-Rank. We speculate this performance difference comes from the varying influence of the prompt on the generated text between the two methods. In SICO-Para, the distribution of generated text is largely influenced by the original AI-generated text, which is in the prompt. However, in SICO-Gen, the distribution of generated text depends more on the previously generated text. Given that statistical detectors have access to the newly generated text but not the prompt, their estimation of token probability becomes less accurate for SICO-Para text, thus misleading the detection. It might also explain why GPT-Para can reduce the performance of statistical detectors.\n\n### Human Evaluation\n\nFrom the users' perspective, using AI-generated text goes beyond evading detection systems; the usability of text is equally critical. For example, for academic writing task, users expect the text to be readable, properly formatted, and relevant to the given topic. Therefore, we evaluate the usability of text based on two criteria: readability and task completion rate. For each task, we randomly sample 200 examples generated by four methods (50 per method), including human-written text. Then we ask three human annotators to rate the readability of text on a scale from 1 to 5, and judge if the text accomplishes the task's goal. More details of human evaluation are shown in Appendix D.\n\nAs shown in Table 2, both SICO-Gen and SICO-Para demonstrate superior performance over DIPPER in terms of task completion and readability over three tasks. Furthermore, SICO-generated text performs competitively compared with human-written text in both metrics, with a negligible difference less than 0.1. In contrast, DIPPER exhibits inferior performance relative to human-written text, particularly with a notable 0.27 decline in readability.\n\nFigure 2: ROC curves of six detectors on the text generated by different evasion methods on academic writing task.\n\n \n\n### Real-life Experiments\n\nTo further assess the applicability of SICO in real-world, we simulate one potential misuse case of LLM, where SICO is deployed as an automatic reply bot on Reddit4, a popular online social platform. We wrote a script to monitor the new posts submitted in the community for asking questions and used GPT-3.5 equipped with SICO-Para to automatically reply them. The prompt we used is trained for question answering task. On Reddit, except for giving comments, users can express their approval of other's responses by clicking the \"like\" or \"dislike\" button. To minimize the social impact, we limit the number of responses to 40 and deleted them after collecting results.\n\nFootnote 4: https://www.reddit.com/\n\nThe quantitative results in Table 3 demonstrate that users generally react positively to the text from SICO. Specifically, 40% of the responses from SICO receive \"likes\" from Reddit users, significantly higher than the 2.5% that are disliked. The remaining 57.5% of responses go unnoticed, which is common in social media. Besides, in 12.5% of cases, users left additional comments to express their approval or to ask further questions, demonstrating their engagement and interest in SICO's response. Figure 3 presents two SICO's responses that got approval from users, as indicated by \"likes\" and comments.\n\n### Cost Efficiency\n\nIn terms of the data prerequisite, SICO only needs \\(K+|\\mathbf{X}_{eval}|\\) human-written input-output examples to build prompt, which is \\(8+32=40\\) in the experiments. The other AI-generated text can be produced by LLM leveraging these human samples. Furthermore, SICO offers the advantage of low cost for prompt construction. Based on three repeated runs, the actual USD costs of SICO-Para are \\(1.04\\pm 0.04\\), \\(1.08\\pm 0.05\\), and \\(0.75\\pm 0.04\\) for Writing, QA, Review tasks, respectively.\n\n## 5 Further Experiments\n\n### Ablation Study\n\nWe conducted an ablation study over academic writing task to to evaluate the contribution of individual components within the SICO framework. \"Human-ICE\" denotes the approach where human-written text is directly utilized as the in-context example for constructing the prompt. \"w/o feature\" and \"w/o ICE\" refer to the prompts without feature text and the optimized in-context examples, respectively. \"w/o OPT\" represents the initial prompt before optimization (see Line 3 in Algorithm 1). In our experiment, we explore SICO-Para on three types of detectors: GPT3-D, OpenAI-D and DetectGPT.\n\nResults in Table 4 shows that directly using human-written text is ineffective, even making the detection more accurate. We speculate that the human-written examples are too heterogeneous\n\n\nTable 2: Human evaluation results. \u201cAvg.D.\u201d represents the average difference between the results achieved by the evasion method and the results achieved by human-written text on the three tasks. The best value of each task is set **bold**.\n\n\nTable 3: Quantitative results of Reddit test.\n\nand characterized in multiple ways, so LLM cannot effectively learn their attributes. Besides, the importance of feature text is comparatively less than that of optimized in-context examples. Furthermore, the result reveals the significant role of the optimization step in SICO. Using in-context examples that are not optimized is essentially equivalent to not using any in-context examples.\n\n### SICO with Different Proxy Detectors and LLMs\n\nAs described in Section 3, SICO requires a proxy detector and a LLM to construct a prompt. In this experiment, we explore the performance of SICO-Para on writing task, using three types of proxy detectors: (1) training-based model GPT-3.5 detector, (2) API detector GPTzero, and (3) statistical method DetectGPT. For different LLMs, we adopt Vicuna-13B [39], an open-source chatbot fine-tuned from LLaMa [3]. Results in Table 5 show that SICO maintains a high degree of detection evasion performance, regardless of proxy detectors or LLMs. In most cases, SICO manages to reduce the AUC of detectors by approximately 0.4.\n\n\nTable 4: The AUC scores of detectors on text generated by different methods. \u201c\u2013\u201d indicates the case where no evasion method is used. \u2019AVG\u2019 represents the average AUC scores across detectors.\n\nFigure 3: Screenshots of replies from SICO that got approved by Reddit users. (Left) Approval is shown by the number of \u201clike\u201ds. (Right) The original poster express thanks to SICO\u2019s response.\n\n\nTable 5: The AUC scores of SICO using different proxy detectors and LLMs on writing task. The first line indicates the performance without applying any evasion method.\n\n \n\n### Examples of the generated text\n\nTable 6 presents the fake reviews generated by SICO-Gen and SICO-Para. The generated text shows high readability and fulfill the task's goal, successfully mentioning all keywords and generating positive reviews of the specified object. The AI probability, denoted as \\(\\mathcal{P}_{\\text{AI}}\\) in the table, is determined by the GPT3-D. More examples are shown in Appendix I.\n\n## 6 Conclusion\n\nIn conclusion, we have proposed a novel in-context learning approach, SICO, designed to guide LLMs in generating text that can effectively evade detectors. Our extensive experiments on evasion demonstrate the superior performance of SICO, which significantly reduces the detection capabilities of existing AI text detectors across three tasks. A comprehensive human evaluation shows SICO text can achieve human-level readability and task completion rates. The experiment in the wild shows the functionality of SICO in real life.\n\nLooking ahead, SICO could act as a data generator and be integrated during the training phase of AI detectors, which may enhance their robustness. Furthermore, the core concept of SICO, namely, substitution-based in-context learning, could be applied to a variety of text generation tasks. We believe that this opens up new avenues for future research in the fields of text generation and in-context learning.\n\n## 7 Ethics statement\n\nThe intention of this paper is not to offer a potential method for evading AI-generated text detection systems. Instead, our aim is to raise awareness within the broader community about the vulnerabilities of existing AI-generated text detection systems to such technology. As many LLMs are public available and free to use, many people can adjust their prompt and generate text that evades these detectors. Given the ease of evasion illustrated in this study, these detectors are not robust yet.\n\nWe hope the research community can stress test their detectors against text generated by carefully crafted prompt, and create more robust detectors in the future. To support the research in this field, we will make our training methods and relevant data/code publicly available.\n\n\nTable 6: Fake reviews generated by SICO. The first line shows the task input of review generation.\n\n\n\n## References\n\n* Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.\n* Chowdhery et al. [2022] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. _arXiv preprint arXiv:2204.02311_, 2022.\n* Touvron et al. [2023] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.\n* and machine-based detection. In Leonard Barolli, Flora Amato, Francesco Moscato, Tomoya Enokido, and Makoto Takizawa, editors, _Advanced Information Networking and Applications\n- Proceedings of the 34th International Conference on Advanced Information Networking and Applications, AINA-2020, Caserta, Italy, 15-17 April_, volume 1151 of _Advances in Intelligent Systems and Computing_, pages 1341-1354. Springer, 2020. doi: 10.1007/978-3-030-44041-1_114. URL https://doi.org/10.1007/978-3-030-44041-1_114.\n* Lin et al. [2022] Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022_, pages 3214-3252. Association for Computational Linguistics, 2022. doi: 10.18653/v1/2022.acl-long.229. URL https://doi.org/10.18653/v1/2022.acl-long.229.\n* Stokel-Walker [2022] Chris Stokel-Walker. Ai bot chatgpt writes smart essays-should academics worry? _Nature_, 2022.\n* Stokelverflow [2023] StackOverflow. Temporary policy: Chatgpt is banned, 2023. URL https://meta.stackoverflow.com/questions/421831/temporary-policy-chatgpt-is-banned.\n* Guo et al. [2023] Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng Wu. How close is chatgpt to human experts? comparison corpus, evaluation, and detection. _CoRR_, abs/2301.07597, 2023. doi: 10.48550/arXiv.2301.07597. URL https://doi.org/10.48550/arXiv.2301.07597.\n* Solaiman et al. [2019] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford, and Jasmine Wang. Release strategies and the social impacts of language models. _CoRR_, abs/1908.09203, 2019. URL http://arxiv.org/abs/1908.09203.\n* Mitchell et al. [2023] Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D. Manning, and Chelsea Finn. Detectgpt: Zero-shot machine-generated text detection using probability curvature. _CoRR_, abs/2301.11305, 2023. doi: 10.48550/arXiv.2301.11305. URL https://doi.org/10.48550/arXiv.2301.11305.\n* Kirchenbauer et al. [2023] John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and Tom Goldstein. A watermark for large language models. _CoRR_, abs/2301.10226, 2023. doi: 10.48550/arXiv.2301.10226. URL https://doi.org/10.48550/arXiv.2301.10226.\n* Tian [2023] Edward Tian. Gptzero: an ai detector, 2023. URL https://gptzero.me/.\n* Krishna et al. [2023] Kalpesh Krishna, Yixiao Song, Marzena Karpinska, John Wieting, and Mohit Iyyer. Paraphrasing evades detectors of ai-generated text, but retrieval is an effective defense. _CoRR_, abs/2303.13408, 2023. doi: 10.48550/arXiv.2303.13408. URL https://doi.org/10.48550/arXiv.2303.13408.\n\n* Sadasivan et al. [2023] Vinu Sankar Sadasivan, Aounon Kumar, Sriram Balasubramanian, Wenxiao Wang, and Soheil Feizi. Can ai-generated text be reliably detected? _CoRR_, abs/2303.11156, 2023. doi: 10.48550/arXiv.2303.11156. URL https://doi.org/10.48550/arXiv.2303.11156.\n* an adversarial study. In _Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 2: Short Papers_. The Association for Computer Linguistics, 2016. doi: 10.18653/v1/p16-2057. URL https://doi.org/10.18653/v1/p16-2057.\n* Zellers et al. [2019] Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. Defending against neural fake news. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alche-Buc, Emily B. Fox, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada_, pages 9051-9062, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/3e9f0fc9b2f89e043bc6233994dfcf76-Abstract.html.\n* Lavergne et al. [2008] Thomas Lavergne, Tanguy Urvoy, and Francois Yvon. Detecting fake content with relative entropy scoring. In Benno Stein, Efstathios Stamatatos, and Moshe Koppel, editors, _Proceedings of the ECAI'08 Workshop on Uncovering Plagiarism, Authorship and Social Software Misuse, Patras, Greece, July 22, 2008_, volume 377 of _CEUR Workshop Proceedings_. CEUR-WS.org, 2008. URL https://ceur-ws.org/Vol-377/paper4.pdf.\n* 21st International Conference on Applications of Natural Language to Information Systems, NLDB 2016, Salford, UK, June 22-24, 2016, Proceedings_, volume 9612 of _Lecture Notes in Computer Science_, pages 421-426. Springer, 2016. doi: 10.1007/978-3-319-41754-7_43. URL https://doi.org/10.1007/978-3-319-41754-7_43.\n* August 2, 2019, Volume 3: System Demonstrations_, pages 111-116. Association for Computational Linguistics, 2019. doi: 10.18653/v1/p19-3019. URL https://doi.org/10.18653/v1/p19-3019.\n* Abdelnabi and Fritz [2021] Sahar Abdelnabi and Mario Fritz. Adversarial watermarking transformer: Towards tracing text provenance with data hiding. In _42nd IEEE Symposium on Security and Privacy, SP 2021, San Francisco, CA, USA, 24-27 May 2021_, pages 121-140. IEEE, 2021. doi: 10.1109/SP40001.2021.00083. URL https://doi.org/10.1109/SP40001.2021.00083.\n* Grinbaum and Adomaiitis [2022] Alexei Grinbaum and Laurynas Adomaiitis. The ethical need for watermarks in machine-generated language. _CoRR_, abs/2209.03118, 2022. doi: 10.48550/arXiv.2209.03118. URL https://doi.org/10.48550/arXiv.2209.03118.\n* Devlin et al. [2019] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)_, pages 4171-4186. Association for Computational Linguistics, 2019. doi: 10.18653/v1/n19-1423. URL https://doi.org/10.18653/v1/n19-1423.\n* Radford et al. [2019] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. _OpenAI blog_, 1(8):9, 2019.\n* Gou et al. [2022] Yunhao Gou, Tom Ko, Hansi Yang, James T. Kwok, Yu Zhang, and Mingxuan Wang. Leveraging per image-token consistency for vision-language pre-training. _CoRR_, abs/2211.15398, 2022. doi: 10.48550/arXiv.2211.15398. URL https://doi.org/10.48550/arXiv.2211.15398.\n\n * Dong et al. [2022] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. A survey for in-context learning. _arXiv preprint arXiv:2301.00234_, 2022.\n* Kim et al. [2022] Hyuhng Joon Kim, Hyunsoo Cho, Junyeob Kim, Taeuk Kim, Kang Min Yoo, and Sang-goo Lee. Self-generated in-context learning: Leveraging auto-regressive language models as a demonstration generator. _CoRR_, abs/2206.08082, 2022. doi: 10.48550/arXiv.2206.08082. URL https://doi.org/10.48550/arXiv.2206.08082.\n* Gonen et al. [2022] Hila Gonen, Srini Iyer, Terra Blevins, Noah A. Smith, and Luke Zettlemoyer. Demystifying prompts in language models via perplexity estimation. _CoRR_, abs/2212.04037, 2022. doi: 10.48550/arXiv.2212.04037. URL https://doi.org/10.48550/arXiv.2212.04037.\n* Rubin et al. [2022] Ohad Rubin, Jonathan Herzig, and Jonathan Berant. Learning to retrieve prompts for in-context learning. In Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz, editors, _Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022, Seattle, WA, United States, July 10-15, 2022_, pages 2655-2671. Association for Computational Linguistics, 2022. doi: 10.18653/v1/2022.naacl-main.191. URL https://doi.org/10.18653/v1/2022.naacl-main.191.\n* Liu et al. [2021] Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What makes good in-context examples for gpt-3? _arXiv preprint arXiv:2101.06804_, 2021.\n* Wei et al. [2022] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. _arXiv preprint arXiv:2201.11903_, 2022.\n* Zhang et al. [2022] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in large language models. _arXiv preprint arXiv:2210.03493_, 2022.\n* Miller [1998] George A Miller. _WordNet: An electronic lexical database_. MIT press, 1998.\n* Rajpurkar et al. [2016] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ Questions for Machine Comprehension of Text. _arXiv e-prints_, art. arXiv:1606.05250, 2016.\n* Fan et al. [2019] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5: long form question answering. In Anna Korhonen, David R. Traum, and Lluis Marquez, editors, _Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers_, pages 3558-3567. Association for Computational Linguistics, 2019. doi: 10.18653/v1/p19-1346. URL https://doi.org/10.18653/v1/p19-1346.\n* Zhang et al. [2015] Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada_, pages 649-657, 2015. URL https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f8dc8b4be867a9a02-Abstract.html.\n* Liu et al. [2019] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining approach. _CoRR_, abs/1907.11692, 2019. URL http://arxiv.org/abs/1907.11692.\n* Mireshghallah et al. [2023] Fatemehsadat Mireshghallah, Justus Mattern, Sicun Gao, Reza Shokri, and Taylor Berg-Kirkpatrick. Smaller language models are better black-box machine-generated text detectors. _CoRR_, abs/2305.09859, 2023. doi: 10.48550/arXiv.2305.09859. URL https://doi.org/10.48550/arXiv.2305.09859.\n* OpenAI [2023] OpenAI. Openai ai text classifier, January 2023. URL https://beta.openai.com/ai-text-classifier.\n\n * Chiang et al. [2023] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, 2023.\n* June 1, 2003_. The Association for Computational Linguistics, 2003. URL https://aclanthology.org/N03-1033/.\n* Ni et al. [2022] Jianmo Ni, Gustavo Hernandez Abrego, Noah Constant, Ji Ma, Keith B. Hall, Daniel Cer, and Yinfei Yang. Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, _Findings of the Association for Computational Linguistics: ACL 2022, Dublin, Ireland, May 22-27, 2022_, pages 1864-1874. Association for Computational Linguistics, 2022. doi: 10.18653/v1/2022.findings-acl.146. URL https://doi.org/10.18653/v1/2022.findings-acl.146.\n* Muennighoff et al. [2023] Niklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers. MTEB: massive text embedding benchmark. In Andreas Vlachos and Isabelle Augenstein, editors, _Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2023, Dubrovnik, Croatia, May 2-6, 2023_, pages 2006-2029. Association for Computational Linguistics, 2023. URL https://aclanthology.org/2023.eacl-main.148.\n* Raffel et al. [2020] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _Journal of Machine Learning Research_, 21(140):1-67, 2020. URL http://jmlr.org/papers/v21/20-074.html.\n\n Implementation Details\n\n### Sico\n\n#### a.1.1 Feature extraction\n\nIn feature extraction step, we instruct LLM to extract 5 features and calculate the utility score \\(\\mathcal{U}\\) of prompts encompassing each of these features. Then we select the feature with the highest utility for further steps. The goal of this step is to find a good feature to accelerate process, and make the whole process stable. Because sometimes LLM cannot extract useful features to evade detectors. The pseudo-code illustrating this selection process is outlined in Algorithm 3. Table 7 presents the prompt template used for feature extraction. Here, \\(K\\) text pairs generated by AI and Human are positioned within their respective positions. Table 20 shows the examples for feature extracted by LLM.\n\n```\n1:list of features \\(T_{\\text{feature}}\\), prompt utility function \\(\\mathcal{U}(\\cdot)\\)\n2:Initialize \\(t^{*}_{\\text{feature}}\\leftarrow\\emptyset\\)\n3:Initialize \\(\\mathcal{U}_{max}\\leftarrow-\\infty\\)\n4:for each feature \\(t_{\\text{feature},i}\\) in \\(T_{\\text{feature}}\\)do\n5: Construct prompt \\(p_{i}\\gets t_{\\text{feature},i}\\oplus p_{\\text{task}}\\)\n6:if\\(\\mathcal{U}(p_{i})>\\mathcal{U}_{max}\\)then\n7:\\(t^{*}_{\\text{feature}}\\gets t_{\\text{feature},i}\\)\n8:\\(\\mathcal{U}_{max}\\leftarrow\\mathcal{U}(p_{i})\\)\n9:endif\n10:endfor\n11:return\\(t^{*}_{\\text{feature}}\\) ```\n\n**Algorithm 3** Feature selections\n\nLLM consistently extract useful features.To test if LLM can reliably extract useful features, we conducted three separated experiments by running three feature extractions on different sets of human-written text and AI-generated text. We use three different extracted features to guide LLM generation and test the AUC drop after adopting 3 different features compared with the originally generated text on the Writing task. Table 8 shows the results, indicating that LLM consistently extracts useful features for detector evasion from different examples.\n\n#### a.1.2 Task instructions\n\nTable 9 shows the actual task instruction \\(p_{\\text{task}}\\) we used in SICO. As mentioned in Section 3, feature text \\(t_{\\text{feature}}\\) at first step will be inserted before these task instructions. The \"Paraphrase\" instruction is \\(p_{\\text{para}}\\) used in paraphrase generation for substitution (Line 6 of Algorithm 1).\n\n\nTable 8: AUC drop of different features extracted based on different human-written and AI-generated text.\n\n \n\n#### a.1.3 Word Substitution\n\nWe employ WordNet synsets to derive synonyms for given words. During optimization of in-context examples, we only substitute content words, namely nouns, verbs, adjectives, and adverbs. Furthermore, we part-of-speech (POS) tag of the synonym to ensure it aligns with the original word. For POS tagging, we utilize the Stanford POS Tagger [40]. Additionally, to maintain fluency in the modified text after substitution, we employed a pretrained mask language model to exclude synonyms with low likelihood. In experiment we use RoBERTa-base model [36].\n\n### Baselines\n\n#### ", "a.2.1 Dipper": "\n\nWe choose the best evasion performance parameter setting from the original paper [13], which is 60 for lexical diversity and 60 for re-ordering. And we set sampling temperature to 0.75, following the original implementation.\n\n#### ", "a.2.2 Human prompt": "\n\nWe carefully design a paraphrase prompt based on the detection idea of GPTzero [12], which distinguish the AI-generated content from Human-written by _perplexity_ and _burstiness_, stated by its creator5. _Perplexity_ is the concept raised in NLP field, which measures how well a language model predicts a text sample. A text with a lower perplexity score indicates that the language model is better at calculating the next word that is likely to occur in a given sequence. On the other hand, _burtiness_ basically measures the variation between sentences, including sentence length and structures. The lower the values for these two factors, the more likely it is that a text was produced by an AI. Table 10 shows the prompt we designed.\n\nFootnote 5: https://theconversation.com/we-pitted-chatgpt-against-tools-for-detecting-ai-written-text-and-the-results-are-troubling-199774\n\n## ", "Appendix B Extra Experiments": "\n\n### ", "Detection capability of human": "\n\nWe conducted a experiment of real-life human detection rate. We randomly selected 150 examples (50 each from AI, SICO-Para, and human responses) from QA task. Four human annotators were then asked to identify whether each answer was AI-generated or human-written. Table 11 shows that SICO remarkably reduces the probability of being recognized by humans. Additionally, the data also reveals that human detection capabilities are not highly accurate compared with machine detectors.\n\n\nTable 9: Task instructions of each task.\n\nApproximately half of AI-generated texts went undetected, and 10% of human-written texts were mistakenly identified as AI-generated.\n\n### ", "Semantic preserving": "\n\nWe measure semantic similarity using t5-based sentence encoder [41], which leads in semantic text similarity task of MTEB benchmark [42]. Table 12 reports a comparison of the cosine similarity of text before and after paraphrasing by different methods.\n\nOur methods successfully preserves the semantic meaning during paraphrasing, and beats the other specifically trained paraphraser. Paraphrasing directly using GPT-3.5 yields the most promising results.\n\n### ", "Consistent effectiveness of SICO": "\n\nSICO consistently procures optimal detection evasion prompts, irrespective of the diversity in AI-human text pairs and randomized samples drawn from the LLMs. This demonstrates the effectiveness of SICO in various initial conditions and settings, highlighting its applicability to diverse scenarios. Figure 4 presents the AI probability of the paraphrased training data, generated by LLM using the current best prompt, denoted as \\(\\mathcal{U}(p^{*})\\) in Equation 1. SICO successfully optimizes the initial prompt (at step 0) and converges to a low level in three separate trials across all three tasks, with different AI-human text pairs.\n\n### ", "SICO performs better against more capable detectors": "\n\nWe use the detectors' performance on the original AI-generated text to represent their base capability. The SICO advancing performance is measured by the AUC difference between best of SICO-Para or\n\n\nTable 10: Human-designed prompt to evade AI-generated text detection.\n\n\nTable 11: Detection rate from human annotators. \u2019AI %\u2019 indicates the proportion of texts identified as AI-generated by annotators.\n\n\nTable 12: Cosine similarity between original AI-generated text and their respective paraphrased versions using different methods. The best scores in each task are highlighted in **bold**.\n\n SICO-Gen and the best-performing paraphraser baselines. The statistical Pearson correlation is \\(0.47\\) with a p-value of \\(0.048\\), indicating a moderate positive correlation.\n\n## ", "Appendix C Detectors": "", "GPT-3.5 Detector": " (GPT3-D) [8]: a RoBERTa model [36] fine-tuned on text generated by GPT-3.5. (ii) ", "GPT2 Detector": "", "DetectGPT": " [10], exploits the phenomenon that AI-generated text tends to lie in the negative curvature regions of log probability of text.\n\n The watermarking methods involve modifying the LLM's text generation process to imprint specific patterns on the generated text, such that it can be detected [20; 21; 11]. Although the proposed method SICO primarily focuses on the first two types of detection methods, it can also help evade watermarking when acted as an external paraphraser, as shown in Appendix G.\n\n### In-context learning\n\nWith the increasing scales of models and corpora [22; 23; 2; 24], LLMs have demonstrated the in-context learning (ICL) ability, allowing them to perform tasks with only a few examples provided as demonstrations [1]. Recent studies have focused on designing demonstrations during inference, which can be divided into demonstration selection, ordering, and formatting [25]. Specifically, demonstrations can be selected based on unsupervised metrics or supervised strategies [26; 27; 28]. For ordering, Liu et al. [29] sorted examples by their distances to the input. Regarding demonstration formatting, Wei et al. [30] proposed the so-called chain-of-thoughts (COT) format, and subsequent works have developed automatic COT [31]. In contrast to these works, we focus on iteratively optimizing demonstrations through substitutions. In principle, the proposed method SICO can be used in combination with the above-mentioned methods, potentially leading to improved performance.\n\n## 3 Substitution-based in-context example optimization (SICO)\n\nThe illustration of SICO is presented in Figure 1. First, LLM is asked to extract language features of human-written text. Then, the in-context examples are initialized and optimized. The final prompt is composed of the feature, task instruction, and optimized in-context examples. Below, we first describe how to evaluate a prompt during its optimization and then elaborate all the steps of SICO.\n\n### Prompt evaluation\n\nGiven a natural language processing task, denote the task input as \\(x\\). To assess the utility of a prompt \\(p\\), we first collect a set of task inputs, \\(X_{eval}\\). For each input \\(x\\in X_{eval}\\), \\(p\\) and \\(x\\) are first concatenated (denoted by \\(p\\oplus x\\)) and fed into the LLM, whose output text (denoted by LLM\\((p\\oplus x)\\)) is then classified by a proxy detector. Let \\(\\mathcal{P}_{\\text{AI}}\\) be the predicted probability of LLM\\((p\\oplus x)\\) to be AI-generated, then the utility score of prompt \\(p\\), denoted by \\(\\mathcal{U}(p)\\), is defined as one minus the averaged predicted\n\nFigure 1: Illustration of how SICO generates prompts for the question answering task. The probability \\(P_{\\text{AI}}\\), as predicted by the proxy detector, indicates the likelihood that the given text is AI-generated. Once SICO prompt is constructed, it serves as a template, allowing users to insert various task inputs (highlighted in purple text).\n\n probability across \\(X_{eval}\\) (the higher \\(\\mathcal{U}\\), the better):\n\n\\[\\mathcal{U}(p)=1-\\frac{1}{|\\mathbf{X}_{eval}|}\\sum_{x\\in X_{eval}}\\mathcal{P}_{ \\text{AI}}(\\text{LLM}(p\\oplus x)).\\] (1)\n\n### Prompt Construction\n\nData collectionWe first collect a set of \\(K\\) triplets, i.e., \\(D=\\{(x^{k}_{\\text{ic}},y^{k}_{\\text{AI}},y^{k}_{\\text{human}})\\}_{k=1}^{K}\\), where \\(x^{k}_{\\text{ic}}\\) is a task input and \\(y^{k}_{\\text{AI}},y^{k}_{\\text{human}}\\) are the corresponding outputs generated by the LLM and humans, respectively. Note \\(D\\) is used for prompt construction and it is independent of \\(X_{eval}\\) which is used for prompt evaluation.\n\nFeature extractionThis step involves the \\(K\\) pairs of AI-generated and human-written outputs from \\(D\\), denoted by \\(\\{(y^{k}_{\\text{AI}},y^{k}_{\\text{human}})\\}_{k=1}^{K}\\). We provide LLM with these pairs and ask LLM to extract the distinct linguistic features of human-written text, denoted as \\(t_{\\text{feature}}\\).\n\nIn-context example optimizationWe initialize the in-context examples as \\((x^{k}_{\\text{ic}},y^{k}_{\\text{ic}})\\), where \\(y^{k}_{\\text{ic}}\\) is generated by paraphrasing \\(y^{k}_{\\text{AI}}\\). More specifically, the text feature \\(t_{\\text{feature}}\\) is concatenated with a paraphrasing instruction to instruct LLM to paraphrase \\(y^{k}_{\\text{AI}}\\) to generate \\(y^{k}_{\\text{ic}}\\) .\n\nThen the in-context output \\(y_{\\text{ic}}\\) is iteratively optimized to be less AI-like, which is directed by the proxy detector. By presenting more and more representative in-context demonstrations to LLM, it is expected to understand how to generate human-like outputs. Formally, the optimization goal can be expressed as:\n\n\\[y^{*}_{\\text{ic}}=\\operatorname*{arg\\,min}_{y^{\\prime}_{\\text{ic}}\\in\\text{SIM }(y^{\\prime}_{\\text{ic}})}\\mathcal{P}_{\\text{AI}}(y^{\\prime}_{\\text{ic}}),\\] (2)\n\nwhere \\(\\text{SIM}(y_{\\text{ic}})\\) denotes the set of text that is semantically similar to \\(y_{\\text{ic}}\\). The goal of setting such semantic restriction is to maintain the usability of the text during optimization. In SICO, we generate semantically similar text by replacing words with their synonyms and rephrasing sentences. This is explained in detail below.\n\nSubstitution typeTo generate \\(y^{\\prime}_{\\text{ic}}\\) that is semantically similar to \\(y_{\\text{ic}}\\), we employ substitution at word level and sentence level in turn. For word-level substitution, we use WordNet [32], a lexical database of English words, to construct a synonym substitution set. We restrict substitutions to content words that carry meanings and ensure that the substitution would not change the part-of-speech tags. We use a mask language model to filter out the candidate words that not fits the context. For sentence-level substitution, we utilize a paraphrasing instruction combined with extracted feature, denoted as \\(t_{\\text{feature}}\\oplus p_{\\text{para}}\\). This combined instruction is used to prompt LLM to generate paraphrases for each sentence in \\(y_{\\text{ic}}\\).\n\n**Algorithm** As illustrated in Algorithm 1, SICO would optimize \\(\\{y_{\\text{ic}}^{k}\\}_{k=1}^{K}\\) for \\(N\\) iterations (lines 4-17). At each iteration, each \\(y_{\\text{ic}}^{k}\\) would be optimized by greedy substitution (line 11), as presented in Algorithm 2. Specifically, for the \\(i\\)-th original word/sentence \\(y_{i}\\) in the text \\(y\\), let \\(C_{i,j}\\) denote its \\(j\\)-th synonym/paraphrase, and let SUB\\((y_{i},C_{i,j})\\) denote the new text resulting from substituting \\(y_{i}\\) with \\(C_{i,j}\\). For each \\(y_{i}\\), SICO finds the best synonym/paraphrase \\(C_{i,*}\\) by checking which \\(C_{i,j}\\) gives the lowest AI-probability when substituting \\(y_{i}\\) (Line 1 in Algorithm 2).\n\nAfter obtaining the optimized in-context output \\(\\hat{y}_{\\text{ic}}\\), the new prompt is constructed as \\(\\hat{p}=t_{\\text{feature}}\\oplus p_{\\text{task}}\\oplus\\{(x_{\\text{ic}}^{k}, \\hat{y}_{\\text{ic}}^{k})\\}_{k=1}^{K}\\), where \\(p_{\\text{task}}\\) is the task instruction, as illustrated in Figure 1. Then \\(\\hat{p}\\) would be compared with the current best prompt \\(p^{*}\\) based on their utility scores as defined in Eq. (1). If \\(\\hat{p}\\) scores higher, SICO replaces \\(p^{*}\\) with it. After \\(N\\) iterations, \\(p^{*}\\) is returned as the final prompt. More implementation details of SICO are shown in Appendix A.\n\n### SICO for Paraphrasing\n\nThe approach described above directly generates the task output to evade detectors. We refer to this direct approach as SICO-Gen. Alternatively, SICO can be easily adapted for paraphrasing, which we term as SICO-Para. Instead of direct generation, SICO-Para evades detectors in two steps. Initially, LLM produces an intermediate task output, typically incapable of evading detectors. Then. this output is paraphrased using SICO-Para to successfully evade detectors. Switching from SICO-Gen to SICO-Para requires only two adjustments: (1) the task input \\(x\\) is set to the AI-generated output text in \\(D\\) and \\(\\mathbf{X}_{eval}\\); (2) task instruction \\(p_{\\text{task}}\\) is modified to paraphrasing instruction.\n\n## 4 Experiments\n\n### Experimental setup\n\n**Tasks & datasets** We consider three real-world tasks that are susceptible to the misuse of LLMs, i.e., academic essay writing (Writing), open-ended question answering (QA), and fake review generation (Review). We use GPT-3.5, one of the most powerful LLMs, to complete the tasks and generate text in our experiments.\n\nFor academic writing, we employ Wikipedia paragraphs from SQuAD dataset [33] as human-written text. Following the approach in Mitchell et al. [10], we use the first 30 words of these paragraphs as task inputs and ask GPT-3.5 to complete the rest. For open-ended question answering, we sample questions from Eli5 [34] dataset and ask GPT-3.5 to generate answers, following Krishna et al. [13]. For fake review generation, we first instruct GPT-3.5 to extract the business name and five keywords from human-written reviews from Yelp dataset [35], and then generate fake reviews based on the extracted information with specified sentiment. For each task, we collect 200 examples from GPT-3.5 (called original AI-generated text) and 200 human-written examples from corresponding dataset. More details about dataset can be found in Appendix F.\n\n**Detectors** Six representative detectors belonging to three different types are considered. Details of these detectors can be found in Appendix C.\n\n_Training-based methods._ (i) GPT-3.5 Detector (GPT3-D) [8]: a RoBERTa model [36] fine-tuned on text generated by GPT-3.5. (ii) GPT2 Detector (GPT2-D) [9]: a RoBERTa detector officially released by OpenAI, fine-tuned on GPT2-generated text.\n\n_Statistical methods._ (i) DetectGPT [10] evaluates the variation in a language model's log probability by introducing minor perturbations to the detected text. (ii) ", "Log-Rank": " [10] is a statistical method that employs a language model to compute the mean prediction rank of each token in a text, given its preceding context. We utilize a relatively smaller language model, GPT2-medium [23], for both methods. Because Mireshghallah et al. [37] find that smaller language models have better detection performance than larger ones.\n\n _APIs_.3 (i) GPTzero [12] is a widely-used commercial detector, cooperated with many academic organizations. (ii) OpenAI Detector (OpenAI-D) [38] is officially offered by OpenAI, fine-tuned from a language model.\n\nFootnote 3: We consider the API versions of May 15, 2023. For OpenAI-D, we follow the implementation of Krishna et al. [13].\n\n**Baselines** We consider four paraphrasing baselines that evade detectors by paraphrasing the original AI-generated text. Specifically, two recently proposed methods are considered: (1) _Parrot_[14] and (2) _DIPPER_[13]. Both methods employ an external neural network specifically trained for paraphrasing. In addition, we include two prompting baselines to instruct GPT-3.5 to paraphrase the original AI-generated text: (3) _GPT-Para_ that uses the straightforward instruction _\"Paraphrase this\"_ to assess the capabilities of GPT-3.5 without intricate prompt engineering, and (4) _Human Prompt_ that utilizes a human-designed prompt. More details can be found in Appendix A.2.\n\n**Evaluation metrics** We use the area under the ROC curve (AUC) to measure the performance of detectors. The ROC curves are also illustrated to show the detection performance under different classification thresholds. For each task, we evaluate AUC score using 200 human-written text and 200 original or paraphrased AI-generated text. For each task input, we run each evasion method _only once_, instead of repeating multiple times until successful evasion, to simulate real-world scenarios where the target detector is inaccessible.\n\n**Experimental settings** We set \\(|\\mathbf{X}_{eval}|=32\\), \\(K=8\\), \\(N=6\\), and use GPT-3.5, specifically _gpt-3.5-turbo-0301_, as the LLM, where the inference parameters are kept in default. And we use GPT3-D as the proxy detector. Experiments using other LLMs and proxy detectors are presented in Section 5.2.\n\n### Evasion Performance and Analysis\n\nTable 1 presents the performance of SICO and other baselines against six detectors in AUC score. SICO consistently outperforms other baselines by a substantial margin in all cases. Notably, in most cases, SICO reduces the AUC score to less than 0.5, equivalent to the expected performance of a random classifier. Figure 2 shows the ROC curves of evasion methods on academic writing task. One\n\n\nTable 1: AUC scores of detectors on text generated by different methods. \u201c-\u201d refers to the detector\u2019s AUC score on the original AI-generated text, without applying any evasion methods. Symbol \u2018*\u2019 represents that SICO uses GPT3-D as the proxy detector for prompt construction. For each detector, the lowest AUC score is indicated in **bold**, and the second-lowest is underlined.\n\n can clearly observe that SICO curves lie below others along different thresholds, often lower than the random classifier curve. More evasion results including ROC cures and detection rates are shown in Appendix H.\n\nOne interesting trend is that SICO-Para consistently outperforms SICO-Gen against statistical detectors, i.e., DetectGPT and Log-Rank. We speculate this performance difference comes from the varying influence of the prompt on the generated text between the two methods. In SICO-Para, the distribution of generated text is largely influenced by the original AI-generated text, which is in the prompt. However, in SICO-Gen, the distribution of generated text depends more on the previously generated text. Given that statistical detectors have access to the newly generated text but not the prompt, their estimation of token probability becomes less accurate for SICO-Para text, thus misleading the detection. It might also explain why GPT-Para can reduce the performance of statistical detectors.\n\n### Human Evaluation\n\nFrom the users' perspective, using AI-generated text goes beyond evading detection systems; the usability of text is equally critical. For example, for academic writing task, users expect the text to be readable, properly formatted, and relevant to the given topic. Therefore, we evaluate the usability of text based on two criteria: readability and task completion rate. For each task, we randomly sample 200 examples generated by four methods (50 per method), including human-written text. Then we ask three human annotators to rate the readability of text on a scale from 1 to 5, and judge if the text accomplishes the task's goal. More details of human evaluation are shown in Appendix D.\n\nAs shown in Table 2, both SICO-Gen and SICO-Para demonstrate superior performance over DIPPER in terms of task completion and readability over three tasks. Furthermore, SICO-generated text performs competitively compared with human-written text in both metrics, with a negligible difference less than 0.1. In contrast, DIPPER exhibits inferior performance relative to human-written text, particularly with a notable 0.27 decline in readability.\n\nFigure 2: ROC curves of six detectors on the text generated by different evasion methods on academic writing task.\n\n \n\n### Real-life Experiments\n\nTo further assess the applicability of SICO in real-world, we simulate one potential misuse case of LLM, where SICO is deployed as an automatic reply bot on Reddit4, a popular online social platform. We wrote a script to monitor the new posts submitted in the community for asking questions and used GPT-3.5 equipped with SICO-Para to automatically reply them. The prompt we used is trained for question answering task. On Reddit, except for giving comments, users can express their approval of other's responses by clicking the \"like\" or \"dislike\" button. To minimize the social impact, we limit the number of responses to 40 and deleted them after collecting results.\n\nFootnote 4: https://www.reddit.com/\n\nThe quantitative results in Table 3 demonstrate that users generally react positively to the text from SICO. Specifically, 40% of the responses from SICO receive \"likes\" from Reddit users, significantly higher than the 2.5% that are disliked. The remaining 57.5% of responses go unnoticed, which is common in social media. Besides, in 12.5% of cases, users left additional comments to express their approval or to ask further questions, demonstrating their engagement and interest in SICO's response. Figure 3 presents two SICO's responses that got approval from users, as indicated by \"likes\" and comments.\n\n### Cost Efficiency\n\nIn terms of the data prerequisite, SICO only needs \\(K+|\\mathbf{X}_{eval}|\\) human-written input-output examples to build prompt, which is \\(8+32=40\\) in the experiments. The other AI-generated text can be produced by LLM leveraging these human samples. Furthermore, SICO offers the advantage of low cost for prompt construction. Based on three repeated runs, the actual USD costs of SICO-Para are \\(1.04\\pm 0.04\\), \\(1.08\\pm 0.05\\), and \\(0.75\\pm 0.04\\) for Writing, QA, Review tasks, respectively.\n\n## 5 Further Experiments\n\n### Ablation Study\n\nWe conducted an ablation study over academic writing task to to evaluate the contribution of individual components within the SICO framework. \"Human-ICE\" denotes the approach where human-written text is directly utilized as the in-context example for constructing the prompt. \"w/o feature\" and \"w/o ICE\" refer to the prompts without feature text and the optimized in-context examples, respectively. \"w/o OPT\" represents the initial prompt before optimization (see Line 3 in Algorithm 1). In our experiment, we explore SICO-Para on three types of detectors: GPT3-D, OpenAI-D and DetectGPT.\n\nResults in Table 4 shows that directly using human-written text is ineffective, even making the detection more accurate. We speculate that the human-written examples are too heterogeneous\n\n\nTable 2: Human evaluation results. \u201cAvg.D.\u201d represents the average difference between the results achieved by the evasion method and the results achieved by human-written text on the three tasks. The best value of each task is set **bold**.\n\n\nTable 3: Quantitative results of Reddit test.\n\nand characterized in multiple ways, so LLM cannot effectively learn their attributes. Besides, the importance of feature text is comparatively less than that of optimized in-context examples. Furthermore, the result reveals the significant role of the optimization step in SICO. Using in-context examples that are not optimized is essentially equivalent to not using any in-context examples.\n\n### SICO with Different Proxy Detectors and LLMs\n\nAs described in Section 3, SICO requires a proxy detector and a LLM to construct a prompt. In this experiment, we explore the performance of SICO-Para on writing task, using three types of proxy detectors: (1) training-based model GPT-3.5 detector, (2) API detector GPTzero, and (3) statistical method DetectGPT. For different LLMs, we adopt Vicuna-13B [39], an open-source chatbot fine-tuned from LLaMa [3]. Results in Table 5 show that SICO maintains a high degree of detection evasion performance, regardless of proxy detectors or LLMs. In most cases, SICO manages to reduce the AUC of detectors by approximately 0.4.\n\n\nTable 4: The AUC scores of detectors on text generated by different methods. \u201c\u2013\u201d indicates the case where no evasion method is used. \u2019AVG\u2019 represents the average AUC scores across detectors.\n\nFigure 3: Screenshots of replies from SICO that got approved by Reddit users. (Left) Approval is shown by the number of \u201clike\u201ds. (Right) The original poster express thanks to SICO\u2019s response.\n\n\nTable 5: The AUC scores of SICO using different proxy detectors and LLMs on writing task. The first line indicates the performance without applying any evasion method.\n\n \n\n### Examples of the generated text\n\nTable 6 presents the fake reviews generated by SICO-Gen and SICO-Para. The generated text shows high readability and fulfill the task's goal, successfully mentioning all keywords and generating positive reviews of the specified object. The AI probability, denoted as \\(\\mathcal{P}_{\\text{AI}}\\) in the table, is determined by the GPT3-D. More examples are shown in Appendix I.\n\n## 6 Conclusion\n\nIn conclusion, we have proposed a novel in-context learning approach, SICO, designed to guide LLMs in generating text that can effectively evade detectors. Our extensive experiments on evasion demonstrate the superior performance of SICO, which significantly reduces the detection capabilities of existing AI text detectors across three tasks. A comprehensive human evaluation shows SICO text can achieve human-level readability and task completion rates. The experiment in the wild shows the functionality of SICO in real life.\n\nLooking ahead, SICO could act as a data generator and be integrated during the training phase of AI detectors, which may enhance their robustness. Furthermore, the core concept of SICO, namely, substitution-based in-context learning, could be applied to a variety of text generation tasks. We believe that this opens up new avenues for future research in the fields of text generation and in-context learning.\n\n## 7 Ethics statement\n\nThe intention of this paper is not to offer a potential method for evading AI-generated text detection systems. Instead, our aim is to raise awareness within the broader community about the vulnerabilities of existing AI-generated text detection systems to such technology. As many LLMs are public available and free to use, many people can adjust their prompt and generate text that evades these detectors. Given the ease of evasion illustrated in this study, these detectors are not robust yet.\n\nWe hope the research community can stress test their detectors against text generated by carefully crafted prompt, and create more robust detectors in the future. To support the research in this field, we will make our training methods and relevant data/code publicly available.\n\n\nTable 6: Fake reviews generated by SICO. The first line shows the task input of review generation.\n\n\n\n## References\n\n* Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.\n* Chowdhery et al. [2022] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. _arXiv preprint arXiv:2204.02311_, 2022.\n* Touvron et al. [2023] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.\n* and machine-based detection. In Leonard Barolli, Flora Amato, Francesco Moscato, Tomoya Enokido, and Makoto Takizawa, editors, _Advanced Information Networking and Applications\n- Proceedings of the 34th International Conference on Advanced Information Networking and Applications, AINA-2020, Caserta, Italy, 15-17 April_, volume 1151 of _Advances in Intelligent Systems and Computing_, pages 1341-1354. Springer, 2020. doi: 10.1007/978-3-030-44041-1_114. URL https://doi.org/10.1007/978-3-030-44041-1_114.\n* Lin et al. [2022] Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022_, pages 3214-3252. Association for Computational Linguistics, 2022. doi: 10.18653/v1/2022.acl-long.229. URL https://doi.org/10.18653/v1/2022.acl-long.229.\n* Stokel-Walker [2022] Chris Stokel-Walker. Ai bot chatgpt writes smart essays-should academics worry? _Nature_, 2022.\n* Stokelverflow [2023] StackOverflow. Temporary policy: Chatgpt is banned, 2023. URL https://meta.stackoverflow.com/questions/421831/temporary-policy-chatgpt-is-banned.\n* Guo et al. [2023] Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng Wu. How close is chatgpt to human experts? comparison corpus, evaluation, and detection. _CoRR_, abs/2301.07597, 2023. doi: 10.48550/arXiv.2301.07597. URL https://doi.org/10.48550/arXiv.2301.07597.\n* Solaiman et al. [2019] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford, and Jasmine Wang. Release strategies and the social impacts of language models. _CoRR_, abs/1908.09203, 2019. URL http://arxiv.org/abs/1908.09203.\n* Mitchell et al. [2023] Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D. Manning, and Chelsea Finn. Detectgpt: Zero-shot machine-generated text detection using probability curvature. _CoRR_, abs/2301.11305, 2023. doi: 10.48550/arXiv.2301.11305. URL https://doi.org/10.48550/arXiv.2301.11305.\n* Kirchenbauer et al. [2023] John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and Tom Goldstein. A watermark for large language models. _CoRR_, abs/2301.10226, 2023. doi: 10.48550/arXiv.2301.10226. URL https://doi.org/10.48550/arXiv.2301.10226.\n* Tian [2023] Edward Tian. Gptzero: an ai detector, 2023. URL https://gptzero.me/.\n* Krishna et al. [2023] Kalpesh Krishna, Yixiao Song, Marzena Karpinska, John Wieting, and Mohit Iyyer. Paraphrasing evades detectors of ai-generated text, but retrieval is an effective defense. _CoRR_, abs/2303.13408, 2023. doi: 10.48550/arXiv.2303.13408. URL https://doi.org/10.48550/arXiv.2303.13408.\n\n* Sadasivan et al. [2023] Vinu Sankar Sadasivan, Aounon Kumar, Sriram Balasubramanian, Wenxiao Wang, and Soheil Feizi. Can ai-generated text be reliably detected? _CoRR_, abs/2303.11156, 2023. doi: 10.48550/arXiv.2303.11156. URL https://doi.org/10.48550/arXiv.2303.11156.\n* an adversarial study. In _Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 2: Short Papers_. The Association for Computer Linguistics, 2016. doi: 10.18653/v1/p16-2057. URL https://doi.org/10.18653/v1/p16-2057.\n* Zellers et al. [2019] Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. Defending against neural fake news. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alche-Buc, Emily B. Fox, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada_, pages 9051-9062, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/3e9f0fc9b2f89e043bc6233994dfcf76-Abstract.html.\n* Lavergne et al. [2008] Thomas Lavergne, Tanguy Urvoy, and Francois Yvon. Detecting fake content with relative entropy scoring. In Benno Stein, Efstathios Stamatatos, and Moshe Koppel, editors, _Proceedings of the ECAI'08 Workshop on Uncovering Plagiarism, Authorship and Social Software Misuse, Patras, Greece, July 22, 2008_, volume 377 of _CEUR Workshop Proceedings_. CEUR-WS.org, 2008. URL https://ceur-ws.org/Vol-377/paper4.pdf.\n* 21st International Conference on Applications of Natural Language to Information Systems, NLDB 2016, Salford, UK, June 22-24, 2016, Proceedings_, volume 9612 of _Lecture Notes in Computer Science_, pages 421-426. Springer, 2016. doi: 10.1007/978-3-319-41754-7_43. URL https://doi.org/10.1007/978-3-319-41754-7_43.\n* August 2, 2019, Volume 3: System Demonstrations_, pages 111-116. Association for Computational Linguistics, 2019. doi: 10.18653/v1/p19-3019. URL https://doi.org/10.18653/v1/p19-3019.\n* Abdelnabi and Fritz [2021] Sahar Abdelnabi and Mario Fritz. Adversarial watermarking transformer: Towards tracing text provenance with data hiding. In _42nd IEEE Symposium on Security and Privacy, SP 2021, San Francisco, CA, USA, 24-27 May 2021_, pages 121-140. IEEE, 2021. doi: 10.1109/SP40001.2021.00083. URL https://doi.org/10.1109/SP40001.2021.00083.\n* Grinbaum and Adomaiitis [2022] Alexei Grinbaum and Laurynas Adomaiitis. The ethical need for watermarks in machine-generated language. _CoRR_, abs/2209.03118, 2022. doi: 10.48550/arXiv.2209.03118. URL https://doi.org/10.48550/arXiv.2209.03118.\n* Devlin et al. [2019] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)_, pages 4171-4186. Association for Computational Linguistics, 2019. doi: 10.18653/v1/n19-1423. URL https://doi.org/10.18653/v1/n19-1423.\n* Radford et al. [2019] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. _OpenAI blog_, 1(8):9, 2019.\n* Gou et al. [2022] Yunhao Gou, Tom Ko, Hansi Yang, James T. Kwok, Yu Zhang, and Mingxuan Wang. Leveraging per image-token consistency for vision-language pre-training. _CoRR_, abs/2211.15398, 2022. doi: 10.48550/arXiv.2211.15398. URL https://doi.org/10.48550/arXiv.2211.15398.\n\n * Dong et al. [2022] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. A survey for in-context learning. _arXiv preprint arXiv:2301.00234_, 2022.\n* Kim et al. [2022] Hyuhng Joon Kim, Hyunsoo Cho, Junyeob Kim, Taeuk Kim, Kang Min Yoo, and Sang-goo Lee. Self-generated in-context learning: Leveraging auto-regressive language models as a demonstration generator. _CoRR_, abs/2206.08082, 2022. doi: 10.48550/arXiv.2206.08082. URL https://doi.org/10.48550/arXiv.2206.08082.\n* Gonen et al. [2022] Hila Gonen, Srini Iyer, Terra Blevins, Noah A. Smith, and Luke Zettlemoyer. Demystifying prompts in language models via perplexity estimation. _CoRR_, abs/2212.04037, 2022. doi: 10.48550/arXiv.2212.04037. URL https://doi.org/10.48550/arXiv.2212.04037.\n* Rubin et al. [2022] Ohad Rubin, Jonathan Herzig, and Jonathan Berant. Learning to retrieve prompts for in-context learning. In Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz, editors, _Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022, Seattle, WA, United States, July 10-15, 2022_, pages 2655-2671. Association for Computational Linguistics, 2022. doi: 10.18653/v1/2022.naacl-main.191. URL https://doi.org/10.18653/v1/2022.naacl-main.191.\n* Liu et al. [2021] Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What makes good in-context examples for gpt-3? _arXiv preprint arXiv:2101.06804_, 2021.\n* Wei et al. [2022] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. _arXiv preprint arXiv:2201.11903_, 2022.\n* Zhang et al. [2022] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in large language models. _arXiv preprint arXiv:2210.03493_, 2022.\n* Miller [1998] George A Miller. _WordNet: An electronic lexical database_. MIT press, 1998.\n* Rajpurkar et al. [2016] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ Questions for Machine Comprehension of Text. _arXiv e-prints_, art. arXiv:1606.05250, 2016.\n* Fan et al. [2019] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5: long form question answering. In Anna Korhonen, David R. Traum, and Lluis Marquez, editors, _Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers_, pages 3558-3567. Association for Computational Linguistics, 2019. doi: 10.18653/v1/p19-1346. URL https://doi.org/10.18653/v1/p19-1346.\n* Zhang et al. [2015] Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada_, pages 649-657, 2015. URL https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f8dc8b4be867a9a02-Abstract.html.\n* Liu et al. [2019] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining approach. _CoRR_, abs/1907.11692, 2019. URL http://arxiv.org/abs/1907.11692.\n* Mireshghallah et al. [2023] Fatemehsadat Mireshghallah, Justus Mattern, Sicun Gao, Reza Shokri, and Taylor Berg-Kirkpatrick. Smaller language models are better black-box machine-generated text detectors. _CoRR_, abs/2305.09859, 2023. doi: 10.48550/arXiv.2305.09859. URL https://doi.org/10.48550/arXiv.2305.09859.\n* OpenAI [2023] OpenAI. Openai ai text classifier, January 2023. URL https://beta.openai.com/ai-text-classifier.\n\n * Chiang et al. [2023] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, 2023.\n* June 1, 2003_. The Association for Computational Linguistics, 2003. URL https://aclanthology.org/N03-1033/.\n* Ni et al. [2022] Jianmo Ni, Gustavo Hernandez Abrego, Noah Constant, Ji Ma, Keith B. Hall, Daniel Cer, and Yinfei Yang. Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, _Findings of the Association for Computational Linguistics: ACL 2022, Dublin, Ireland, May 22-27, 2022_, pages 1864-1874. Association for Computational Linguistics, 2022. doi: 10.18653/v1/2022.findings-acl.146. URL https://doi.org/10.18653/v1/2022.findings-acl.146.\n* Muennighoff et al. [2023] Niklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers. MTEB: massive text embedding benchmark. In Andreas Vlachos and Isabelle Augenstein, editors, _Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2023, Dubrovnik, Croatia, May 2-6, 2023_, pages 2006-2029. Association for Computational Linguistics, 2023. URL https://aclanthology.org/2023.eacl-main.148.\n* Raffel et al. [2020] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _Journal of Machine Learning Research_, 21(140):1-67, 2020. URL http://jmlr.org/papers/v21/20-074.html.\n\n Implementation Details\n\n### Sico\n\n#### a.1.1 Feature extraction\n\nIn feature extraction step, we instruct LLM to extract 5 features and calculate the utility score \\(\\mathcal{U}\\) of prompts encompassing each of these features. Then we select the feature with the highest utility for further steps. The goal of this step is to find a good feature to accelerate process, and make the whole process stable. Because sometimes LLM cannot extract useful features to evade detectors. The pseudo-code illustrating this selection process is outlined in Algorithm 3. Table 7 presents the prompt template used for feature extraction. Here, \\(K\\) text pairs generated by AI and Human are positioned within their respective positions. Table 20 shows the examples for feature extracted by LLM.\n\n```\n1:list of features \\(T_{\\text{feature}}\\), prompt utility function \\(\\mathcal{U}(\\cdot)\\)\n2:Initialize \\(t^{*}_{\\text{feature}}\\leftarrow\\emptyset\\)\n3:Initialize \\(\\mathcal{U}_{max}\\leftarrow-\\infty\\)\n4:for each feature \\(t_{\\text{feature},i}\\) in \\(T_{\\text{feature}}\\)do\n5: Construct prompt \\(p_{i}\\gets t_{\\text{feature},i}\\oplus p_{\\text{task}}\\)\n6:if\\(\\mathcal{U}(p_{i})>\\mathcal{U}_{max}\\)then\n7:\\(t^{*}_{\\text{feature}}\\gets t_{\\text{feature},i}\\)\n8:\\(\\mathcal{U}_{max}\\leftarrow\\mathcal{U}(p_{i})\\)\n9:endif\n10:endfor\n11:return\\(t^{*}_{\\text{feature}}\\) ```\n\n**Algorithm 3** Feature selections\n\nLLM consistently extract useful features.To test if LLM can reliably extract useful features, we conducted three separated experiments by running three feature extractions on different sets of human-written text and AI-generated text. We use three different extracted features to guide LLM generation and test the AUC drop after adopting 3 different features compared with the originally generated text on the Writing task. Table 8 shows the results, indicating that LLM consistently extracts useful features for detector evasion from different examples.\n\n#### a.1.2 Task instructions\n\nTable 9 shows the actual task instruction \\(p_{\\text{task}}\\) we used in SICO. As mentioned in Section 3, feature text \\(t_{\\text{feature}}\\) at first step will be inserted before these task instructions. The \"Paraphrase\" instruction is \\(p_{\\text{para}}\\) used in paraphrase generation for substitution (Line 6 of Algorithm 1).\n\n\nTable 8: AUC drop of different features extracted based on different human-written and AI-generated text.\n\n \n\n#### a.1.3 Word Substitution\n\nWe employ WordNet synsets to derive synonyms for given words. During optimization of in-context examples, we only substitute content words, namely nouns, verbs, adjectives, and adverbs. Furthermore, we part-of-speech (POS) tag of the synonym to ensure it aligns with the original word. For POS tagging, we utilize the Stanford POS Tagger [40]. Additionally, to maintain fluency in the modified text after substitution, we employed a pretrained mask language model to exclude synonyms with low likelihood. In experiment we use RoBERTa-base model [36].\n\n### Baselines\n\n#### a.2.1 Dipper\n\nWe choose the best evasion performance parameter setting from the original paper [13], which is 60 for lexical diversity and 60 for re-ordering. And we set sampling temperature to 0.75, following the original implementation.\n\n#### a.2.2 Human prompt\n\nWe carefully design a paraphrase prompt based on the detection idea of GPTzero [12], which distinguish the AI-generated content from Human-written by _perplexity_ and _burstiness_, stated by its creator5. _Perplexity_ is the concept raised in NLP field, which measures how well a language model predicts a text sample. A text with a lower perplexity score indicates that the language model is better at calculating the next word that is likely to occur in a given sequence. On the other hand, _burtiness_ basically measures the variation between sentences, including sentence length and structures. The lower the values for these two factors, the more likely it is that a text was produced by an AI. Table 10 shows the prompt we designed.\n\nFootnote 5: https://theconversation.com/we-pitted-chatgpt-against-tools-for-detecting-ai-written-text-and-the-results-are-troubling-199774\n\n## Appendix B Extra Experiments\n\n### Detection capability of human\n\nWe conducted a experiment of real-life human detection rate. We randomly selected 150 examples (50 each from AI, SICO-Para, and human responses) from QA task. Four human annotators were then asked to identify whether each answer was AI-generated or human-written. Table 11 shows that SICO remarkably reduces the probability of being recognized by humans. Additionally, the data also reveals that human detection capabilities are not highly accurate compared with machine detectors.\n\n\nTable 9: Task instructions of each task.\n\nApproximately half of AI-generated texts went undetected, and 10% of human-written texts were mistakenly identified as AI-generated.\n\n### Semantic preserving\n\nWe measure semantic similarity using t5-based sentence encoder [41], which leads in semantic text similarity task of MTEB benchmark [42]. Table 12 reports a comparison of the cosine similarity of text before and after paraphrasing by different methods.\n\nOur methods successfully preserves the semantic meaning during paraphrasing, and beats the other specifically trained paraphraser. Paraphrasing directly using GPT-3.5 yields the most promising results.\n\n### Consistent effectiveness of SICO\n\nSICO consistently procures optimal detection evasion prompts, irrespective of the diversity in AI-human text pairs and randomized samples drawn from the LLMs. This demonstrates the effectiveness of SICO in various initial conditions and settings, highlighting its applicability to diverse scenarios. Figure 4 presents the AI probability of the paraphrased training data, generated by LLM using the current best prompt, denoted as \\(\\mathcal{U}(p^{*})\\) in Equation 1. SICO successfully optimizes the initial prompt (at step 0) and converges to a low level in three separate trials across all three tasks, with different AI-human text pairs.\n\n### SICO performs better against more capable detectors\n\nWe use the detectors' performance on the original AI-generated text to represent their base capability. The SICO advancing performance is measured by the AUC difference between best of SICO-Para or\n\n\nTable 10: Human-designed prompt to evade AI-generated text detection.\n\n\nTable 11: Detection rate from human annotators. \u2019AI %\u2019 indicates the proportion of texts identified as AI-generated by annotators.\n\n\nTable 12: Cosine similarity between original AI-generated text and their respective paraphrased versions using different methods. The best scores in each task are highlighted in **bold**.\n\n SICO-Gen and the best-performing paraphraser baselines. The statistical Pearson correlation is \\(0.47\\) with a p-value of \\(0.048\\), indicating a moderate positive correlation.\n\n## Appendix C Detectors\n\nIn this section, we introduce the mechanism and settings of the detectors in our experiments.\n\n### GPT-3.5 Detector\n\nGPT-3.5 detector is trained on Human ChatGPT Comparison Corpus (HC3) dataset [8], which including answers generated by ChatGPT and human. English-version of HC3 dataset contains five splits: reddit-eli5, OpenQA, Wikipedia, medicine and finance. The base model is RoBERTa-base and we use the model that only take answers as input.\n\n### GPT2 Detector\n\nGPT-2 detector is obtained by fine-tuning a RoBERTa model with the outputs of the 1.5B-parameter GPT-2 model. The detector and the GPT-2 output dataset are both provided by OpenAI. Although it is trained on GPT-2 outputs, our experiments shows that it can effectively detect text from GPT-3.5.\n\n### DetectGPT\n\nDetectGPT identifies if a text is generated by a model by observing a unique characteristic: AI-generated text tends to be in areas where the language model's log probability function has a negative curve. Here's how it works: It first perturbs the input text and constructs multiple perturbations of input text. The perturb step is completed by a mask language model. Then it checks the log probability of these variations against the original text by a inner language model. Finally, the text is considered AI-generated if the log probability of the original input text is significantly higher than the log probability of perturbations.\n\nWe use z-score implementation of DetectGPT and set sample number to 100 and replacement ratio to 0.3. The inner language model is GPT2-medium and the mask language model is t5-large [43].\n\n### Log-Rank\n\nLog-Rank method employs the mean prediction rank of each token in a text. Specifically, for each word in a text, given its previous context, it can calculate the absolute rank of this word by an inner language model. Then, for a given text, we compute the score of the text by averaging the rank value of each word. Note that a smaller score denotes the text is more likely to be machine-generated. In experiment, we use GPT2-medium to calculate the rank of tokens to align with the implementation of DetectGPT.\n\nFigure 4: The trajectory of the \\(\\mathcal{U}(p^{*})\\) during prompt optimization. This plot is derived from three distinct training runs on three tasks.\n\n \n\n### ", "GPTZero": "", "OpenAI Detector": " (OpenAI-D) [38] is officially offered by OpenAI, fine-tuned from a language model.\n\nFootnote 3: We consider the API versions of May 15, 2023. For OpenAI-D, we follow the implementation of Krishna et al. [13].\n\n**Baselines** We consider four paraphrasing baselines that evade detectors by paraphrasing the original AI-generated text. Specifically, two recently proposed methods are considered: (1) _Parrot_[14] and (2) _DIPPER_[13]. Both methods employ an external neural network specifically trained for paraphrasing. In addition, we include two prompting baselines to instruct GPT-3.5 to paraphrase the original AI-generated text: (3) _GPT-Para_ that uses the straightforward instruction _\"Paraphrase this\"_ to assess the capabilities of GPT-3.5 without intricate prompt engineering, and (4) _Human Prompt_ that utilizes a human-designed prompt. More details can be found in Appendix A.2.\n\n**Evaluation metrics** We use the area under the ROC curve (AUC) to measure the performance of detectors. The ROC curves are also illustrated to show the detection performance under different classification thresholds. For each task, we evaluate AUC score using 200 human-written text and 200 original or paraphrased AI-generated text. For each task input, we run each evasion method _only once_, instead of repeating multiple times until successful evasion, to simulate real-world scenarios where the target detector is inaccessible.\n\n**Experimental settings** We set \\(|\\mathbf{X}_{eval}|=32\\), \\(K=8\\), \\(N=6\\), and use GPT-3.5, specifically _gpt-3.5-turbo-0301_, as the LLM, where the inference parameters are kept in default. And we use GPT3-D as the proxy detector. Experiments using other LLMs and proxy detectors are presented in Section 5.2.\n\n### Evasion Performance and Analysis\n\nTable 1 presents the performance of SICO and other baselines against six detectors in AUC score. SICO consistently outperforms other baselines by a substantial margin in all cases. Notably, in most cases, SICO reduces the AUC score to less than 0.5, equivalent to the expected performance of a random classifier. Figure 2 shows the ROC curves of evasion methods on academic writing task. One\n\n\nTable 1: AUC scores of detectors on text generated by different methods. \u201c-\u201d refers to the detector\u2019s AUC score on the original AI-generated text, without applying any evasion methods. Symbol \u2018*\u2019 represents that SICO uses GPT3-D as the proxy detector for prompt construction. For each detector, the lowest AUC score is indicated in **bold**, and the second-lowest is underlined.\n\n can clearly observe that SICO curves lie below others along different thresholds, often lower than the random classifier curve. More evasion results including ROC cures and detection rates are shown in Appendix H.\n\nOne interesting trend is that SICO-Para consistently outperforms SICO-Gen against statistical detectors, i.e., DetectGPT and Log-Rank. We speculate this performance difference comes from the varying influence of the prompt on the generated text between the two methods. In SICO-Para, the distribution of generated text is largely influenced by the original AI-generated text, which is in the prompt. However, in SICO-Gen, the distribution of generated text depends more on the previously generated text. Given that statistical detectors have access to the newly generated text but not the prompt, their estimation of token probability becomes less accurate for SICO-Para text, thus misleading the detection. It might also explain why GPT-Para can reduce the performance of statistical detectors.\n\n### Human Evaluation\n\nFrom the users' perspective, using AI-generated text goes beyond evading detection systems; the usability of text is equally critical. For example, for academic writing task, users expect the text to be readable, properly formatted, and relevant to the given topic. Therefore, we evaluate the usability of text based on two criteria: readability and task completion rate. For each task, we randomly sample 200 examples generated by four methods (50 per method), including human-written text. Then we ask three human annotators to rate the readability of text on a scale from 1 to 5, and judge if the text accomplishes the task's goal. More details of human evaluation are shown in Appendix D.\n\nAs shown in Table 2, both SICO-Gen and SICO-Para demonstrate superior performance over DIPPER in terms of task completion and readability over three tasks. Furthermore, SICO-generated text performs competitively compared with human-written text in both metrics, with a negligible difference less than 0.1. In contrast, DIPPER exhibits inferior performance relative to human-written text, particularly with a notable 0.27 decline in readability.\n\nFigure 2: ROC curves of six detectors on the text generated by different evasion methods on academic writing task.\n\n \n\n### Real-life Experiments\n\nTo further assess the applicability of SICO in real-world, we simulate one potential misuse case of LLM, where SICO is deployed as an automatic reply bot on Reddit4, a popular online social platform. We wrote a script to monitor the new posts submitted in the community for asking questions and used GPT-3.5 equipped with SICO-Para to automatically reply them. The prompt we used is trained for question answering task. On Reddit, except for giving comments, users can express their approval of other's responses by clicking the \"like\" or \"dislike\" button. To minimize the social impact, we limit the number of responses to 40 and deleted them after collecting results.\n\nFootnote 4: https://www.reddit.com/\n\nThe quantitative results in Table 3 demonstrate that users generally react positively to the text from SICO. Specifically, 40% of the responses from SICO receive \"likes\" from Reddit users, significantly higher than the 2.5% that are disliked. The remaining 57.5% of responses go unnoticed, which is common in social media. Besides, in 12.5% of cases, users left additional comments to express their approval or to ask further questions, demonstrating their engagement and interest in SICO's response. Figure 3 presents two SICO's responses that got approval from users, as indicated by \"likes\" and comments.\n\n### Cost Efficiency\n\nIn terms of the data prerequisite, SICO only needs \\(K+|\\mathbf{X}_{eval}|\\) human-written input-output examples to build prompt, which is \\(8+32=40\\) in the experiments. The other AI-generated text can be produced by LLM leveraging these human samples. Furthermore, SICO offers the advantage of low cost for prompt construction. Based on three repeated runs, the actual USD costs of SICO-Para are \\(1.04\\pm 0.04\\), \\(1.08\\pm 0.05\\), and \\(0.75\\pm 0.04\\) for Writing, QA, Review tasks, respectively.\n\n## 5 Further Experiments\n\n### Ablation Study\n\nWe conducted an ablation study over academic writing task to to evaluate the contribution of individual components within the SICO framework. \"Human-ICE\" denotes the approach where human-written text is directly utilized as the in-context example for constructing the prompt. \"w/o feature\" and \"w/o ICE\" refer to the prompts without feature text and the optimized in-context examples, respectively. \"w/o OPT\" represents the initial prompt before optimization (see Line 3 in Algorithm 1). In our experiment, we explore SICO-Para on three types of detectors: GPT3-D, OpenAI-D and DetectGPT.\n\nResults in Table 4 shows that directly using human-written text is ineffective, even making the detection more accurate. We speculate that the human-written examples are too heterogeneous\n\n\nTable 2: Human evaluation results. \u201cAvg.D.\u201d represents the average difference between the results achieved by the evasion method and the results achieved by human-written text on the three tasks. The best value of each task is set **bold**.\n\n\nTable 3: Quantitative results of Reddit test.\n\nand characterized in multiple ways, so LLM cannot effectively learn their attributes. Besides, the importance of feature text is comparatively less than that of optimized in-context examples. Furthermore, the result reveals the significant role of the optimization step in SICO. Using in-context examples that are not optimized is essentially equivalent to not using any in-context examples.\n\n### SICO with Different Proxy Detectors and LLMs\n\nAs described in Section 3, SICO requires a proxy detector and a LLM to construct a prompt. In this experiment, we explore the performance of SICO-Para on writing task, using three types of proxy detectors: (1) training-based model GPT-3.5 detector, (2) API detector GPTzero, and (3) statistical method DetectGPT. For different LLMs, we adopt Vicuna-13B [39], an open-source chatbot fine-tuned from LLaMa [3]. Results in Table 5 show that SICO maintains a high degree of detection evasion performance, regardless of proxy detectors or LLMs. In most cases, SICO manages to reduce the AUC of detectors by approximately 0.4.\n\n\nTable 4: The AUC scores of detectors on text generated by different methods. \u201c\u2013\u201d indicates the case where no evasion method is used. \u2019AVG\u2019 represents the average AUC scores across detectors.\n\nFigure 3: Screenshots of replies from SICO that got approved by Reddit users. (Left) Approval is shown by the number of \u201clike\u201ds. (Right) The original poster express thanks to SICO\u2019s response.\n\n\nTable 5: The AUC scores of SICO using different proxy detectors and LLMs on writing task. The first line indicates the performance without applying any evasion method.\n\n \n\n### Examples of the generated text\n\nTable 6 presents the fake reviews generated by SICO-Gen and SICO-Para. The generated text shows high readability and fulfill the task's goal, successfully mentioning all keywords and generating positive reviews of the specified object. The AI probability, denoted as \\(\\mathcal{P}_{\\text{AI}}\\) in the table, is determined by the GPT3-D. More examples are shown in Appendix I.\n\n## 6 Conclusion\n\nIn conclusion, we have proposed a novel in-context learning approach, SICO, designed to guide LLMs in generating text that can effectively evade detectors. Our extensive experiments on evasion demonstrate the superior performance of SICO, which significantly reduces the detection capabilities of existing AI text detectors across three tasks. A comprehensive human evaluation shows SICO text can achieve human-level readability and task completion rates. The experiment in the wild shows the functionality of SICO in real life.\n\nLooking ahead, SICO could act as a data generator and be integrated during the training phase of AI detectors, which may enhance their robustness. Furthermore, the core concept of SICO, namely, substitution-based in-context learning, could be applied to a variety of text generation tasks. We believe that this opens up new avenues for future research in the fields of text generation and in-context learning.\n\n## 7 Ethics statement\n\nThe intention of this paper is not to offer a potential method for evading AI-generated text detection systems. Instead, our aim is to raise awareness within the broader community about the vulnerabilities of existing AI-generated text detection systems to such technology. As many LLMs are public available and free to use, many people can adjust their prompt and generate text that evades these detectors. Given the ease of evasion illustrated in this study, these detectors are not robust yet.\n\nWe hope the research community can stress test their detectors against text generated by carefully crafted prompt, and create more robust detectors in the future. To support the research in this field, we will make our training methods and relevant data/code publicly available.\n\n\nTable 6: Fake reviews generated by SICO. The first line shows the task input of review generation.\n\n\n\n## References\n\n* Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.\n* Chowdhery et al. [2022] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. _arXiv preprint arXiv:2204.02311_, 2022.\n* Touvron et al. [2023] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.\n* and machine-based detection. In Leonard Barolli, Flora Amato, Francesco Moscato, Tomoya Enokido, and Makoto Takizawa, editors, _Advanced Information Networking and Applications\n- Proceedings of the 34th International Conference on Advanced Information Networking and Applications, AINA-2020, Caserta, Italy, 15-17 April_, volume 1151 of _Advances in Intelligent Systems and Computing_, pages 1341-1354. Springer, 2020. doi: 10.1007/978-3-030-44041-1_114. URL https://doi.org/10.1007/978-3-030-44041-1_114.\n* Lin et al. [2022] Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022_, pages 3214-3252. Association for Computational Linguistics, 2022. doi: 10.18653/v1/2022.acl-long.229. URL https://doi.org/10.18653/v1/2022.acl-long.229.\n* Stokel-Walker [2022] Chris Stokel-Walker. Ai bot chatgpt writes smart essays-should academics worry? _Nature_, 2022.\n* Stokelverflow [2023] StackOverflow. Temporary policy: Chatgpt is banned, 2023. URL https://meta.stackoverflow.com/questions/421831/temporary-policy-chatgpt-is-banned.\n* Guo et al. [2023] Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng Wu. How close is chatgpt to human experts? comparison corpus, evaluation, and detection. _CoRR_, abs/2301.07597, 2023. doi: 10.48550/arXiv.2301.07597. URL https://doi.org/10.48550/arXiv.2301.07597.\n* Solaiman et al. [2019] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford, and Jasmine Wang. Release strategies and the social impacts of language models. _CoRR_, abs/1908.09203, 2019. URL http://arxiv.org/abs/1908.09203.\n* Mitchell et al. [2023] Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D. Manning, and Chelsea Finn. Detectgpt: Zero-shot machine-generated text detection using probability curvature. _CoRR_, abs/2301.11305, 2023. doi: 10.48550/arXiv.2301.11305. URL https://doi.org/10.48550/arXiv.2301.11305.\n* Kirchenbauer et al. [2023] John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and Tom Goldstein. A watermark for large language models. _CoRR_, abs/2301.10226, 2023. doi: 10.48550/arXiv.2301.10226. URL https://doi.org/10.48550/arXiv.2301.10226.\n* Tian [2023] Edward Tian. Gptzero: an ai detector, 2023. URL https://gptzero.me/.\n* Krishna et al. [2023] Kalpesh Krishna, Yixiao Song, Marzena Karpinska, John Wieting, and Mohit Iyyer. Paraphrasing evades detectors of ai-generated text, but retrieval is an effective defense. _CoRR_, abs/2303.13408, 2023. doi: 10.48550/arXiv.2303.13408. URL https://doi.org/10.48550/arXiv.2303.13408.\n\n* Sadasivan et al. [2023] Vinu Sankar Sadasivan, Aounon Kumar, Sriram Balasubramanian, Wenxiao Wang, and Soheil Feizi. Can ai-generated text be reliably detected? _CoRR_, abs/2303.11156, 2023. doi: 10.48550/arXiv.2303.11156. URL https://doi.org/10.48550/arXiv.2303.11156.\n* an adversarial study. In _Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 2: Short Papers_. The Association for Computer Linguistics, 2016. doi: 10.18653/v1/p16-2057. URL https://doi.org/10.18653/v1/p16-2057.\n* Zellers et al. [2019] Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. Defending against neural fake news. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alche-Buc, Emily B. Fox, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada_, pages 9051-9062, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/3e9f0fc9b2f89e043bc6233994dfcf76-Abstract.html.\n* Lavergne et al. [2008] Thomas Lavergne, Tanguy Urvoy, and Francois Yvon. Detecting fake content with relative entropy scoring. In Benno Stein, Efstathios Stamatatos, and Moshe Koppel, editors, _Proceedings of the ECAI'08 Workshop on Uncovering Plagiarism, Authorship and Social Software Misuse, Patras, Greece, July 22, 2008_, volume 377 of _CEUR Workshop Proceedings_. CEUR-WS.org, 2008. URL https://ceur-ws.org/Vol-377/paper4.pdf.\n* 21st International Conference on Applications of Natural Language to Information Systems, NLDB 2016, Salford, UK, June 22-24, 2016, Proceedings_, volume 9612 of _Lecture Notes in Computer Science_, pages 421-426. Springer, 2016. doi: 10.1007/978-3-319-41754-7_43. URL https://doi.org/10.1007/978-3-319-41754-7_43.\n* August 2, 2019, Volume 3: System Demonstrations_, pages 111-116. Association for Computational Linguistics, 2019. doi: 10.18653/v1/p19-3019. URL https://doi.org/10.18653/v1/p19-3019.\n* Abdelnabi and Fritz [2021] Sahar Abdelnabi and Mario Fritz. Adversarial watermarking transformer: Towards tracing text provenance with data hiding. In _42nd IEEE Symposium on Security and Privacy, SP 2021, San Francisco, CA, USA, 24-27 May 2021_, pages 121-140. IEEE, 2021. doi: 10.1109/SP40001.2021.00083. URL https://doi.org/10.1109/SP40001.2021.00083.\n* Grinbaum and Adomaiitis [2022] Alexei Grinbaum and Laurynas Adomaiitis. The ethical need for watermarks in machine-generated language. _CoRR_, abs/2209.03118, 2022. doi: 10.48550/arXiv.2209.03118. URL https://doi.org/10.48550/arXiv.2209.03118.\n* Devlin et al. [2019] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)_, pages 4171-4186. Association for Computational Linguistics, 2019. doi: 10.18653/v1/n19-1423. URL https://doi.org/10.18653/v1/n19-1423.\n* Radford et al. [2019] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. _OpenAI blog_, 1(8):9, 2019.\n* Gou et al. [2022] Yunhao Gou, Tom Ko, Hansi Yang, James T. Kwok, Yu Zhang, and Mingxuan Wang. Leveraging per image-token consistency for vision-language pre-training. _CoRR_, abs/2211.15398, 2022. doi: 10.48550/arXiv.2211.15398. URL https://doi.org/10.48550/arXiv.2211.15398.\n\n * Dong et al. [2022] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. A survey for in-context learning. _arXiv preprint arXiv:2301.00234_, 2022.\n* Kim et al. [2022] Hyuhng Joon Kim, Hyunsoo Cho, Junyeob Kim, Taeuk Kim, Kang Min Yoo, and Sang-goo Lee. Self-generated in-context learning: Leveraging auto-regressive language models as a demonstration generator. _CoRR_, abs/2206.08082, 2022. doi: 10.48550/arXiv.2206.08082. URL https://doi.org/10.48550/arXiv.2206.08082.\n* Gonen et al. [2022] Hila Gonen, Srini Iyer, Terra Blevins, Noah A. Smith, and Luke Zettlemoyer. Demystifying prompts in language models via perplexity estimation. _CoRR_, abs/2212.04037, 2022. doi: 10.48550/arXiv.2212.04037. URL https://doi.org/10.48550/arXiv.2212.04037.\n* Rubin et al. [2022] Ohad Rubin, Jonathan Herzig, and Jonathan Berant. Learning to retrieve prompts for in-context learning. In Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz, editors, _Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022, Seattle, WA, United States, July 10-15, 2022_, pages 2655-2671. Association for Computational Linguistics, 2022. doi: 10.18653/v1/2022.naacl-main.191. URL https://doi.org/10.18653/v1/2022.naacl-main.191.\n* Liu et al. [2021] Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What makes good in-context examples for gpt-3? _arXiv preprint arXiv:2101.06804_, 2021.\n* Wei et al. [2022] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. _arXiv preprint arXiv:2201.11903_, 2022.\n* Zhang et al. [2022] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in large language models. _arXiv preprint arXiv:2210.03493_, 2022.\n* Miller [1998] George A Miller. _WordNet: An electronic lexical database_. MIT press, 1998.\n* Rajpurkar et al. [2016] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ Questions for Machine Comprehension of Text. _arXiv e-prints_, art. arXiv:1606.05250, 2016.\n* Fan et al. [2019] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5: long form question answering. In Anna Korhonen, David R. Traum, and Lluis Marquez, editors, _Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers_, pages 3558-3567. Association for Computational Linguistics, 2019. doi: 10.18653/v1/p19-1346. URL https://doi.org/10.18653/v1/p19-1346.\n* Zhang et al. [2015] Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada_, pages 649-657, 2015. URL https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f8dc8b4be867a9a02-Abstract.html.\n* Liu et al. [2019] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining approach. _CoRR_, abs/1907.11692, 2019. URL http://arxiv.org/abs/1907.11692.\n* Mireshghallah et al. [2023] Fatemehsadat Mireshghallah, Justus Mattern, Sicun Gao, Reza Shokri, and Taylor Berg-Kirkpatrick. Smaller language models are better black-box machine-generated text detectors. _CoRR_, abs/2305.09859, 2023. doi: 10.48550/arXiv.2305.09859. URL https://doi.org/10.48550/arXiv.2305.09859.\n* OpenAI [2023] OpenAI. Openai ai text classifier, January 2023. URL https://beta.openai.com/ai-text-classifier.\n\n * Chiang et al. [2023] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, 2023.\n* June 1, 2003_. The Association for Computational Linguistics, 2003. URL https://aclanthology.org/N03-1033/.\n* Ni et al. [2022] Jianmo Ni, Gustavo Hernandez Abrego, Noah Constant, Ji Ma, Keith B. Hall, Daniel Cer, and Yinfei Yang. Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, _Findings of the Association for Computational Linguistics: ACL 2022, Dublin, Ireland, May 22-27, 2022_, pages 1864-1874. Association for Computational Linguistics, 2022. doi: 10.18653/v1/2022.findings-acl.146. URL https://doi.org/10.18653/v1/2022.findings-acl.146.\n* Muennighoff et al. [2023] Niklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers. MTEB: massive text embedding benchmark. In Andreas Vlachos and Isabelle Augenstein, editors, _Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2023, Dubrovnik, Croatia, May 2-6, 2023_, pages 2006-2029. Association for Computational Linguistics, 2023. URL https://aclanthology.org/2023.eacl-main.148.\n* Raffel et al. [2020] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _Journal of Machine Learning Research_, 21(140):1-67, 2020. URL http://jmlr.org/papers/v21/20-074.html.\n\n Implementation Details\n\n### Sico\n\n#### a.1.1 Feature extraction\n\nIn feature extraction step, we instruct LLM to extract 5 features and calculate the utility score \\(\\mathcal{U}\\) of prompts encompassing each of these features. Then we select the feature with the highest utility for further steps. The goal of this step is to find a good feature to accelerate process, and make the whole process stable. Because sometimes LLM cannot extract useful features to evade detectors. The pseudo-code illustrating this selection process is outlined in Algorithm 3. Table 7 presents the prompt template used for feature extraction. Here, \\(K\\) text pairs generated by AI and Human are positioned within their respective positions. Table 20 shows the examples for feature extracted by LLM.\n\n```\n1:list of features \\(T_{\\text{feature}}\\), prompt utility function \\(\\mathcal{U}(\\cdot)\\)\n2:Initialize \\(t^{*}_{\\text{feature}}\\leftarrow\\emptyset\\)\n3:Initialize \\(\\mathcal{U}_{max}\\leftarrow-\\infty\\)\n4:for each feature \\(t_{\\text{feature},i}\\) in \\(T_{\\text{feature}}\\)do\n5: Construct prompt \\(p_{i}\\gets t_{\\text{feature},i}\\oplus p_{\\text{task}}\\)\n6:if\\(\\mathcal{U}(p_{i})>\\mathcal{U}_{max}\\)then\n7:\\(t^{*}_{\\text{feature}}\\gets t_{\\text{feature},i}\\)\n8:\\(\\mathcal{U}_{max}\\leftarrow\\mathcal{U}(p_{i})\\)\n9:endif\n10:endfor\n11:return\\(t^{*}_{\\text{feature}}\\) ```\n\n**Algorithm 3** Feature selections\n\nLLM consistently extract useful features.To test if LLM can reliably extract useful features, we conducted three separated experiments by running three feature extractions on different sets of human-written text and AI-generated text. We use three different extracted features to guide LLM generation and test the AUC drop after adopting 3 different features compared with the originally generated text on the Writing task. Table 8 shows the results, indicating that LLM consistently extracts useful features for detector evasion from different examples.\n\n#### a.1.2 Task instructions\n\nTable 9 shows the actual task instruction \\(p_{\\text{task}}\\) we used in SICO. As mentioned in Section 3, feature text \\(t_{\\text{feature}}\\) at first step will be inserted before these task instructions. The \"Paraphrase\" instruction is \\(p_{\\text{para}}\\) used in paraphrase generation for substitution (Line 6 of Algorithm 1).\n\n\nTable 8: AUC drop of different features extracted based on different human-written and AI-generated text.\n\n \n\n#### a.1.3 Word Substitution\n\nWe employ WordNet synsets to derive synonyms for given words. During optimization of in-context examples, we only substitute content words, namely nouns, verbs, adjectives, and adverbs. Furthermore, we part-of-speech (POS) tag of the synonym to ensure it aligns with the original word. For POS tagging, we utilize the Stanford POS Tagger [40]. Additionally, to maintain fluency in the modified text after substitution, we employed a pretrained mask language model to exclude synonyms with low likelihood. In experiment we use RoBERTa-base model [36].\n\n### Baselines\n\n#### a.2.1 Dipper\n\nWe choose the best evasion performance parameter setting from the original paper [13], which is 60 for lexical diversity and 60 for re-ordering. And we set sampling temperature to 0.75, following the original implementation.\n\n#### a.2.2 Human prompt\n\nWe carefully design a paraphrase prompt based on the detection idea of GPTzero [12], which distinguish the AI-generated content from Human-written by _perplexity_ and _burstiness_, stated by its creator5. _Perplexity_ is the concept raised in NLP field, which measures how well a language model predicts a text sample. A text with a lower perplexity score indicates that the language model is better at calculating the next word that is likely to occur in a given sequence. On the other hand, _burtiness_ basically measures the variation between sentences, including sentence length and structures. The lower the values for these two factors, the more likely it is that a text was produced by an AI. Table 10 shows the prompt we designed.\n\nFootnote 5: https://theconversation.com/we-pitted-chatgpt-against-tools-for-detecting-ai-written-text-and-the-results-are-troubling-199774\n\n## Appendix B Extra Experiments\n\n### Detection capability of human\n\nWe conducted a experiment of real-life human detection rate. We randomly selected 150 examples (50 each from AI, SICO-Para, and human responses) from QA task. Four human annotators were then asked to identify whether each answer was AI-generated or human-written. Table 11 shows that SICO remarkably reduces the probability of being recognized by humans. Additionally, the data also reveals that human detection capabilities are not highly accurate compared with machine detectors.\n\n\nTable 9: Task instructions of each task.\n\nApproximately half of AI-generated texts went undetected, and 10% of human-written texts were mistakenly identified as AI-generated.\n\n### Semantic preserving\n\nWe measure semantic similarity using t5-based sentence encoder [41], which leads in semantic text similarity task of MTEB benchmark [42]. Table 12 reports a comparison of the cosine similarity of text before and after paraphrasing by different methods.\n\nOur methods successfully preserves the semantic meaning during paraphrasing, and beats the other specifically trained paraphraser. Paraphrasing directly using GPT-3.5 yields the most promising results.\n\n### Consistent effectiveness of SICO\n\nSICO consistently procures optimal detection evasion prompts, irrespective of the diversity in AI-human text pairs and randomized samples drawn from the LLMs. This demonstrates the effectiveness of SICO in various initial conditions and settings, highlighting its applicability to diverse scenarios. Figure 4 presents the AI probability of the paraphrased training data, generated by LLM using the current best prompt, denoted as \\(\\mathcal{U}(p^{*})\\) in Equation 1. SICO successfully optimizes the initial prompt (at step 0) and converges to a low level in three separate trials across all three tasks, with different AI-human text pairs.\n\n### SICO performs better against more capable detectors\n\nWe use the detectors' performance on the original AI-generated text to represent their base capability. The SICO advancing performance is measured by the AUC difference between best of SICO-Para or\n\n\nTable 10: Human-designed prompt to evade AI-generated text detection.\n\n\nTable 11: Detection rate from human annotators. \u2019AI %\u2019 indicates the proportion of texts identified as AI-generated by annotators.\n\n\nTable 12: Cosine similarity between original AI-generated text and their respective paraphrased versions using different methods. The best scores in each task are highlighted in **bold**.\n\n SICO-Gen and the best-performing paraphraser baselines. The statistical Pearson correlation is \\(0.47\\) with a p-value of \\(0.048\\), indicating a moderate positive correlation.\n\n## Appendix C Detectors\n\nIn this section, we introduce the mechanism and settings of the detectors in our experiments.\n\n### GPT-3.5 Detector\n\nGPT-3.5 detector is trained on Human ChatGPT Comparison Corpus (HC3) dataset [8], which including answers generated by ChatGPT and human. English-version of HC3 dataset contains five splits: reddit-eli5, OpenQA, Wikipedia, medicine and finance. The base model is RoBERTa-base and we use the model that only take answers as input.\n\n### GPT2 Detector\n\nGPT-2 detector is obtained by fine-tuning a RoBERTa model with the outputs of the 1.5B-parameter GPT-2 model. The detector and the GPT-2 output dataset are both provided by OpenAI. Although it is trained on GPT-2 outputs, our experiments shows that it can effectively detect text from GPT-3.5.\n\n### DetectGPT\n\nDetectGPT identifies if a text is generated by a model by observing a unique characteristic: AI-generated text tends to be in areas where the language model's log probability function has a negative curve. Here's how it works: It first perturbs the input text and constructs multiple perturbations of input text. The perturb step is completed by a mask language model. Then it checks the log probability of these variations against the original text by a inner language model. Finally, the text is considered AI-generated if the log probability of the original input text is significantly higher than the log probability of perturbations.\n\nWe use z-score implementation of DetectGPT and set sample number to 100 and replacement ratio to 0.3. The inner language model is GPT2-medium and the mask language model is t5-large [43].\n\n### Log-Rank\n\nLog-Rank method employs the mean prediction rank of each token in a text. Specifically, for each word in a text, given its previous context, it can calculate the absolute rank of this word by an inner language model. Then, for a given text, we compute the score of the text by averaging the rank value of each word. Note that a smaller score denotes the text is more likely to be machine-generated. In experiment, we use GPT2-medium to calculate the rank of tokens to align with the implementation of DetectGPT.\n\nFigure 4: The trajectory of the \\(\\mathcal{U}(p^{*})\\) during prompt optimization. This plot is derived from three distinct training runs on three tasks.\n\n \n\n### GPTZero\n\nGPTzero is a recently proposed commercial detector, employed by many users and organizations. As claimed in its websites6, GPTzero can be used to detect the outputs from detect ChatGPT, GPT4, Bard, LLaMa, and other AI models. GPTZero is the leading AI detector for checking whether a document was written by a large language model such as ChatGPT. GPTZero detects AI on sentence, paragraph, and document level. GPTzero was trained on a large, diverse corpus of human-written and AI-generated text, with a focus on English prose. GPTZero has served over 2.5 million users around the world, and works with over 100 organizations in education, hiring, publishing, legal, and more.\n\nFootnote 6: https://gptzero.me/\n\n### OpenAI Detector\n\nOpenAI detector is officially provided by OpenAI after the release of ChatGPT. Although it only offers a web interface, we adopted the API implementation from [13], which uses \"model-detect-v2\" in the OpenAI API. Through reverse engineering of the website, we determined that the web interfaces indeed call this model.\n\nOn July 20, 2023, OpenAI discontinued this detector, \"due to its low rate of accuracy.\"7. Considering the discontinuation of the OpenAI detector aligns with our findings, we choose to present the results of it in our paper, though it is out of date.\n\nFootnote 7: https://openai.com/blog/new-ai-classifier-for-indicating-ai-written-text\n\n## ", "Appendix D Human Evaluation Details": "\n\nFor each text, we show two questions for the human annotator. In terms of readability, we present the human annotator five options, with the scale of 1 to 5. The higher the value, the more readable of the presented texts. The question is identical for three tasks. The actual question and options are presented in Table 13. For task completion rate, we design three task-specific questions, as show in Table 14. Figure 6 shows the interface of our annotation platform. We estimated that the evaluation time of each text ranges from 60 to 90 seconds.\n\nHuman evaluation on Parrot.As Parrot method performs better than DIPPER in the Writing task, we conducted a small experiment to evaluate the usability of text generated text, similar to Table 2. We randomly sampled 120 examples (40 for Parrot, SICO-Para and SICO-Gen) from the writing task and asked two human annotators to evaluate them. The experiment result in Table 15 shows that SICO still outperforms Parrot by a large margin.\n\n## ", "Appendix E Details of Real-life Experiments": "\n\nWe employ the **Praw** package, a Reddit API wrapper, to write the auto-reply bot. On each run, the bot first collect the posts that are newly submitted within 2 hours and extract the title of poster as question. Then the bot will utilize GPT-3.5 to generate the original answer, and use SICO-Para to paraphrase it. And we set rules to filter out personal question like \"_Which movie do you like best?_\". Because GPT-3.5 will reply like \"_I'm sorry, as a language model..._\", which will directly explore itself. The prompt we deploy is trained for QA task. We only reply in the communities that do not ban AI-generated content: specifically _explainlikeimfive_ and _NoStupidQuestions_. We only generate 40 responses and deleted them after collecting the data.\n\n\nTable 13: Question and options designed for readability.\n\n\n\n## ", "Appendix F Datasets": "\n\nTable 16 presents the prompts we employed to create the initial AI-generated text \\(y_{\\text{AI}}\\). For academic writing, we sample Wikipedia paragraphs from SQuAD dataset. Then we give GPT-3.5 the first 30 words of these paragraphs and ask GPT-3.5 to complete the rest.\n\nFor open-ended question answering, we sample questions from Eli5 dataset and ask GPT-3.5 to generate answers.\n\nFor fake review generation, we first instruct GPT-3.5 to extract the business name and five keywords from human-written reviews from Yelp dataset, and then generate fake reviews based on the extracted information with specified sentiment.\n\nThe statistics of three datasets are shown in Table 17\n\n\nTable 16: Prompt for dataset creation.\n\n\nTable 14: Question and options designed for task completion.\n\n\nTable 15: Human evaluation results of Parrot and SICO.\n\n\n\n## ", "Appendix G Evade Watermarking Detection": "\n\nSICO-Para can also be utilized to evade watermark detection, similarly to paraphrase approach. The watermarking algorithm we applied was introduced by Kirchenbauer et al. [11], which only requires access to the LLM's logits at each time step to add watermarks. This algorithm operates in three steps:\n\n1. Mark a random subset of the vocabulary as \"green tokens\"using the hash of the previously generated token as a random seed.\n2. Increase the logit value for every green token by a constant, which denotes the watermark strength.\n3. Sample sequences using decoding algorithms.\n\nVerification of this watermark is achievable with blackbox access to the LM and knowledge of the hash function, achieved by tokenizing the text, calculating the standard normal score (z-score) through a hypothesis test, and comparing the observed proportion of green tokens to the expected proportion.\n\nIn our experiments, we used the text generated by a watermarked GPT-2, provided by Krishna et al. [13]. We employed the GPT-3.5 detector as proxy detector for training. The AUC and and the detection accuracy associated with various paraphrasing methods are presented in Table 18, where the threshold is set to 2.2 for accuracy measurement.\n\nThe results reveal that SICO-Para significantly outperforms other paraphrase techniques in evading watermark detection. Notably, both the AUC score and detection accuracy of SICO-Para are significantly lower than that of other methods. Furthermore, even the simpler baseline model, GPT-Para, proved to be effective at evading the watermark detection.\n\n## ", "Appendix H Evasion Performance": "\n\n### ", "ROC Curves": "\n\nFigure 2 shows ROC curves of different detectors presented with text generated by different methods, on open-ended question answering and fake review generation task. SICO curves lie below other baselines.\n\n### ", "Detection Accuracy": "\n\nGiven that detection rates highly depend on the selected detection threshold, we establish two thresholds for each detector. The high threshold fixes the _false positive rate_ (FPR) at a low level of 0.05, which means only 5% of human-written text will be classified as AI-generated. The low\n\n\nTable 17: Average character length of human-written text and the original AI-generated text.\n\n\nTable 18: Performance of paraphrase methods on watermarking detection.\n\nthreshold fixes the _true positive rate_ (TPR) at a high level of 0.9, based on the original AI-generated\n\nFigure 5: ROC curves.\n\n text. In this case, 90% of original AI-generated text will be correctly classified. Table 19 shows the detection accuracy on three task. In comparison with other paraphrasing methods, SICO yields the lowest detection rates in most cases.\n\n## ", "Appendix I Examples of Text Generated by SICO": "\n\nThe examples of text generated by SICO across three tasks are presented in Tabel 21-23.\n\n\nTable 21: Text generated by SICO for open-ended question answering task.\n\n\n\n\nTable 22: Text generated by SICO for fake review generation task.\n\n\n\n\\begin{table}\n\\begin{tabular}{p{142.3pt} p{142.\n\nHere is an answer for a question:\n\nQuestion: Why do we recognize certain sounds (such as a gentle \"hey wake up\"), but not others when we sleep, given that they are the same volume?\n\nAnswer: When we sleep, our brain is still processing sounds and other sensory information, but it does so at a different level of consciousness. It is possible that we may recognize certain sounds more easily than others because they are familiar to us or because they have a particular significance. Additionally, the brain may be more likely to filter out certain sounds that it determines aren't important or relevant to our current state of rest. This filtering process is an important part of how sleep helps us recover and feel refreshed upon waking.\n\n* [172-1] Does the answer relate to the question? (regardless of correctness)\n\nNo, not relative.\n\nYes, it is relative.\n\n* [172-2] On a scale of 1-5,how would you rate the readability of the given text?\n\n1: Very difficult to read and understand.\n\n2: Difficult to read, need extra time to understand.\n\n3: Neutral.\n\n4: Easy to read.\n\n5: Very clear and easy to read.\n\nFigure 6: The interface of the annotation platform used in our experiment.\n\n "}, "Texto4.pdf": {"RADAR: Robust AI-Text Detection via Adversarial Learning": "\n\n Xiaomeng Hu\n\nThe Chinese University of Hong Kong\n\nSha Tin, Hong Kong\n\nxmhu23@cse.cuhk.edu.hk\n\nPin-Yu Chen\n\nIBM Research\n\nNew York, USA\n\npin-yu.chen@ibm.com\n\nTsung-Yi Ho\n\nThe Chinese University of Hong Kong\n\nSha Tin, Hong Kong\n\ntyho@cse.cuhk.edu.hk\n\n###### ", "Abstract": "\n\nRecent advances in large language models (LLMs) and the intensifying popularity of ChatGPT-like applications have blurred the boundary of high-quality text generation between humans and machines. However, in addition to the anticipated revolutionary changes to our technology and society, the difficulty of distinguishing LLM-generated texts (AI-text) from human-generated texts poses new challenges of misuse and fairness, such as fake content generation, plagiarism, and false accusations of innocent writers. While existing works show that current AI-text detectors are not robust to LLM-based paraphrasing, this paper aims to bridge this gap by proposing a new framework called RADAR, which jointly trains a robust AI-text detector via adversarial learning. RADAR is based on adversarial training of a paraphraser and a detector. The paraphraser's goal is to generate realistic content to evade AI-text detection. RADAR uses the feedback from the detector to update the paraphraser, and vice versa. Evaluated with 8 different LLMs (Pythia, Dolly 2.0, Palmyra, Camel, GPT-J, Dolly 1.0, LLaMA, and Vicuna) across 4 datasets, experimental results show that RADAR significantly outperforms existing AI-text detection methods, especially when paraphrasing is in place. We also identify the strong transferability of RADAR from instruction-tuned LLMs to other LLMs, and evaluate the improved capability of RADAR via GPT-3.5-Turbo.\n\n**Project Page and Demos:**https://radar.vizhub.ai\n\nIBM demo is developed by Hendrik Strobelt and Benjamin Hoover at IBM Research\n\nHuggingFace demo is developed by Xiaomeng Hu\n\n## ", "1 Introduction": "\n\nLarge language models (LLMs) are high-capacity neural networks that are pretrained at web-scale datasets. They are foundation models achieving state-of-the-art performance in a wide range of natural language processing tasks (e.g. document completion, question answering, machine translation, and content creation with text prompts) with advanced capabilities such as in-context learning and reasoning (e.g. chain of thoughts). In particular, LLMs are the backbone of many ChatGPT-like conversational bots that enable text generation with high fluency and accuracy. However, while LLMs and their derived applications are expected to become ubiquitous in our future technology and society,new risks in failing to distinguish the so-called \"AI text\" generated by LLMs have emerged and gained considerable attention due to various reasons. The problem of reliable AI-text detection is motivated by realistic socio-technological challenges such as fake content generation, AI plagiarism (e.g. using LLMs for writing tests), and false accusations of innocent writers. According to a report released by OpenAI1, their latest AI-text detector is admittedly not fully reliable. In the reported evaluation of some challenging cases for English texts, their classifier only correctly identifies 26% of AI-text (true positives) while incorrectly classifying 9% of human-written text (false positives). Moreover, a recent study [21] found that state-of-the-art AI-text detectors demonstrated severely degraded performance when encountering texts written by non-native English speakers.\n\nFootnote 1: https://openai.com/blog/new-ai-classifier-for-indicating-ai-written-text\n\nWhat can be even more challenging in AI-text detection is that existing AI-text detectors are prone to be manipulated. The authors in [27; 17] showed that using LLMs as a paraphraser can easily evade several AI-text detection methods, even in the scenario when the original AI-text had been watermarked. These findings sparked a heated debate about whether and how we can successfully design a reliable AI-text detector. While [27] theoretically quantifies the best detector's performance with respect to the total variation distance between AI-text and human-text distributions and argues that AI-text is difficult to detect, another work [3] proves that it is possible to obtain a reliable AI-text detector unless the human-text distribution is exactly the same as the AI-text distribution, based on an information-theoretical analysis (i.e., the sample complexity of Chernoff information and likelihood-ratio-based detectors).\n\nTo improve AI-text detection, we propose **RADAR**, a framework for training a robust AI-text detector using adversarial learning. An overview of RADAR is illustrated in Figure 1. Our proposal draws inspiration from adversarial machine learning techniques that train a high-quality generator by introducing a discriminator to form a two-player game, such as generative adversarial networks (GANs) [10]. In RADAR, we introduce a paraphraser and a detector as two players with opposite objectives. The paraphraser's goal is to generate realistic content that can evade AI-text detection, while the detector's goal is to enhance AI-text detectability. In our framework, both the paraphraser and the detector are parametrized by separate LLMs. During training, the paraphraser learns to rewrite the text from a training corpus (generated by a target LLM from a human-text corpus) with the\n\nFigure 1: Overview of RADAR. An AI-text corpus is first generated from a target (frozen) language model from a human-text corpus. In RADAR, we introduce a paraphraser (a tunable language model) and a detector (a separate tunable language model). In the training stage, the detector aims to discern human-text v.s. AI-text, while the paraphraser aims to rewrite AI-text to evade detection. The model parameters of the paraphraser and the detector are updated in an adversarial learning manner as described in Section 3. In the evaluation stage, the trained detector is deployed to predict the likelihood of AI-generated content for any input instance.\n\n aim of decreasing the likelihood of AI-text prediction by the detector, whereas the detector aims to enhance the detection performance by learning to compare human-text v.s. AI-text from the training data and the paraphraser's output. These two players iteratively update their model parameters until their respective validation loss becomes stable. Specifically, the paraphraser treats the prediction of the detector as a reward and uses Proximal Policy Optimization (PPO) [28] for updates. The detector updates its parameters based on a logistic loss function evaluated on the human-text and AI-text corpora (including the texts generated by the paraphraser). In the evaluation phase, the trained detector is deployed to predict the likelihood of AI-written content for any input instance. When compared with 6 existing detectors, our experimental results on 8 different LLMs and 4 datasets show that RADAR attains similar detection performance on the original AI-generated texts (a relatively easy task) and simultaneously improves the AI-text detectability when facing an \"unseen\" paraphraser (i.e. this paraphraser is not used in RADAR). The result is summarized in Figure 2. When facing an unseen paraphraser (GPT-3.5-Turbo), the area under the receiver operating characteristic (AUROC) score of RADAR is improved by 31.64% compared to the best existing detector, suggesting a significant improvement and reliable AI-text detection power enabled by RADAR.\n\nWe summarize our **main contributions** as follows:\n\n* To the best of our knowledge, RADAR is the first study that leverages the idea of adversarial learning between a paraphraser and a detector for training a robust AI-text detector.\n* The experiments on 8 different LLMs (Pythia, Dolly 2.0, Palmyra, Camel, GPT-J, Dolly 1.0, LLaMA, and Vicuna) and 4 datasets show that unlike the six existing supervised and unsupervised AI-text detection methods, RADAR is the only robust detector that attains consistently high detection performance. RADAR's detector is not weakened by paraphrasing, as shown in Figure 2.\n* We also find the strong transferability of RADAR's detection capability. The detectors of RADAR obtained from instruction-tuned first-class LLMs (e.g., Vicuna-7B) are also effective on other LLMs, suggesting the possibility of training a universal AI-text detector based on the state-of-the-art LLMs.\n\n## ", "2 Related Work": "\n\n**AI-Text Detection.** The research in AI-text detection can be divided into three approaches. (i) _Statistical methods_: some statistics such as entropy [19], n-gram frequency, and perplexity are used as a threshold to discern AI-text. A typical example is GLTR [8], which exploits entropy, probability, and probability rank for detection. A more recent work is DetectGPT [23], which assumes that the machine-generated text always lies in the negative curvature region of the log probability of the LLM of interest. Based on this hypothesis, DetectGPT perturbs the input text with a mask-filling language\n\nFigure 2: Performance evaluation (AUROC) of 8 LLMs over 4 human-text datasets. _w/o paraphraser_ means the evaluation with the original AI-text corpora (the yellow bin \\(\\mathcal{M}\\) in Figure 1). _RADAR-Unseen paraphraser_ means the evaluation with the paraphrased AI-text (the green bin \\(\\mathcal{P}\\) in Figure 1) generated from an independent paraphraser (OpenAI\u2019s GPT-3.5-Turbo API) that is not used in RADAR. The black error bar represents the standard deviation of the detection AUROCs across 8 LLMs. Please refer to Section 4.2 for more implementation details.\n\n model, such as T5 [25]. Then, AI-text detection is performed by comparing the log probability of the text and its infilled variants. (ii) Classification methods: AI-text detection is formulated as a binary classification task, and a classifier is trained for a target language model [37; 29; 26; 13]. For example, OpenAI trains its AI-text classifier with a RoBERTa-based model [29].\n\nThe developers collected samples from the WebText dataset2 and labeled them as human-generated. Then, for each target GPT-2 model, they collected the generated samples and labeled them as machine-generated. Finally, they fine-tuned the pretrained RoBERTa-based model [29] for AI-text classification. More recently, with the appearance of CharGPT, OpenAI tuned a GPT model called AI-Classifier1 using data from several sources. The human-written text comes from three sources: a new Wikipedia dataset, the WebText dataset collected in 2019, and a set of human demonstrations collected as part of training InstructGPT [24]. To collect machine-generated text, for the Wikipedia and WebText datasets, they truncated the articles sampled from the original corpus and used 34 models to generate article completion, pairing each generated text with the original article. For the demonstrations, they used a model to generate responses for each prompt and paired them with the corresponding human demonstrations. This detector was only accessible via a web interface since its release in January 2023, and it has been taken down since July 2023. (iii) Watermark methods: post-hoc watermarking techniques, such as rule-based methods [1; 15; 31] and deep-learning-based methods [6; 32], can be applied to an LLM. At inference time, [16] proposed a soft watermarking scheme to embed a watermark in each word of the generated sentence by dividing the vocabulary into different lists and sampling the next token in a differentiated manner. However, many existing AI-text detectors are shown to be significantly weakened by paraphrasing in [27].\n\nFootnote 2: https://huggingface.co/datasets/openwebtext\n\n**Adversarial Learning for Natural Language Generation.** The success of GAN [10] in the computer vision domain has motivated many studies in natural language generation. However, since text generation is a sequential sampling process that occurs in a discrete vocabulary space, it is difficult to directly train a text generator using back-propagation in an end-to-end manner [36; 7; 5; 35]. There are two common approaches to tackle this problem. The first one is to replace the discrete sampling operation with continuous approximation techniques [35; 5], such as Gumbel-Softmax [14; 22]. The second one is to view text generation as a decision-making process and cast the generator as a policy [36; 34; 7; 33]. A typical example is SeqGAN [36]. During generation, SeqGAN considers the generated tokens as the state and the next token to be generated as the action, and it adopts Monte Carlo search to collect reward signals from the discriminator. Instead of using a classifier as the discriminator, the Diversity-Promoting GAN [34] uses a unidirectional LSTM as the discriminator and combines both word-level and sentence-level rewards into training. TextGAIL [33] proposed an imitation learning paradigm in which the rewards of the human-written text are regarded as a constant value. Then, both the rewards from human-text and AI-text are used to optimize the generator with PPO. These works all used warm-up training for the generator with maximum likelihood estimation (MLE) on the probability of the generated text sequence. On the other hand, [7] trained a language GAN from scratch. Our proposed RADAR differs from these works in that we focus on training a robust AI-text detector with a tunable paraphraser. Another line of work, such as [20; 4], uses paraphrasing techniques to find adversarial examples for natural language processing tasks and for training a robust language model via adversarial training. Their focus is on the correctness of natural language understanding, which is beyond our scope of AI-text detection.\n\n## ", "3 RADAR: Methodology and Algorithms": "\n\nWe start this section by giving an overview and mathematical notations of our proposed RADAR framework in Figure 1. Then, in Sections 3.1 and 3.2, we provide the details on the design and training of the paraphraser and detector, respectively. Finally, we will summarise the entire training process into an algorithmic procedure in Section 3.3.\n\n**High-Level Methodology.** Our RADAR framework consists of three neural-network-based language models (LMs): the target LM \\(\\mathcal{T}_{\\theta}\\), the detector \\(\\mathcal{D}_{\\phi}\\) and the paraphraser \\(\\mathcal{G}_{\\sigma}\\), parameterized with \\(\\theta\\), \\(\\phi\\) and \\(\\sigma\\), respectively. We note that \\(\\mathcal{T}_{\\theta}\\) is frozen (no updates on \\(\\theta\\)) in the entire process. We summarize RADAR into three key steps:\n\n* **Step 1 (Data preparation):** Before training, we build \\(\\mathcal{M}\\), the corpus of AI-text, by applying document completion based on the prefix span of text in the human-text corpus \\(\\mathcal{H}\\) using \\(\\mathcal{T}_{\\theta}\\).\n\n * **Step 2 (Paraphraser update):** We collect AI-text samples \\(x_{m}\\) from \\(\\mathcal{M}\\) and use \\(\\mathcal{G}_{\\phi}\\) to do paraphrasing on \\(x_{m}\\) to generate paraphrased AI-text \\(x_{p}\\) to form a corpus \\(\\mathcal{P}\\). Then, we use the reward of \\(x_{p}\\) returned by the detector \\(\\mathcal{D}_{\\theta}\\) to update the paraphraser \\(\\mathcal{G}_{\\phi}\\) using PPO.\n* **Step 3 (Dectector update):** We use the human-text samples \\(x_{h}\\) from \\(\\mathcal{H}\\), the original AI-text samples \\(x_{m}\\) from \\(\\mathcal{M}\\), and the paraphrased AI-text samples \\(x_{p}\\) from \\(\\mathcal{P}\\) in step 2 to update the detector \\(\\mathcal{D}_{\\theta}\\) with a logistic loss function.\n* **Step 4 (Performance Validation and Evaluation):** During training, we use the test set of WebText as the validation dataset to estimate RADAR's performance. For evaluation, we use \\(\\mathcal{T}_{\\theta}\\) to generate AI-text for the evaluation dataset and to calculate RADAR's detection AUROC.\n\nStep 2 to Step 3 can be repeated until there is no improvement in the AUROC evaluated on the validation dataset. The nature of rivalry in adversarial learning and the introduced competition helps the detector to learn to be robust in detecting both original and paraphrased AI-text.\n\n### ", "Training Paraphraser via Clipped PPO with Entropy Penalty": "\n\nIn RADAR, the goal of the paraphraser \\(\\mathcal{G}_{\\sigma}\\) is to paraphrase the input machine-generated text \\(x_{m}\\). We model the generation of paraphrased text as a decision-making process, taking \\(x_{m}\\) as the state and the output text \\(x_{p}\\) as the action. In particular, we optimize \\(\\mathcal{G}_{\\sigma}\\) using the reward feedback from the detector \\(\\mathcal{D}_{\\phi}\\) with PPO. The output of \\(\\mathcal{D}_{\\phi}(x_{p})\\) is the predicted likelihood of \\(x_{p}\\) being Human-text. The reward returned by \\(x_{p}\\) and the log probability of the text \\(x_{p}\\) are defined in Eq. 1:\n\n\\[R(x_{p},\\phi)=\\mathcal{D}_{\\phi}(x_{p})\\in[0,1];\\ \\ \\log P_{\\mathcal{G}_{ \\sigma}}(x_{p}|x_{m})=\\sum_{i=1}^{N}\\log P_{\\mathcal{G}_{\\sigma}}(x_{p}^{i}|x_ {m},x_{p}^{1:i-1}),\\] (1)\n\nwhere \\(x_{p}^{i}\\) means the \\(i\\)-th token in the sentence \\(x_{p}\\) of length \\(N\\) and \\(x_{p}^{1:i-1}\\) represents the first \\(i-1\\) tokens in \\(x_{p}\\) (\\(x_{p}^{1:0}\\) means the default starting token).\n\nWe propose Clipped PPO with Entropy Penalty (cppo-ep) in RADAR to optimize \\(\\mathcal{G}_{\\sigma}\\). Let \\(\\text{clip}(\\cdot,a,b)\\) denote a value-clipping operation with a lower limit \\(a\\) and an upper limit \\(b\\), \\(r(\\sigma,x_{m},x_{p})\\) be the importance sampling ratio between a new policy \\(\\mathcal{G}_{\\sigma}\\) and an old policy \\(\\mathcal{G}_{\\sigma^{\\prime}}\\), and \\((x_{m},x_{p})\\sim P_{\\mathcal{G}_{\\sigma^{\\prime}}}\\) be a state-action pair sampled from \\(\\mathcal{G}_{\\sigma^{\\prime}}\\). The loss of cppo-ep is defined as:\n\n\\[L_{\\mathcal{G}}(\\sigma)=\\mathbb{E}_{(x_{m},x_{p})\\sim P_{\\mathcal{G}_{\\sigma^ {\\prime}}}}\\underbrace{-\\min\\{\\text{clip}(r(\\sigma,x_{m},x_{p}),1-\\epsilon,1+ \\epsilon),r(\\sigma,x_{m},x_{p})\\}\\cdot A(x_{p},\\phi)}_{L_{\\mathcal{A}}}\\cdot \\underbrace{-\\gamma S(\\sigma)}_{L_{\\mathcal{E}}}\\] (2)\n\nwhere \\(\\mathbb{E}\\) denotes expectation, \\(\\epsilon\\) is a parameter used in clipping to avoid the importance ratio \\(r\\) from being too large, \\(A(x_{p},\\phi)\\) is the advantage item of the paraphrased text \\(x_{p}\\) obtained by applying normalization to \\(R(x_{p},\\phi)\\) across the entire PPO sample buffer \\(\\mathcal{B}\\). \\(S(\\sigma)=\\mathbb{E}_{(x_{m},x_{p})\\sim P_{\\mathcal{G}_{\\sigma}}}-P_{\\mathcal{ G}_{\\sigma}}(x_{p}|x_{m})\\log P_{\\mathcal{G}_{\\sigma}}(x_{p}|x_{m})\\), which is an entropy term introduced to encourage \\(\\mathcal{G}_{\\sigma}\\) to explore more diverse generation policy. \\(\\gamma\\) is a coefficient to control the ratio between \\(L_{\\mathcal{A}}\\) and \\(L_{\\mathcal{E}}\\), in order to make a balance between advantage (\\(L_{\\mathcal{A}}\\)) and diversity (\\(L_{\\mathcal{E}}\\)) when paraphrasing.\n\n### ", "Training Detector via Reweighted Logistic Loss": "\n\nIn a typical GAN training process, the discriminator receives an equal amount of positive and negative samples in each step, assuring an in-batch sample balance. However, in RADAR, by construction, the number of AI-text samples is twice the number of human-text samples, because each \\(x_{h}\\) from the human-text corpus \\(\\mathcal{H}\\) is paired with a sample \\(x_{m}\\) from the original AI-text corpus \\(\\mathcal{M}\\) as well as a paraphrased sample \\(x_{p}\\) generated by the paraphraser \\(\\mathcal{G}_{\\phi}\\). To handle this in-batch imbalance problem, we use a reweighted logistic loss function to optimize the detector \\(D_{\\phi}\\), as described in Eq. 3:\n\n\\[L_{\\mathcal{D}}(\\phi)=\\underbrace{-\\mathbb{E}_{x_{h}\\sim\\mathcal{H}}\\log \\mathcal{D}_{\\phi}(x_{h})}_{L_{\\mathcal{H}}:\\text{ loss on human-text}}+\\lambda \\underbrace{\\mathbb{E}_{x_{m}\\sim\\mathcal{M}}-\\log\\left(1-\\mathcal{D}_{\\phi}(x_ {m})\\right)}_{L_{\\mathcal{M}}^{1}:\\text{ loss on original AI-text}}+\\lambda \\underbrace{\\mathbb{E}_{x_{m}\\sim\\mathcal{M}}-\\log\\left(1-\\mathcal{D}_{\\phi}( \\mathcal{G}_{\\sigma}(x_{m}))\\right)}_{L_{\\mathcal{M}}^{2}:\\text{ loss on perplexed AI-text}}\\] (3)\n\nRecall that \\(\\mathcal{D}_{\\phi}(x)\\in[0,1]\\) is the predicted probability of an input instance \\(x\\) being Human-text. \\(L_{\\mathcal{H}}\\) is the loss to improve the correctness of predicting \\(x_{h}\\sim\\mathcal{H}\\) as human-written. \\(L_{\\mathcal{M}}=L_{\\mathcal{M}}^{1}+L_{\\mathcal{M}}^{2}\\), where \\(L_{\\mathcal{M}}^{1}\\) and \\(L_{\\mathcal{M}}^{2}\\) are used to avoid \\(x_{m}\\) and \\(x_{p}\\) from being predicted as human-text, respectively. \\(\\lambda\\) is a coefficient ranging from 0 to 1. We introduce \\(\\lambda\\) to adjust the proportion of AI-text components in the overall loss function to alleviate the effects of sample imbalance.\n\n \n\n### ", "RADAR Algorithm": "\n\nThe entire training procedure of RADAR is summarized in Algorithm 1. For a given target LLM, RADAR returns a trained paraphraser and a trained detector through the designed training steps. In the evaluation phase, the detector is used to predict the likelihood of AI-text for any input instance.\n\n```\n1:Data initialization:\n2:Collect human-written text to build human-text corpus \\(\\mathcal{H}\\)\n3:Select a target language model \\(\\mathcal{T}_{\\theta}\\) to perform document completion on \\(\\mathcal{H}\\) to build the corresponding AI-text corpus \\(\\mathcal{M}\\)\n4:Build a replay buffer \\(\\mathcal{B}\\) to store samples temporarily collected for training\n5:Build a validation dataset \\(\\mathcal{V}\\) from \\(\\mathcal{H}\\) and \\(\\mathcal{M}\\)\n6:Model initialization:\n7:Detector \\(\\mathcal{D}_{\\phi}\\leftarrow\\phi_{\\text{pretrain}}\\) (a pretrained language model)\n8:Paraphraser \\(\\mathcal{G}_{\\sigma}\\leftarrow\\sigma_{\\text{pretrain}}\\) (a pretrained language model)\n9:Model training:\n10:for\\(i=1\\) : maximum step do\n11:Sample \\(x_{n}\\) and its corresponding \\(x_{m}\\) from \\(\\mathcal{H}\\) and \\(\\mathcal{M}\\) respectively\n12:Use \\(\\mathcal{G}_{\\sigma}\\) to paraphrase \\(x_{m}\\) and generate \\(x_{p}\\)\n13:Collect reward \\(R(x_{p},\\phi)\\) as in Eq. 1\n14:Normalize \\(R(x_{p},\\phi)\\) to compute the advantage function \\(A(x_{p},\\phi)\\) used in Eq. 2\n15: Fill \\(\\mathcal{B}\\) with \\((x_{h},x_{m},x_{p},A(x_{p},\\phi))\\)\n16:\\(\\sigma^{\\prime}\\leftarrow\\sigma\\) # ", "initialize the old policy \\(\\sigma^{\\prime}\\) as the current policy \\(\\sigma\\)": "\n17:for\\((x_{h},x_{m},x_{p},A(x_{p},\\phi))\\in\\mathcal{B}\\)do\n18:Compute the log probability \\(\\log P_{\\mathcal{G}_{\\sigma}}(x_{p}|x_{m})\\) and \\(\\log P_{\\mathcal{G}_{\\sigma}^{\\prime}}(x_{p}|x_{m})\\) using Eq. 1\n19: Update \\(\\mathcal{G}_{\\sigma}\\) using Eq. 2\n20:endfor\n21:for\\((x_{h},x_{m},x_{p},A(x_{p},\\phi))\\in\\mathcal{B}\\)do\n22: Update \\(\\mathcal{D}_{\\phi}\\) using Eq. 3\n23:endfor\n24:Clear \\(\\mathcal{B}\\)\n25:Evaluate AUROC of \\(\\mathcal{D}_{\\phi}\\) on the validation dataset \\(\\mathcal{V}\\)\n26:endfor\n27:Detector \\(\\mathcal{D}_{\\phi}\\leftarrow\\phi_{\\text{best}}\\) (the detector model with the best AUROC on the validation dataset)\n28:Paraphraser \\(\\mathcal{G}_{\\sigma}\\leftarrow\\sigma_{\\text{best}}\\) (the paraphraser model which pairs with \\(\\phi_{\\text{best}}\\))\n29:Return \\(\\mathcal{D}_{\\phi}\\) and \\(\\mathcal{G}_{\\sigma}\\) ```\n\n**Algorithm 1** RADAR: Robust AI-Text Detection via Adversarial Learning\n\n## ", "4 Experiments": "\n\n### ", "Experiment Setup": "\n\n**Datasets and Metrics.** For training, we sampled 160K documents from WebText [9] to build the human-text corpus \\(\\mathcal{H}\\). Then, we build the original AI-text corpus \\(\\mathcal{M}\\) from \\(\\mathcal{H}\\) using a target language model \\(\\mathcal{T}_{\\theta}\\), which performs text completion using the first 30 tokens as the prompt and limits the sentence length to be 200 tokens. For evaluation, we select four human-text datasets covering different domains. Following [23], we use Xsum, SQuAD, and Reddit WritingPrompts (WP) to test a detector's ability to detect fake news, avoid academic fraud, and identify machine-generated literature innovation, respectively. In addition, we also use the non-native-authored TOEFL dataset (TOFEL) [21] to evaluate a detector's bias when encountering non-native-authored English text. Please see Appendix A for more details about the evaluation datasets. Following existing works, we report the area under the receiver operating characteristic curve (AUROC) score by varying the detector's threshold as the performance measure (higher is better), which captures the relationship between the true positive rate and the false positive rate.\n\n**Comparisons.** We compare RADAR with various detection methods. These methods include the OpenAI (RoBERTa) model which is fine-tuned on WebText [9] and GPT-2 [2] generations, as well as the statistical approaches including log probability, rank, log rank, entropy, and DetectGPT [8; 19; 23].\n\nSpecifically, we implemented DetectGPT using the trained T5-large model as the mask-filling model and performed 10 perturbations for each sentence to be detected.\n\n**Large Language Models.** For the target LLM \\(\\mathcal{T}_{\\theta}\\), we select 4 pairs of LLMs and summarize them in Table 1. Each pair contains an open-source LLM and its fine-tuned version via instruction-tuning.\n\n**Paraphrase Configurations.** We consider two settings: _without (w/o) paraphrasing_ and _with paraphrasing_. To prepare the machine-generated text for evaluation, for the w/o paraphrasing setting, we use the original AI-text corpus \\(\\mathcal{M}\\) generated by a target LLM based on an evaluation dataset. For the with paraphrasing setting, we define two types of paraphrasing: _seen paraphraser_ and _unseen paraphraser_. The seen paraphraser refers to the paraphraser \\(\\mathcal{G}_{\\sigma}\\) returned by RADAR. The unseen paraphraser means a new paraphraser that has not participated in training the detector of RADAR. We used the OpenAI API service of GPT-3.5-Turbo as the default unseen paraphraser. The prompt we used for paraphrasing is \"Enhance word choices to make the sentence sound more like a human\", as inspired by [21].\n\n**Implementation Details.** We provide the detailed setups when implementing Algorithm 1. We build a PPO buffer \\(\\mathcal{B}\\) that can temporarily store 256 pairs of data for subsequent training. We use the pre-trained T5-large and RoBERTa-large models as the initialization of \\(\\mathcal{G}_{\\sigma}\\) and \\(\\mathcal{D}_{\\phi}\\) respectively. During training, we set the batch size to 32 and train the models until the validation loss converges. We use AdamW as the optimizer with the initial learning rate set to 1e-5 and use linear decay for both \\(\\mathcal{G}_{\\sigma}\\) and \\(\\mathcal{D}_{\\phi}\\). We set \\(\\lambda=0.5\\) for sample balancing in Eq. 3 and set \\(\\gamma=0.01\\) in Eq. 2. We follow the same construction principle of the training dataset to create the 4 evaluation datasets based on Xsum, SQuAD, WP, and TOFEL. Experiments were run on 2 GPUS (NVIDIA Tesla V100 32GB).\n\n### ", "Performance Evaluation and Comparison with Existing Methods": "\n\nWe run three groups of experiments (w/o paraphraser, seen paraphraser, and unseen paraphraser) and report the overall results of RADAR and the compared methods on all 4 datasets in Table 2. The reported AUROC scores are averaged over the 8 considered LLMs. In the relatively easy case of without paraphrasing, most detectors attain good AUROC scores. RADAR attains a comparable performance (0.856) to the best existing detector (log rank, 0.904). The slightly worse performance of RADAR can be explained by the tradeoff in enhancing AI-text detection against paraphrasing.\n\nWhen facing paraphrasing, all existing methods except entropy show significant performance degradation. The drop in AUROC compared to the w/o paraphrasing case ranges from \\(10.4\\%\\) to \\(81.7\\%\\). While entropy is shown to be more robust to paraphrasing, its AUROC score can be quite low. On the contrary, RADAR demonstrates robust and superior detection power, attaining the best performance on every dataset. As shown in Figure 2, the average AUROC score of RADAR (0.857) improves the best existing method (entropy, 0.651) by 31.64% against the unseen paraphraser. On average, RADAR is more robust to the seen paraphraser than the unseen paraphraser, because the seen paraphraser is what is used to train the detector in RADAR. More importantly, the detection performance of RADAR is stable across different paraphrasing schema, suggesting that RADAR can successfully mitigate the performance drop in AI-text detection.\n\n\nTable 1: Summary of the studied large language models \n\n### ", "AI-Text Detection Transferability of RADAR": "\n\nWe explore the AI-text detection transferability of RADAR between the 8 LLMs and report the ratio F(A,B)=AUROC(A,B)/AUROC(B,B) for each LLM pair (A,B), where AUROC(A,B) means using the RADAR's detector trained on model A to evaluate the AI-text generated by model B. A larger ratio means better transferability from A to B. Figure 3 shows the matrix of pairwise detection transferability and the bar chart of the holistic detection transferability to all the 8 LLMs in the without and unseen paraphrasing settings. We highlight two key observations as follows.\n\n**(I) Instruction-tuned models have better detection transferability.** Partitioning the LLMs into two groups, we can find that the detector targeting an instruction-tuned LLM (top 4 rows) generally transfers better than the detector targeting the corresponding LLM without instruction-tuning (bottom 4 rows). Take the pair (Vicuna-7B, LLaMA-7B) as an example, we can see that without paraphrasing, F(Vicuna-7B,LLaMA) can reach up to \\(95.0\\%\\). On the other hand, F(LLaMA-7B,Vicuna-7B) can only\n\n\nTable 2: AUROC score averaged over 8 target LLMs. RADAR-Seen Paraphraser means the paraphraser used in RADAR (\\(\\mathcal{G}_{\\sigma}\\)). RADAR-Unseen Paraphraser is OpenAI\u2019s GPT-3.5-Turbo API. The notations \\(\\{\\medaccount for \\(68.2\\%\\). Sorting the detectors according to the holistic detection transferablbility (which is presented in the bar chart), we can see the top-3 detectors are all trained with the instruction-tuned LLMs. A similar conclusion can be made for the with paraphrasing setting. Moreover, there is no obvious trend between the target LLM size and the resulting detection performance. The effect of instruction tuning on transferability is more prominent than model size.\n\n**(II) RADAR achieves better detection transferability against paraphrasing.** Another interesting finding is that RADAR's transferability is generally improved when paraphrasing is in place. Comparing the two bar charts in Fig. 2(a) and Fig. 2(b), the average holistic detection transferability (over all LLMs) is increased by \\(11.6\\%\\). Except for LLaMA-7B (3.8% drop) and GPT-J-6B (1.4% drop), all other LLMs' holistic transferability scores are improved from 2.4% (Palmyra-base) to 47.6% (Camel-5B).\n\n**Transfer detection on AI-text generated by GPT-4.** We also test RADAR detectors on the texts generated by GPT-4. The results show that 5 out of 8 RADAR models can outperform the OpenAI (RoBERTa), and three of them can achieve more than 0.8 detection AUROC. For example, RADAR trained on Camel-5B can achieve 0.915 detection AUROC on GPT-4 generations. The results show that the RADAR can achieve good transfer detection for GPT-4. The details are given in Appendix K.\n\n**Ensemble detection.** We also explored whether and how ensemble learning benefits detection by combining the outputs of detectors. The results show that the detection performance can be lifted by carefully tuning the ensemble ratio and the model to be combined. Please see Appendix G for the exact experiment results.\n\nTo sum up, we believe our findings suggest promising results for training a universal robust AI-text detector by leveraging state-of-the-art LLMs, and RADAR can use a smaller-sized and weaker LLM to achieve good detection performance on texts generated from top-notching LLMs (such as GPT-4).\n\n### ", "Variants of Paraphrasing": "\n\nIn addition to paraphrasing the original LLM-generated texts, we also evaluate the detection performance when paraphrasing human texts (the output is labeled as AI-text). We also allow paraphrasing multiple times in our analysis. We conduct our experiments on the Xsum dataset using the detector trained with Camel-5B. The paraphraser for evaluation is GPT-3.5-Turbo. As shown in Figure 3(a), we find that RADAR is the only detector robust to multi-round paraphrasing. On paraphrased AI-text, all existing methods suffer from a notable performance drop. On paraphrased human-text, RADAR remains effective, along with two existing methods (OpenAI (RoBERTa) and entropy). In general, multi-round paraphrasing does not seem to increase the difficulty of AI-text detection. We also find RADAR is robust to Dipper [18], another paraphrase model. Please see Appendix I for details.\n\n### ", "Evaluation on RADAR's Paraphraser": "\n\nAlthough our focus is on training a robust AI-text detector via RADAR, as a by-product, we expect to obtain a better paraphraser through adversarial learning. To verify this hypothesis, we compare the quality of the initial paraphraser (a pretrained LLM) and the final paraphraser returned by RADAR\n\nFigure 4: Detection AUROC of RADAR against multiple paraphrasing. The experiments are conducted on Xsum using the detector trained for Camel-5B.\n\n using GPT-3.5-Turbo's response. We select 100 documents from WebText [9] and use 4 different paraphrasers from RADAR to paraphrase the documents. Then, we ask GPT-3.5-Turbo to rate sentences generated by these paraphrasers versus their initial version (T5-large). Figure 4(a) shows that RADAR also improves the quality of paraphrasing. Figure 4(b) shows that the RADAR's paraphraser can score higher if it is trained with a larger target LLM with instruction tuning. Following [11; 12], we also evaluate RADAR's paraphrasers on Quora Question Pairs (QQP8) and use iBLEU (\\(\\alpha=0.8\\)) [30] as the metric (higher is better). Figure 4(c) shows that the paraphrasing performance can be improved via RADAR as all the RADAR-paraphrasers can achieve a larger iBLEU score than T5-large.\n\nFootnote 8: https://www.kaggle.com/c/quora-question-pairs/data\n\n### ", "Balancing the Detection Performance in the with and without Paraphrasing Settings": "\n\nFrom Figure 2, we can observe that though RADAR can achieve robust detection under paraphrasing, it is (slightly) worse than some of the existing baselines when AI-text data are unperturbed (i.e., w/o paraphrasing). We run a trade-off analysis on the weight coefficient \\(\\lambda\\) in Equation (3) to study whether RADAR can be further tuned to achieve competitive performance on unperturbed data while still being robust to paraphrasing. We use Vicuna-7B as the target model to train 10 RADAR detectors by varying \\(\\lambda\\) from 0.1 to 1.0 with 0.1 increment, and then evaluate these detectors as well as other detection baselines on the evaluation datasets. The results in Appendix J show that we can promote RADAR's performance on unperturbed data while still preserving high detection AUROC on paraphrased data. Take \\(\\lambda=0.6\\) as an example. When we change \\(\\lambda\\) from \\(0.5\\) (the default value of \\(\\lambda\\)) to \\(0.6\\), the AUROC of w/o paraphrasing increases from \\(0.906\\) to \\(0.937\\), while the AUROC of unseen-paraphrasing also increases from \\(0.892\\) to \\(0.920\\). The result suggests that the detection performance of RADAR in the with and without paraphrasing settings can be simultaneously improved or better balanced with careful tuning of the hyperparameter \\(\\lambda\\) during training.\n\n## ", "5 Conclusion": "\n\nIn this paper, we presented a robust AI-text detector training framework called RADAR, which adopts adversarial learning to jointly train a detector and a paraphraser. RADAR addresses the shortcoming of existing detectors when facing LLM-paraphrased texts. Our extensive experiments on 8 LLMs and 4 datasets validated the effectiveness of RADAR and demonstrated its strong transferability across LLMs. We believe our results shed new light on improving AI-text detection.\n\n## ", "6 Limitations and Ethical Considerations": "\n\nWhile RADAR is more robust to paraphrasing than existing baselines measured on 4 datasets, sometimes it may show degraded detection performance against native LLM-generated texts (without paraphrasing) when compared to the best existing detection method. Moreover, like every existing AI-text detector, we acknowledge that our detector is not perfect and will likely give incorrect predictions in some cases. In terms of ethical considerations, we suggest users use our tool to assist with identifying AI-written content at scale and with discretion. If the detection result is to be used as evidence, further validation steps are necessary as RADAR cannot always make correct predictions.\n\nFigure 5: Evaluation of RADAR\u2019s paraphraser versus its initial version (T5-large).\n\n \n\n## ", "Acknowledgement": "\n\nThe authors thank James Sanders and Jonathon Hartley for providing examples that can weaken the detection of RADAR models.\n\n## ", "References": "\n\n* [1] Jack Brassil, Steven H. Low, Nicholas F. Maxemchuk, and Lawrence O'Gorman. Electronic marking and identification techniques to discourage document copying. _IEEE J. Sel. Areas Commun._, 13(8):1495-1504, 1995.\n* [2] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In _NeurIPS_, 2020.\n* [3] Souradip Chakraborty, Amrit Singh Bedi, Sicheng Zhu, Bang An, Dinesh Manocha, and Furong Huang. On the possibilities of ai-generated text detection. _CoRR_, abs/2304.04736, 2023.\n* [4] Minhao Cheng, Jinfeng Yi, Pin-Yu Chen, Huan Zhang, and Cho-Jui Hsieh. Seq2sick: Evaluating the robustness of sequence-to-sequence models with adversarial examples. In _Proceedings of the AAAI conference on artificial intelligence_, volume 34, pages 3601-3608, 2020.\n* [5] Pedro Colon-Hernandez, Henry Lieberman, Yida Xin, Claire Yin, Cynthia Breazeal, and Peter Chin. Adversarial transformer language models for contextual commonsense inference. _CoRR_, abs/2302.05406, 2023.\n* [6] Long Dai, Jiarong Mao, Xuefeng Fan, and Xiaoyi Zhou. Deephider: A multi-module and invisibility watermarking scheme for language model. _CoRR_, abs/2208.04676, 2022.\n* [7] Cyprien de Masson d'Autume, Shakir Mohamed, Mihaela Rosca, and Jack W. Rae. Training language gans from scratch. In _NeurIPS_, pages 4302-4313, 2019.\n* [8] Sebastian Gehrmann, Hendrik Strobelt, and Alexander M. Rush. GLTR: statistical detection and visualization of generated text. In _ACL_, pages 111-116, 2019.\n* [9] Aaron Gokaslan, Vanya Cohen, Ellie Pavlick, and Stefanie Tellex. Openwebtext corpus. http://Skylion007.github.io/OpenWebTextCorpus, 2019.\n* [10] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. _Communications of the ACM_, 63(11):139-144, 2020.\n* [11] Tom Hosking and Mirella Lapata. Factorising meaning and form for intent-preserving paraphrasing. In _ACL/IJCNLP_, pages 1405-1418, 2021.\n* [12] Tom Hosking, Hao Tang, and Mirella Lapata. Hierarchical sketch induction for paraphrase generation. In _ACL_, pages 2489-2501, 2022.\n* [13] Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, and Douglas Eck. Automatic detection of generated text is easiest when humans are fooled. In _ACL_, pages 1808-1822, 2020.\n* [14] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In _ICLR_, 2017.\n* [15] Mohan S. Kankanhalli and K. F. Hau. Watermarking of electronic text documents. _Electron. Commer. Res._, 2(1-2):169-187, 2002.\n* [16] John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and Tom Goldstein. A watermark for large language models. _CoRR_, abs/2301.10226, 2023.\n* [17] Kalpesh Krishna, Yixiao Song, Marzena Karpinska, John Wieting, and Mohit Iyyer. Paraphrasing evades detectors of ai-generated text, but retrieval is an effective defense. _CoRR_, abs/2303.13408, 2023.\n\n* Krishna et al. [2023] Kalpesh Krishna, Yixiao Song, Marzena Karpinska, John Wieting, and Mohit Iyyer. Paraphrasing evades detectors of ai-generated text, but retrieval is an effective defense. _CoRR_, abs/2303.13408, 2023.\n* Lavergne et al. [2008] Thomas Lavergne, Tanguy Urvoy, and Francois Yvon. Detecting fake content with relative entropy scoring. In _Proceedings of the ECAI'08 Workshop on Uncovering Plagiarism, Authorship and Social Software Misuse, Patras, Greece, July 22, 2008_, volume 377 of _CEUR Workshop Proceedings_, 2008.\n* Lei et al. [2019] Qi Lei, Lingfei Wu, Pin-Yu Chen, Alex Dimakis, Inderjit S Dhillon, and Michael J Witbrock. Discrete adversarial attacks and submodular optimization with applications to text classification. _Proceedings of Machine Learning and Systems_, 1:146-165, 2019.\n* Liang et al. [2023] Weixin Liang, Mert Yukekgonul, Yining Mao, Eric Wu, and James Zou. GPT detectors are biased against non-native english writers. _CoRR_, abs/2304.02819, 2023.\n* Maddison et al. [2017] Chris J. Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous relaxation of discrete random variables. In _ICLR_, 2017.\n* Mitchell et al. [2023] Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D. Manning, and Chelsea Finn. Detectgpt: Zero-shot machine-generated text detection using probability curvature. _CoRR_, abs/2301.11305, 2023.\n* Ouyang et al. [2022] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In _NeurIPS_, 2022.\n* Raffel et al. [2020] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _JMLR_, 2020.\n* Rodriguez et al. [2022] Juan Rodriguez, Todd Hay, David Gros, Zain Shamsi, and Ravi Srinivasan. Cross-domain detection of gpt-2-generated technical text. In _NAACL_, pages 1213-1233, 2022.\n* Sadasivan et al. [2023] Vinu Sankar Sadasivan, Aounon Kumar, Sriram Balasubramanian, Wenxiao Wang, and Soheil Feizi. Can ai-generated text be reliably detected? _CoRR_, abs/2303.11156, 2023.\n* Schulman et al. [2017] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. _CoRR_, abs/1707.06347, 2017.\n* Solaiman et al. [2019] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford, Gretchen Krueger, Jong Wook Kim, Sarah Kreps, et al. Release strategies and the social impacts of language models. _arXiv preprint arXiv:1908.09203_, 2019.\n* Sun and Zhou [2012] Hong Sun and Ming Zhou. Joint learning of a dual SMT system for paraphrase generation. In _ACL_, pages 38-42, 2012.\n* Topkara et al. [2006] Umut Topkara, Mercan Topkara, and Mikhail J. Atallah. The hiding virtues of ambiguity: quantifiably resilient watermarking of natural language text through synonym substitutions. In _MM&Sec_, pages 164-174, 2006.\n* Ueoka et al. [2021] Honai Ueoka, Yugo Murawaki, and Sadao Kurohashi. Frustratingly easy edit-based linguistic steganography with a masked language model. In _NAACL_, pages 5486-5492, 2021.\n* Wu et al. [2021] Qingyang Wu, Lei Li, and Zhou Yu. Textgail: Generative adversarial imitation learning for text generation. In _AAAI_, pages 14067-14075, 2021.\n* Xu et al. [2018] Jingjing Xu, Xuancheng Ren, Junyang Lin, and Xu Sun. Diversity-promoting GAN: A cross-entropy based generative adversarial network for diversified text generation. In _EMNLP_, pages 3940-3949, 2018.\n* Yang et al. [2018] Zichao Yang, Zhiting Hu, Chris Dyer, Eric P. Xing, and Taylor Berg-Kirkpatrick. Unsupervised text style transfer using language models as discriminators. In _NeurIPS_, pages 7298-7309, 2018.\n* Yu et al. [2017] Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. Seqan: Sequence generative adversarial nets with policy gradient. In _AAAI_, pages 2852-2858, 2017.\n* Zellers et al. [2019] Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. Defending against neural fake news. In _NeurIPS_, pages 9051-9062, 2019.\n\n \n\n## ", "Appendix A Human-text Corpora": "\n\nWe summarize the human-text corpora we used in RADAR's training, validation, and evaluation phases in Table A1. It shows the usage of these corpora, the source where they come from, and the number of samples we select from them for evaluation.\n\n## ", "Appendix B Details of Existing Detectors": "\n\nEvery detector assigns a score to the given text and determines whether the text is generated by AI based on the score. We introduce the scores used in existing detectors in the following.\n\n**Unsupervised Methods.** In this paper, we leverage log-p, rank, log-rank, and entropy as the baselines. They are all unsupervised methods. They depend on statistical metrics of the given text to determine if it is an AI-text. Specifically, they input the given text to the target language model \\(\\mathcal{T}_{\\theta}\\), and sniff statistics from \\(\\mathcal{T}_{\\theta}\\)'s output. We use \\(M_{\\text{log-p}}\\), \\(M_{\\text{ank}}\\), \\(M_{\\text{log-rank}}\\), and \\(M_{\\text{entropy}}\\) to represent the score for them respectively. These scores are calculated as below:\n\n\\[M_{\\text{log-p}}(x)=\\frac{1}{N-1}\\sum_{i=2}^{N}\\log P_{\\mathcal{T}_{\\theta}}(x ^{i}|x^{1:i-1})\\] (A1)\n\n\\[M_{\\text{ank}}(x)=-\\frac{1}{N-1}\\sum_{i=2}^{N}\\text{index}(\\text{sort}(\\log P _{\\mathcal{T}_{\\theta}}(\\cdot|x^{1:i-1})),x^{i})\\] (A2)\n\n\\[M_{\\text{log-rank}}(x)=-\\frac{1}{N-1}\\sum_{i=2}^{N}\\log(\\text{index}(\\text{ sort}(\\log P_{\\mathcal{T}_{\\theta}}(\\cdot|x^{1:i-1})),x^{i}))\\] (A3)\n\n\\[M_{\\text{entropy}}(x)=-\\frac{1}{N-1}\\sum_{i=2}^{N}\\sum_{j=1}^{C}P_{\\mathcal\n\n**Supervised Methods.** RADAR, as well as OpenAI (RoBERTa), are both supervised detection methods. Let \\(D\\) denote the OpenAI (RoBERTa) or RADAR's detector. The score \\(M_{D}\\) that they assigned to the input text \\(x\\) is defined below:\n\n\\[M_{D}(x)=\\text{Softmax}(f_{D}(x))[0]\\] (A6)\n\nwhere \\(f_{D}(x)\\) means \\(D\\)'s [CLS] token's logits over the whole label set. \\(\\text{Softmax}(f_{D}(x))[0]\\) means the prediction probability of AI-text, and \\(\\text{Softmax}(f_{D}(x))[1]\\) means the prediction probability of human-text. Since there are only two labels (AI-text vs human-text) in the label set, the detection is equivalent to a logistic regression task with a scalar output.\n\n## ", "Appendix C RADAR Loss Visualization": "\n\nWe visualize RADAR's training process by presenting the training loss and validation performance below. We take the Camel-5B language model as an example.\n\nFrom Figure A1a and Figure A1b we can see that the loss for both the Detector and the Paraphraser converge. From Figure A1c we can conclude that RADAR's detection capacity on the validation dataset is promoted to a stable state as the training loss of the detector and the paraphraser converge.\n\n## ", "Appendix D Paraphrase Settings": "\n\nWhen using a RADAR-seen paraphraser, the input is: _Paraphrase: [s]_, Where [s] is the slot for the input AI-text. The paraphraser adopts top-k sampling and nucleus sampling strategy to decode a new word. Top-k sampling only focuses on the \\(k\\) highest-probability tokens. Nucleus sampling sorts the sampling distribution in descending order and selects top-n tokens from the sorted distribution until their cumulative probability exceeds \\(p\\), and then samples the next token from the top-n candidates. In our experiment, we set \\(k\\) to 50 and \\(p\\) to 0.95.\n\nWhen using GPT-3.5-Turbo's API service as another paraphrasing tool (RADAR-Unseen paraphraser) to paraphrase the AI-texts, the instruction we input is _Enhance the word choices in the sentence to sound more like that of a human_. When paraphrasing the human-texts, the instruction is _Worsen the word choices in the sentence to sound less like that of a human_.\n\nMulti-round paraphrasing can be easily achieved by using the paraphrased text as input text and using the paraphraser to re-paraphrase it.\n\n## ", "Appendix E Complete Experimental Results": "\n\nWe show all the evaluation results in Table A2 (without paraphraser), Table A3 (with RADAR-Unseen paraphraser) and Table A4 (with RADAR-Seen paraphraser).\n\n## ", "Appendix F Case Study for RADAR": "\n\n**Sample Selection.** For each evaluation dataset (Xsum, SQuAD, WP, and TOFEL), we randomly select one sample from the human-text corpora and use 8 instruction-tuned LLMs (Vicuna-7B,Dolly-V1-6B, Camel-5B, Dolly-V2-3B, LLaMA-7B, GPT-J-6B, Palmyra-base, Pythia-2.8B) to generate completions for each text, then use GPT-3.5-Turbo API service to perform paraphrase to all completions. Thus, we get 64 AI-texts in total (32 completions, 32 paraphrases).\n\n**Case Selection**. Each AI-text has a source model, for the detection of one given text, we use the RADAR-detector trained for its source model. Then, we collect the one with the largest confidence to be machine-generated (most likely to be correctly detected) and the one with the smallest confidence to be machine-generated (mostly likely to evade detection) for the following case study.\n\n**Analysis**. We show two detection cases in Table A5. One with a higher probability (0.9999) is an easy-to-detect case, and another with a lower probability (0.0031) is a difficult-to-detect case. Specifically, the latter can only be detected when our detection threshold drops below 0.0031. From the table, we can see that the misclassification has a good explanation, because the AI-text is nearly identical to the original Human-text, except for the inclusion of several additional words. In fact, the AI-text can be seen as the Human-text with a suffix composed of several words appended to it.\n\n \n\n## ", "Appendix G Effectiveness of Ensembling Detectors from RADAR": "\n\nWe study the ensembling detection performance by combining two RADAR-detectors' prediction probability and report the detection AUROC calculated using the combined prediction probability. The combined prediction probability \\(E(A,B,\\beta,x)=(1-\\beta)\\mathcal{D}_{\\phi}^{A}(x)+\\beta\\mathcal{D}_{\\phi}^{B}(x)\\) is a weighted sum of the prediction probability of the base model \\(A\\) and the augmented model \\(B\\). The detection performances are shown in Table A6. We explore various ensembling ratios and ensembling models. Ensembling with the base model itself and setting the ensembling ratio to 0 both mean no ensembling. Setting the ensembling ratio to 1 is another extreme case, which refers to the transfer detection scheme mentioned in Section 4.3. From the results, we can see that the ensembling detection's effectiveness can be influenced by both the ensemble model and the ensemble ratio.\n\n\\begin{table}\n\\begin{tabular}{c|c|c|c|c|c|c|c|c} \\hline \\multirow{2}{*}{**Source Model**} & \\multirow{2}{*}{**Source Dataset**} & \\multicolumn{4}{c|}{**Original Random Test**} & \\multicolumn{4}{c|}{**Perplexer**} & \\multicolumn{2}{c|}{**Total**} & \\multicolumn{2}{c}{**Predability to be a test**} \\\\ \\cline{3-10}  & & & \\multicolumn{1}{c|}{class size\n\n## ", "Appendix H Text Detection Towards Different Lengths": "\n\nWe study RADAR's effects on AI-generated texts with different lengths. We grouped the evaluation dataset into 5 subsets according to the length of the AI-text. The results are shown in Figure A2. We summarize our observations below:\n\n* For the group {log probability, rank, log rank, DetectGPT}, without paraphrasing, these methods are not really sensitive to the length of the text. When facing paraphrasing, however, their performance increases with a longer text length.\n* For the group {entropy, OpenAI (RoBERTa), RADAR}, without paraphrasing, these methods have a better detection performance for longer texts. On the contrary, their performance degrades when facing longer paraphrased AI-text (even though RADAR seems much better for short-text detection, it still outperforms other methods by a large margin, see Figure A2b).\n\nDetection on Dipper Paraphrasing\n\nWe also explore the use of RADAR to detect other advanced paraphrasers. We use Dipper proposed in [18] (L60-O60 version) to paraphrase the evaluation dataset following the same setup of RADAR-Unseen paraphrasing and RADAR-Seen paraphrasing. The results are shown in Figure A3. We can see from the results that the green bars are much higher than the red bars except for DetectGPT, which means in general, Dipper is less destructive than GPT-3.5-Turbo to these detectors. RADAR's detection AUROC on Dipper reaches \\(0.9\\). One thing to be noted is that though the OpenAI (RoBERTa) can perform well under Dipper paraphrasing, it still cannot be regarded as robust because it can be bypassed by other paraphrasers (RADAR-Seen and RADAR-Unseen).\n\n## ", "Appendix J Sensitivity Analysis of RADAR on the Hyperparameter \\(\\lambda\\)": "\n\nPlease refer to our discussion in Section 4.6.\n\nFigure A4: Sensitivity analysis for \\(\\lambda\\) in Eq. (3). The RADAR models are trained with Vicuna-7B with different \\(\\lambda\\) values ranging from 0.1 to 1.0. We then compare these RADAR detectors with other baselines on Vicuna-7B\u2019s 4 evaluation datasets. The horizontal/vertical axis shows the AUROC of unperturbed/paraphrased AI-text.\n\n Detection on GPT-4 Generated Texts\n\nFigure A5 shows RADAR's transfer detection for GPT-4. We use RADAR detectors trained on weaker LLMs (Vicuna-7B, Camel-5B, etc.) to detect the generation of GPT-4. We prompted GPT-4 with the instruction: _You are a helpful assistant to complete given text:_, to generate texts based on the same evaluation datasets (Xsum, SQuAD, WP and TOFEL) used when evaluating other LLMs. The detection performance have been discussed in Section 4.3.\n\n## ", "Appendix L Use GPT-3.5 to Assess RADAR-paraphrasers": "\n\nAfter RADAR training, we not only get a detector but also a paraphraser. We use GPT-3.5-Turbo to assign a score to these paraphrasers' generation to assess the language generation capability of these paraphrasers and compare with their initial version (T5-large) to see how adversarial training benefits them. We compare these paraphrasers on a WebText subset with 100 samples. For each sentence in this subset, we first use GPT-2-XL to generate one sentence, and then use the 5 paraphrasers (T5-large and 4 paraphrasers trained on 4 instruction-tuned models' generation) to paraphrase this sentence respectively. Then we input these 5 paraphrased sentences combining an instruction to GPT-3.5-Turbo. Our instruction is _You are given an array of five sentences. Please rate these sentences and reply with an array of scores assigned to these sentences. Each score is on a scale from 1 to 10, the higher the score, the sentence is written more like a human. Your reply example: [2,2,2,2,2]._ Then we sniff the score for each sentence from the returned answer and make the comparison. The results have been discussed in Section 4.5.\n\n "}}